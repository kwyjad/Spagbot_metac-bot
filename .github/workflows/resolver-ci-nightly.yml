name: resolver-ci-nightly

on:
  schedule:
    - cron: "17 1 * * *"
  workflow_dispatch: {}

permissions:
  contents: read

concurrency:
  group: resolver-nightly
  cancel-in-progress: false

jobs:
  full_connectors:
    name: "Run connector (full): ${{ matrix.connector }}"
    runs-on: ubuntu-latest
    timeout-minutes: 90
    strategy:
      fail-fast: false
      matrix:
        connector:
          - acled_client
          - dtm_client
          - emdat_client
          - gdacs_client
          - hdx_client
          - reliefweb_client
          - unhcr_client
          - wfp_mvam_client
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version-file: .python-version
          cache: pip
          cache-dependency-path: |
            resolver/pyproject.toml
            resolver/requirements.txt
            resolver/requirements-dev.txt

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r resolver/requirements.txt

      - name: Run ${{ matrix.connector }} (full)
        env:
          RESOLVER_FAIL_ON_STUB_ERROR: "0"
        run: |
          python -m resolver.ingestion.run_all_stubs --mode real-only ${{ matrix.connector }} --log-level INFO --log-format json

      - name: Record job status
        if: always()
        env:
          CONNECTOR: ${{ matrix.connector }}
          JOB_STATUS: ${{ job.status }}
        run: |
          import json
          import os
          from pathlib import Path

          connector = os.environ["CONNECTOR"]
          status = os.environ.get("JOB_STATUS", "unknown")
          log_dir = Path("resolver/logs/ingestion")
          log_dir.mkdir(parents=True, exist_ok=True)
          summary_path = log_dir / f"{connector}-job-status.json"
          summary_path.write_text(json.dumps({"connector": connector, "job_status": status}))
        shell: python

      - name: Upload nightly artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: nightly-${{ matrix.connector }}
          path: |
            resolver/logs/ingestion/**
            resolver/exports/**
            resolver/staging/**
          retention-days: 7

  aggregate:
    if: ${{ always() }}
    needs: full_connectors
    runs-on: ubuntu-latest
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: nightly-artifacts
          merge-multiple: true

      - name: Build summary
        run: |
          import csv
          import json
          from pathlib import Path

          root = Path("nightly-artifacts")
          records = []
          for status_file in root.rglob("*-job-status.json"):
              try:
                  data = json.loads(status_file.read_text())
              except json.JSONDecodeError:
                  continue
              connector = data.get("connector") or status_file.stem.replace("-job-status", "")
              job_status = data.get("job_status", "unknown")
              record = {
                  "connector": connector,
                  "job_status": job_status,
                  "status": "unknown",
                  "attempts": None,
                  "duration_ms": None,
                  "notes": "",
              }
              # Heuristic enrichment from any sibling log file
              log_dir = status_file.parent
              for log_file in sorted(log_dir.glob("*")):
                  if not log_file.is_file():
                      continue
                  try:
                      lines = log_file.read_text(errors="ignore").splitlines()
                  except Exception:
                      continue
                  for s in (ln.strip() for ln in lines if ln.strip()):
                      if s.startswith("{") and s.endswith("}"):
                          try:
                              payload = json.loads(s)
                          except Exception:
                              continue
                          level = (payload.get("level") or "").upper()
                          if level == "ERROR":
                              record["status"] = "error"
                          elif level == "WARNING" and record["status"] != "error":
                              record["status"] = "warning"
                          elif level == "INFO" and record["status"] not in ("error", "warning"):
                              record["status"] = "ok"
                          record["attempts"] = payload.get("attempt", record["attempts"])
                          record["duration_ms"] = payload.get("duration_ms", record["duration_ms"])
                      else:
                          if "ERROR" in s:
                              record["status"] = "error"
                          elif "WARNING" in s and record["status"] != "error":
                              record["status"] = "warning"
                  break
              records.append(record)

          records.sort(key=lambda item: item["connector"] or "")
          summary_dir = Path("nightly-summary")
          summary_dir.mkdir(exist_ok=True)
          json_path = summary_dir / "summary.json"
          csv_path = summary_dir / "summary.csv"
          json_path.write_text(json.dumps(records, indent=2, sort_keys=True))
          with csv_path.open("w", newline="", encoding="utf-8") as handle:
              writer = csv.DictWriter(
                  handle,
                  fieldnames=["connector", "job_status", "status", "attempts", "duration_ms", "notes"],
              )
              writer.writeheader()
              for row in records:
                  writer.writerow(row)
        shell: python

      - name: Upload nightly summary
        uses: actions/upload-artifact@v4
        with:
          name: nightly-summary
          path: nightly-summary
          retention-days: 7
