# Will the US or California require licenses to train large AI models before 2026? (QID: 38854)

- Type: binary

- URL: https://www.metaculus.com/questions/38854/

- Classifier: politics | strategic=False (score=0.00)

### SeenGuard

- enabled=True | lock_status=acquired

- run_filter: before=2 | skipped=0 | after=2

- debug_note=lock acquired

## Research (summary)

CALIBRATION GUIDANCE (auto-generated weekly):
BINARY CALIBRATION
- No resolved binary questions (with pre-resolution forecasts) yet.
- Advice: keep using base rates + small evidence-weighted updates; avoid big swings.

MULTIPLE-CHOICE CALIBRATION (Top-1)
- No resolved MCQ questions (with pre-resolution forecasts) yet.

NUMERIC CALIBRATION (PIT-lite + CRPS)
- No resolved numeric questions (with pre-resolution forecasts) yet.
- When numeric results exist, we’ll check p10/p50/p90 coverage and CRPS.

General takeaway: apply **small** nudges only where gaps are consistent with decent sample sizes (≥10).
— end calibration —

### Reference class & base rates
1.  **New Federal Tech Regulation on a Novel Risk:** This class includes regulations for novel technologies with complex, debated risks (e.g., internet privacy, cryptocurrency, genetic engineering). The process from public concern to implemented regulation is typically slow, often taking 5-10 years. The base rate for a *licensing regime* (a high bar of regulation) being enacted and implemented within 3 years of a major public call-to-action (Altman's 2023 testimony) is low, likely **5-15%**. This is due to legislative gridlock, industry lobbying (both for and against), and the time needed for agency rulemaking.
2.  **State-Level "First Mover" Tech Legislation (California):** California often acts faster than the federal government on tech and environmental issues (e.g., CCPA for data privacy, vehicle emission standards). The California Consumer Privacy Act (CCPA) was passed in June 2018 and became effective in January 2020, a ~1.5-year lag. The base rate for California passing and implementing a major tech regulation within a ~2.5-year window is significantly higher than the federal rate, perhaps **30-40%**.
3.  **Creation of Licensing Regimes for High-Consequence Activities:** This class includes activities like operating nuclear power plants, developing new pharmaceuticals, or commercial aviation. These regimes are established in response to clear, demonstrated harms or catastrophic risks. They are characterized by powerful federal agencies (NRC, FDA, FAA) and extensive, time-consuming approval processes. The base rate for establishing a *new* such agency and licensing process from scratch in under 3 years is extremely low, likely **<5%**.

### Recent developments (timeline bullets)
- **2025-10-27** OpenAI's proposed shift to a public benefit corporation is scrutinized, potentially increasing calls for external, government-led regulation rather than self-regulation. (↑, small)
- **2025-10-21** California's SB 53, the Transparency in Frontier Artificial Intelligence Act (TFAIA), is signed into law, establishing a governance framework for advanced AI. (↑, large) — This is the most significant legislative action to date in the US.
- **2025-10-21** The implementation date for California's SB 53 is set for January 2026, after the question's resolution date of Dec 31, 2025. (↓, very large) — This timeline detail is critical and severely undermines a YES resolution based on this specific bill.
- **2025-09-26** Sam Altman predicts AGI by 2029, spurring U.S. lawmakers to increase regulatory efforts due to perceived geopolitical and existential risks. (↑, moderate) — Increases political urgency, but may not translate to legislative speed.
- **2025-06-10** Altman's blog post "The Gentle Singularity" states "the takeoff has started," framing AI progress as rapid and ongoing, adding pressure for immediate governance. (↑, moderate) — Reinforces the narrative of urgency for policymakers.
- **2024-07-01** A review of global AI regulation highlights the complexity and long timelines of frameworks like the EU AI Act, which is expected to be implemented around 2025. (↓, moderate) — Provides a parallel case suggesting that even with political will, implementation is not immediate.

### Mechanisms & drivers (causal levers)
1.  **Legislative Inertia vs. Political Urgency:** The primary brake on a YES resolution is the slow pace of the US legislative process. However, the perceived existential risk and rapid capability gains of AI create a powerful counter-pressure for urgent action. (Driver for NO: large; Driver for YES: moderate).
2.  **A Major AI-Related Catastrophe:** A significant, publicly visible disaster (e.g., a large-scale cyberattack, critical infrastructure failure, or severe disinformation campaign attributed to a frontier model) could shatter legislative inertia and force emergency action, such as an executive order. (Driver for YES: large, but low probability).
3.  **Industry Lobbying for "Regulatory Moats":** As seen in Altman's 2023 testimony, incumbent labs may favor licensing to legitimize their work, manage public perception, and create barriers to entry for new competitors. This provides powerful insider support for regulation. (Driver for YES: moderate).
4.  **Definitional and Jurisdictional Disputes:** Disagreements over what constitutes a "large AI model," what a "license" entails, and whether states or the federal government should have authority can indefinitely stall progress. (Driver for NO: large).
5.  **Implementation Lag:** Even if a law is passed, the time required for a government agency to draft rules, hire staff, and stand up a licensing process is substantial, typically 12-24 months. This makes the pre-2026 deadline extremely challenging. (Driver for NO: very large).

### Differences vs. the base rate (what’s unusual now)
- **Proactive Industry Calls for Regulation:** Unlike most industries that resist regulation, key AI leaders have publicly called for a licensing regime. This is a significant departure from the typical adversarial dynamic and accelerates political consideration.
- **Existential Risk Framing:** The debate is frequently framed in terms of national security and potential existential risk, a far more potent political motivator than typical consumer protection or economic arguments that drove regulations like CCPA.
- **Unprecedented Speed of Technological Advance:** AI capabilities are advancing at a rate that outpaces traditional legislative cycles, creating a unique pressure on policymakers to act preemptively rather than reactively.
- **Concentration in California:** The overwhelming concentration of leading AI labs (OpenAI, Google, Anthropic, Meta) in California makes state-level action uniquely powerful and relevant, increasing the odds of impactful sub-national regulation compared to other industries.

### Bayesian update sketch (for the statistician)
- **Prior:** Based on the reference class of new federal tech regulation, a prior of ~15% seems appropriate, reflecting the high procedural hurdles for creating a new licensing regime on a short timeline.
- **Evidence mapping:**
    - **↑ (Large):** California has passed a landmark AI governance bill (SB 53), demonstrating concrete political will and action, a necessary first step.
    - **↓ (Very Large):** SB 53's implementation date is explicitly January 2026, placing its requirements outside the resolution window. This is the single most powerful piece of evidence against a YES resolution.
    - **↓ (Moderate):** The description of SB 53 focuses on "transparency" and "governance," which may not meet the resolution's definition of a "license or permit to *do* training runs" (i.e., a pre-approval requirement).
    - **↑ (Small):** Continued high-profile statements from industry leaders about rapid AI progress and risk increase the ambient pressure on lawmakers, but this has not yet overcome legislative inertia.
    - **↓ (Moderate):** The continued lack of a viable, fast-moving federal bill makes a US-wide license before 2026 highly improbable.
- **Net effect:** The evidence strongly points to a downward revision of the prior. The passage of SB 53 is initially a positive signal, but its post-deadline implementation date is a decisive negative fact. The posterior probability should be significantly lower than the prior.

### Indicators to watch (leading signals; next weeks/months)
- **UP indicators:**
    - Introduction of emergency legislation in the US Congress or California legislature to accelerate AI oversight.
    - A Presidential Executive Order establishing a temporary or emergency AI licensing body.
    - A major, undeniable AI-caused public incident that galvanizes public opinion.
    - Leaks or reports of a new, far more powerful model (

Market snapshot debug:
- Metaculus: no open binary results in response
- Manifold: best score 0.50 below threshold 0.55 for 'Will the US require purchasers of large numbers of GPUs to report their usage before 2026?'

### Sources
- Sam Altman believes AI will change the world (and everything else) (mitsloan.mit.edu) — https://mitsloan.mit.edu/ideas-made-to-matter/sam-altman-believes-ai-will-change-world-and-everything-else
- OpenAI CEO Sam Altman Agrees AI Must Be Regulated (time.com) — https://time.com/6280376/sam-altman-openai-regulation-congress/
- Governing Frontier AI: California's SB 53 (www.lawfaremedia.org) — https://www.lawfaremedia.org/article/governing-frontier-ai-california-s-sb-53
- The Gentle Singularity (blog.samaltman.com) — https://blog.samaltman.com/the-gentle-singularity
- Sam Altman Just Revealed What's Coming in 2025, 2026 & 2027 (www.youtube.com) — https://www.youtube.com/watch?v=zjkCRgK1p-s
- A Timeline of the Battle for OpenAI: Musk, Altman, and the For-Profit Shift (time.com) — https://time.com/7089791/openai-timeline-musk-altman-nonprofit-restructure/
- Sam Altman Predicts AI Will Surpass Human Intelligence (evrimagaci.org) — https://evrimagaci.org/sam-altman-predicts-ai-will-surpass-human-intelligence-15243
- Frontier AI regulation: what form should it take? (www.emerald.com) — https://www.emerald.com/insight/content/doi/10.1108/JITP-03-2024-0020/full/html

### Research (debug)

- source=GoogleGrounding | llm=google/gemini-2.5-pro | cached=0 | n_raw=10 | n_kept=8 | cost=$0.022814

- query: format; metac_closes_in_period; info; post_id; question_id; Sam Altman; Frontier AI; 2025; 2023; 2026

### GTMC1 (debug)

- strategic_class=False | strategic_score=0.00 | source=llm

- gtmc1_active=False | qtype=binary | t_ms=0

- actors_parsed=0

- exceedance_ge_50=None | coalition_rate=None | median=None | dispersion=None

### Ensemble (model outputs)

- OpenRouter-Default: ok=True t_ms=16455 p=0.0800

- Claude-3.7-Sonnet (OR): ok=True t_ms=29078 p=0.0100

- Gemini: ok=True t_ms=58710 p=0.0200

- Grok: ok=False t_ms=138



### Per-model (raw + usage/cost)

#### OpenRouter-Default

- ok=True | t_ms=16455 | tokens: prompt=3289, completion=589, total=3878 | cost=$0.014113

```md

To forecast the probability of the US or California requiring licenses to train large AI models before 2026, we will follow a structured Bayesian reasoning approach.

1. **Base Rate (Prior) Selection**
   - **Federal Regulation Base Rate:** The base rate for new federal tech regulation on novel risks within a 3-year window is low, around 5-15%, due to legislative gridlock and the time required for agency rulemaking.
   - **California State Regulation Base Rate:** California has a higher base rate for passing tech legislation quickly, estimated at 30-40%, given its history of proactive tech regulation.
   - **Overall Prior:** Considering both federal and state possibilities, a weighted average prior of around 20% seems reasonable, reflecting the higher likelihood of state action.

2. **Comparison to Base Case**
   - **Similarities:** The situation involves novel technology with complex risks, similar to past tech regulations. California's proactive stance aligns with its history of leading tech regulation.
   - **Differences:** The existential risk framing and proactive industry calls for regulation are unique, potentially accelerating the process compared to historical cases.

3. **Evidence Evaluation (Likelihoods)**
   - **California SB 53 Passage (↑, Large):** Demonstrates political will and action, increasing the likelihood of regulation.
   - **SB 53 Implementation Date (↓, Very Large):** The implementation date is set for January 2026, outside the resolution window, significantly decreasing the probability.
   - **Federal Legislative Inertia (↓, Moderate):** No fast-moving federal bill exists, reducing the likelihood of federal action.
   - **Industry Calls for Regulation (↑, Small):** Increases ambient pressure but hasn't overcome legislative inertia.

4. **Bayesian Updating (Posterior Probability)**
   - **Prior Odds:** 20% probability translates to odds of 0.25.
   - **Likelihood Ratio:** The large negative impact of SB 53's implementation date (very large decrease) outweighs the positive signals, suggesting a likelihood ratio less than 1.
   - **Posterior Odds:** Adjusting for the evidence, the posterior odds decrease significantly.
   - **Posterior Probability:** The resulting probability is much lower than the prior, likely around 5-10%.

5. **Red Team Thinking**
   - **Overconfidence Check:** The forecast considers both federal and state actions, with a focus on California's potential for faster regulation.
   - **Tail Risks:** A major AI-related catastrophe could force emergency action, but this is low probability.
   - **Alternative Forecast:** A plausible alternative is a sudden federal executive order, but this remains unlikely given current evidence.

6. **Final Forecast**
   - The final probability reflects the base rate, the significant negative impact of SB 53's timeline, and the lack of federal action.
   - **Final Probability:** 8%

Final: 8%

```

#### Claude-3.7-Sonnet (OR)

- ok=True | t_ms=29078 | tokens: prompt=3722, completion=1549, total=5271 | cost=$0.034401

```md

# Will the US or California require licenses to train large AI models before 2026?

## 1. Base Rate (Prior) Selection

For this question, I need to identify an appropriate reference class and base rate for the likelihood of implementing a licensing regime for AI model training before 2026.

The most relevant reference class is **new regulatory regimes for emerging technologies with potential high-consequence risks**. Looking at historical examples:

- FDA drug approval processes: Took decades to fully establish
- Nuclear power plant licensing: Established over many years following initial development
- Commercial drone regulations: Took ~5 years from recognition to implementation
- Cryptocurrency regulation: Still evolving after more than a decade

Given that the question asks about implementation before December 31, 2025 (approximately 2 months from now), and considering the typical timeline for establishing regulatory frameworks, I'll set a low prior probability.

**Prior probability: 10%**

This reflects the general difficulty of establishing new regulatory regimes quickly, especially for complex technologies.

## 2. Comparison to Base Case

**Similarities to base case:**
- Involves complex technology with uncertain risks
- Requires technical expertise to design effective regulations
- Faces competing stakeholder interests (industry, security, innovation)
- Needs legislative or executive action followed by implementation

**Key differences:**
- Unusually strong industry support for regulation (OpenAI CEO explicitly called for licensing)
- Higher perceived existential risk compared to many other technologies
- Concentrated industry geography (California focus makes state-level action more feasible)
- Extremely rapid technological advancement creating urgency

These differences suggest some upward pressure on the probability, but the extremely short timeframe (2 months remaining) is a severe constraint.

## 3. Evidence Evaluation

**Evidence 1: California passed SB 53 (Transparency in Frontier AI Act)**
- P(E|H): Very high (~0.9) - If licensing were to be implemented, this bill would be a logical precursor
- P(E|~H): Moderate (~0.4) - This bill could exist without leading to licensing by 2026
- Likelihood ratio: ~2.25 (moderately supports hypothesis)
- **However**, critically, the implementation date is January 2026, which is after the resolution date

**Evidence 2: Implementation timeline of SB 53**
- P(E|H): Very low (~0.05) - If licensing were to be implemented before 2026, the bill would need to take effect earlier
- P(E|~H): Very high (~0.95) - A post-2025 implementation date is consistent with no licensing before 2026
- Likelihood ratio: ~0.05 (strongly against hypothesis)

**Evidence 3: Industry leaders' continued calls for regulation**
- P(E|H): High (~0.8) - Industry support is necessary for rapid implementation
- P(E|~H): Moderate (~0.5) - Industry may call for regulation that doesn't materialize quickly
- Likelihood ratio: ~1.6 (slightly supports hypothesis)

**Evidence 4: Lack of federal legislation close to passage**
- P(E|H): Low (~0.1) - Federal licensing would require legislation in advanced stages
- P(E|~H): High (~0.9) - Absence of legislation is consistent with no licensing by deadline
- Likelihood ratio: ~0.11 (strongly against hypothesis)

**Evidence 5: Rapid AI capability advancement creating urgency**
- P(E|H): High (~0.8) - Perceived acceleration would drive faster regulation
- P(E|~H): Moderate (~0.6) - Urgency alone doesn't overcome procedural hurdles
- Likelihood ratio: ~1.33 (slightly supports hypothesis)

## 4. Bayesian Updating

Starting with prior odds of 10:90 (0.11)

First update (SB 53 passage):
- Posterior odds = 0.11 × 2.25 = 0.25
- Posterior probability = 0.25/(1+0.25) ≈ 20%

Second update (SB 53 implementation date after deadline):
- Posterior odds = 0.25 × 0.05 = 0.0125
- Posterior probability = 0.0125/(1+0.0125) ≈ 1.2%

Third update (Industry support):
- Posterior odds = 0.0125 × 1.6 = 0.02
- Posterior probability = 0.02/(1+0.02) ≈ 2%

Fourth update (Lack of federal legislation):
- Posterior odds = 0.02 × 0.11 = 0.0022
- Posterior probability = 0.0022/(1+0.0022) ≈ 0.22%

Fifth update (Urgency from capability advancement):
- Posterior odds = 0.0022 × 1.33 = 0.00293
- Posterior probability = 0.00293/(1+0.00293) ≈ 0.29%

This extremely low probability reflects the overwhelming evidence against implementation before the deadline, particularly the explicit January 2026 implementation date for California's SB 53.

## 5. Red Team Thinking

**Potential blind spots:**
- I may be underestimating the possibility of emergency executive action
- There could be confidential regulatory efforts not visible in public reporting
- A major AI incident in the next two months could trigger expedited action

**Alternative scenarios:**
- Emergency executive order: The President could issue an executive order establishing temporary licensing requirements
- Accelerated implementation: California could move up SB 53's implementation date through emergency legislation
- Reinterpretation of existing authority: A regulatory agency might assert that existing laws give it authority to require licenses

These scenarios are possible but highly unlikely given:
1. The lack of precedent for such rapid regulatory action
2. The explicit January 2026 implementation date in SB 53
3. The complexity of establishing a functional licensing system

Even accounting for these possibilities, I would adjust my estimate only slightly upward, to around 1-2%.

## 6. Final Forecast

The evidence overwhelmingly suggests that neither the US federal government nor California will implement a licensing requirement for training large AI models before December 31, 2025. The most promising path (California's SB 53) explicitly sets its implementation date as January 2026, after the resolution deadline. There is no federal legislation close to passage, and the remaining time (2 months) is insufficient for designing and implementing a functional licensing system.

While there is always a small chance of emergency action in response to a crisis, or accelerated implementation of existing legislation, these possibilities are remote.

Final: 1%

```

#### Gemini

- ok=True | t_ms=58710 | tokens: prompt=3588, completion=1618, total=9359 | cost=$0.020665

```md

Here is a step-by-step probabilistic forecast based on the provided information.

### 1. Base Rate (Prior) Selection
The question asks whether a licensing regime for training large AI models will be required by the US or California before 2026. The most relevant reference class is the speed at which novel, complex technology is regulated.

*   **Federal Regulation:** Creating a new federal licensing regime is a slow, contentious process. Historically, such frameworks take many years to move from concept to implementation. A base rate for this happening in a ~2.5-year window (from mid-2023 to end-2025) would be very low, perhaps 10%.
*   **California Regulation:** California often moves faster on tech regulation than the federal government (e.g., the CCPA). A base rate for California passing and implementing a major new tech regulation in ~2.5 years is significantly higher, perhaps 30-40%.

Since the question is "US *or* California," the California path is the more plausible one. I will select a blended prior that accounts for the high bar of a "licensing regime" but also California's potential for speed.

**Initial Prior: 25%**. This reflects a belief that while such regulation is difficult, the unique circumstances around AI make it more likely than for a typical new technology. In odds, this is 0.25 / 0.75 = 1:3.

### 2. Comparison to Base Case
The current situation with AI differs from typical tech regulation scenarios in several key ways:

*   **Similarities:** The issue involves a complex technology with uncertain risks and benefits, and a high concentration of industry players in California.
*   **Differences:**
    *   **Proactive Industry Calls for Regulation:** Key industry leaders (like Sam Altman) have publicly called for licensing, which is highly unusual and could accelerate the political process.
    *   **Existential Risk Framing:** The debate is often framed around national security and existential risk, which are powerful political motivators that can overcome legislative inertia more effectively than consumer privacy or economic concerns.
    *   **Pace of Progress:** The rapid advancement of AI capabilities creates a sense of urgency for policymakers to act preemptively.

These differences suggest that action could happen faster than the historical base rate would imply.

### 3. Evidence Evaluation (Likelihoods)
The provided research report is dated October 28, 2025. This is the most critical piece of context, as there are only two months remaining until the December 31, 2025 deadline.

*   **Evidence 1: California passed SB 53, the Transparency in Frontier Artificial Intelligence Act (TFAIA).**
    *   This is strong evidence that the political will to regulate AI exists in California. The passage of a landmark bill is a necessary precursor to any regulation.
    *   Likelihood Ratio: P(Bill Passed | License Required) / P(Bill Passed | No License Required) > 1. This evidence, in isolation, **increases** the probability.

*   **Evidence 2: SB 53's implementation date is January 2026.**
    *   This is the single most decisive piece of evidence. The law that has been passed will take effect *after* the resolution deadline.
    *   P(Implementation in Jan 2026 | License Required by Dec 2025) is extremely low. This evidence directly contradicts the "Yes" scenario.
    *   P(Implementation in Jan 2026 | No License Required by Dec 2025) is very high. This is exactly what we would expect if the deadline were not met.
    *   Likelihood Ratio: << 1. This evidence **massively decreases** the probability.

*   **Evidence 3: Lack of a fast-moving federal bill.**
    *   With two months to go, the US Congress passing and implementing a licensing law is practically impossible. This closes one of the two paths (the "US" path) to a "Yes" resolution.
    *   Likelihood Ratio: < 1. This **decreases** the probability.

### 4. Bayesian Updating (Posterior Probability)
Let's update our 25% prior (1:3 odds) with the new evidence.

The evidence of SB 53's implementation date is so strong that it almost completely overrides the prior. The legislative path to a "Yes" resolution by the deadline is now effectively closed. The only remaining possibility is a non-legislative, emergency action.

*   **Prior Odds:** 1:3
*   **Evidence:** A law was passed, but its implementation is after the deadline. This means any "Yes" outcome must come from a *different*, new action in the next two months. The likelihood of needing a *second* legislative or executive action after one has just been finalized is very low.
*   **Likelihood Ratio of Evidence:** The evidence (SB 53's timeline) makes the "Yes" hypothesis extremely unlikely. Let's assign a strong likelihood ratio of 1/50 against the "Yes" case.
*   **Posterior Odds:** (1/3) * (1/50) = 1/150.
*   **Posterior Probability:** (1/150) / (1 + 1/150) = 1/151 ≈ 0.7%.

This calculation shows that based on the legislative facts, the probability is extremely low. The forecast must now account for the only remaining, albeit highly improbable, scenarios.

### 5. Red Team Thinking
My reasoning leads to a very low probability. What could I be missing?

*   **Alternative Scenario: Executive Order.** The most plausible path to a "Yes" is an emergency executive order from the President of the United States or the Governor of California. This would likely only happen in response to a major, undeniable AI-caused catastrophe occurring between now (Oct 28) and the end of the year.
*   **Probability of this Scenario:** This requires two low-probability events:
    1.  A "black swan" AI incident occurs in the next two months. (Let's estimate a 2% chance).
    2.  The immediate political response is an emergency order that constitutes a "license to train" and is implemented before year-end. (Let's estimate a 50% chance, as other responses like investigations or temporary moratoriums are also possible).
*   **Combined Probability:** 2% * 50% = 1%.
*   **Misinterpretation:** Could SB 53 contain some minor, early-activating clause that counts as a "permit"? The report suggests its focus is "transparency," making this unlikely. But it's a source of uncertainty.

Considering the small but non-zero chance of an emergency executive order, a forecast slightly above the ~0.7% calculated from the legislative evidence is warranted. A 1-3% range seems appropriate to capture this tail risk.

### 6. Final Forecast
The timeline is the dominant factor. With only two months until the deadline, the legislative path is closed. The only remaining path is an emergency executive action triggered by a major crisis, which is a very low-probability scenario. The evidence from California's enacted bill (SB 53) points squarely to regulation being implemented in 2026, not 2025.

Final: 2%

```

#### Grok

- ok=False | t_ms=138 | tokens: prompt=0, completion=0, total=0 | cost=$0.000000

- error=HTTP 429: Your team 0101076a-72f1-4f25-a038-b346d74bbc8d has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.

### Aggregation (BMC)

- final_probability=0.0667

- bmc_summary={"mean": 0.0667259355028816, "p10": 4.885740510414407e-06, "p50": 0.009652719032176557, "p90": 0.2163268309576519}