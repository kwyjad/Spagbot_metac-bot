# How many Frontier AI labs will there be on Dec 31, 2025? (QID: 38836)

- Type: numeric

- URL: https://www.metaculus.com/questions/38836/

- Classifier: technology | strategic=False (score=0.00)

### SeenGuard

- enabled=True | lock_status=acquired

- run_filter: before=1 | skipped=0 | after=1

- debug_note=lock acquired

## Research (summary)

CALIBRATION GUIDANCE (auto-generated weekly):
BINARY CALIBRATION
- No resolved binary questions (with pre-resolution forecasts) yet.
- Advice: keep using base rates + small evidence-weighted updates; avoid big swings.

MULTIPLE-CHOICE CALIBRATION (Top-1)
- No resolved MCQ questions (with pre-resolution forecasts) yet.

NUMERIC CALIBRATION (PIT-lite + CRPS)
- No resolved numeric questions (with pre-resolution forecasts) yet.
- When numeric results exist, we’ll check p10/p50/p90 coverage and CRPS.

General takeaway: apply **small** nudges only where gaps are consistent with decent sample sizes (≥10).
— end calibration —

Here is my research plan.

### **Step 1: Identify Core Questions**

1.  **What is the current, definitive list of Frontier AI Labs as of late 2025?** The provided sources are dated up to October 2025, so they should provide a near-final list.
2.  **What is the current compute threshold for a "frontier" model?** The original question used GPT-4's ~2e25 FLOPs as the benchmark in April 2023. I need to estimate the new SOTA compute level for late 2025 to determine the new threshold (SOTA / 10).
3.  **What are the primary barriers to entry and forces of consolidation?** This involves understanding the capital, talent, and data requirements that limit the number of labs.
4.  **What are the primary drivers for expansion or new entrants?** This includes commercial competition, open-source movements, and, critically, national strategic interests.
5.  **Who are the plausible "on the bubble" candidates?** Are there any labs that are close to the threshold and could plausibly announce a qualifying model in the final two months of 2025?
6.  **How does the resolution criteria's reliance on "credible media reporting" or "credible research institution" estimates affect the outcome?** This introduces a dependency on public information and estimation, not just private capabilities.

### **Step 2: Propose Search Queries (Internal Analysis of Provided Sources)**

Since the sources are provided and very recent, my "search" will be an analysis of these documents.

1.  **For Current Labs:**
    *   Scan "AI Frontier Model Builders Cheatsheet (May 2025)" for listed companies.
    *   Scan "State of AI Report 2025 (Oct 2025)" for mentions of leading labs and new competitors.
    *   Scan "THE CALIFORNIA REPORT ON FRONTIER AI POLICY (June 2025)" for companies identified as needing regulation.
2.  **For Compute Threshold:**
    *   Analyze the "State of AI Report 2025" for mentions of new SOTA models (e.g., GPT-5, Gemini 2.0) and any associated FLOP estimates.
    *   If no explicit number is given, I will extrapolate from the historical trend (roughly 10x every 18-24 months) starting from GPT-4's 2e25 FLOPs in early 2023.
3.  **For Barriers/Drivers:**
    *   Synthesize information from all reports. The Cheatsheet's mention of "significant investments" points to capital barriers. The State of AI Report's mention of "intensified competition" and "China's DeepSeek" points to commercial and geopolitical drivers. The California report points to regulatory barriers.
4.  **For "On the Bubble" Candidates:**
    *   Review the Cheatsheet and State of AI Report for mentions of second-tier but rapidly advancing labs (e.g., Mistral, Cohere, Inflection, or sovereign AI labs in UAE/Saudi Arabia).
5.  **For Resolution Criteria:**
    *   This is a meta-analysis. The key is that a lab only counts if its model's compute is *publicly estimated* by a credible source. A secret project does not count.

### **Step 3: Synthesize Key Findings**

*   **Established Labs:** The May 2025 Cheatsheet and June 2025 CA Report confirm a core group of US-based labs: **OpenAI, Google DeepMind, Anthropic, and Meta AI**.
*   **New Entrant:** The October 2025 State of AI Report is the most crucial piece of new information, explicitly noting that **China's DeepSeek** is "closing the gap with leaders." This is strong evidence of a new, non-US lab crossing the frontier threshold.
*   **Compute Threshold Update:** GPT-4 was ~2e25 FLOPs in early 2023. By late 2025 (~32 months later), a 10x increase is highly plausible. A new SOTA model (e.g., GPT-5 or equivalent) is likely in the ~2e26 FLOPs range. This sets the new frontier threshold at **>2e25 FLOPs**. The labs listed above are all but certain to have trained models at this scale.
*   **Other Candidates:** The Hugging Face and NYU Lab sources describe different types of organizations. Hugging Face is a platform/ecosystem, not a frontier model trainer. The NYU lab is an academic consortium, which lacks the state-level funding and compute resources to train a >2e25 FLOP model. Other potential commercial labs (e.g., Mistral) are not mentioned in the recent sources as having reached this specific compute scale, making their inclusion by Dec 31, 2025 unlikely.
*   **Net Count:** The evidence points strongly to a count of 5 labs: the 4 established US players plus the new Chinese entrant, DeepSeek. The short two-month horizon makes additional surprise entrants highly improbable, as training runs of this magnitude take many months to complete and are usually preceded by significant public activity (e.g., chip orders, talent acquisition).

### **Step 4: Draft the Comprehensive Answer**

I will now proceed to write the research brief based on this synthesis, following the required format precisely.

***

### Reference class & base rates
1.  **Oligopolistic High-Tech Industries (e.g., Mainframe Computers, Commercial Airliners, Cloud Providers):** Industries with extremely high R&D and capital expenditure barriers tend to consolidate into a small number of dominant players. This reference class suggests a low number, likely in the **3-7 range**. The dynamics of massive upfront investment and specialized talent pools are highly analogous.
2.  **Leading-Edge Semiconductor Fabs:** The number of companies capable of producing chips at the most advanced nodes (e.g., TSMC, Samsung, Intel) is very small. This is a strong parallel due to the reliance on immense, concentrated capital and cutting-edge physics/engineering. This suggests a base rate in the **2-4 range**.
3.  **Early Question State Trend:** In early 2023, the number of labs was arguably 2 (OpenAI, Google DeepMind), with Anthropic on the cusp. Extrapolating a slow, capital-intensive growth from that base suggests a net addition of 2-3 labs over ~2.5 years, pointing to a **4-5 range**.

### Recent developments (timeline bullets)
*   [2025-10-01] State of AI Report notes China's DeepSeek is "closing the gap" with leaders. — ↑ — Strongest evidence for a new, non-US entrant, directly increasing the lab count.
*   [2025-06-17] California DOJ report on frontier AI policy focuses on OpenAI, Google, and Anthropic. — ↔ — Confirms the core group of established US players but also signals rising regulatory friction, which could deter new entrants.
*   [2025-05-01] AI Frontier Model Builders Cheatsheet lists OpenAI, Google DeepMind, Anthropic, and Meta AI as major players. — ↔ — Confirms Meta's position within the frontier club, solidifying the count of established US labs at four.
*   [2024-05-24] NYU establishes a Global AI Frontier Lab with Korean institutions. — ↓ — Illustrates that not all entities using the "Frontier Lab" name are training SOTA models; academic labs do not meet the compute threshold.
*   [2023-08-26] Hugging Face raises a large round to be the "GitHub of machine learning." — ↓ — Reinforces the distinction between frontier model *trainers* and model/tooling *platforms*. Platforms enable access but don't increase the count of labs training at the frontier scale.

### Mechanisms & drivers (causal levers)
1.  **Extreme Capital Costs:** Training a frontier model now costs hundreds of millions to over a billion dollars in compute alone. This is the primary driver of consolidation and the largest barrier to entry. (Large effect size)
2.  **National Strategic Interest:** AI capability is seen as a critical element of geopolitical power, leading nations (esp. China, and potentially UAE, Saudi Arabia) to fund or subsidize national champions to compete with US labs. This is the primary expansionary force. (Large effect size)
3.  **Commercial Competition:** The race for dominance in cloud computing, search, and enterprise AI software provides massive commercial incentives for tech giants (Google, Meta, Microsoft) to fund frontier research. (Large effect size)
4.  **Talent Concentration:** The world's top AI research talent is concentrated in a handful of existing labs, creating a significant hiring barrier for any new potential entrant. (Moderate effect size)
5.  **Compute & Supply Chain Bottlenecks:** Access to the vast GPU clusters required for frontier training is a major constraint, limited by chip supply (from e.g., NVIDIA) and the engineering expertise to build and manage the infrastructure. (Moderate effect size)
6.  **Regulatory Scrutiny:** Increasing policy focus on the risks of frontier models (per the CA report) adds compliance costs and potential liability, slightly disincentivizing new entrants who are not already

Market snapshot debug:
- Metaculus: no open binary results in response
- Manifold: no open binary results in response

### Sources
- AI Frontier Model Builders Cheatsheet (Updated May 2025) (lifearchitect.ai) — https://lifearchitect.ai/models/
- State of AI Report 2025 (www.stateof.ai) — https://www.stateof.ai/
- THE CALIFORNIA REPORT ON FRONTIER AI POLICY (oag.ca.gov) — https://oag.ca.gov/system/files/media/crai-final-report-2025.pdf
- Democratizing AI: The Hugging Face Ethos of Accessible Machine Learning - Turing Post (www.turingpost.com) — https://www.turingpost.com/p/democratizing-ai-hugging-face-ethos
- Global AI Frontier Lab - NYU (www.nyu.edu) — https://www.nyu.edu/research/global-ai-frontier-lab.html

### Research (debug)

- source=GoogleGrounding | llm=google/gemini-2.5-pro | cached=0 | n_raw=12 | n_kept=5 | cost=$0.023492

- query: format; metac_closes_in_period; info; post_id; question_id; Frontier AI Lab; Frontier AI; Frontier Labs; Google Deepmind; Hugging Face; 2023; 2025

### GTMC1 (debug)

- strategic_class=False | strategic_score=0.00 | source=llm

- gtmc1_active=False | qtype=numeric | t_ms=0

- actors_parsed=0

- exceedance_ge_50=None | coalition_rate=None | median=None | dispersion=None

### Ensemble (model outputs)

- OpenRouter-Default: ok=True t_ms=11672 P10=31.000, P50=2025.000, P90=1.000

- Claude-3.7-Sonnet (OR): ok=True t_ms=21051 P10=31.000, P50=2025.000, P90=1.000

- Gemini: ok=True t_ms=52590 P10=29.000, P50=2025.000, P90=31.000

- Grok: ok=False t_ms=205



### Per-model (raw + usage/cost)

#### OpenRouter-Default

- ok=True | t_ms=11672 | tokens: prompt=3446, completion=529, total=3975 | cost=$0.013905

```md

To forecast the number of Frontier AI labs on December 31, 2025, we'll follow a structured Bayesian approach.

### 1. Base Rate (Prior) Selection
- **Reference Class**: Oligopolistic high-tech industries and leading-edge semiconductor fabs are appropriate reference classes due to their high barriers to entry and capital intensity.
- **Base Rate**: Historically, the number of labs has been small, with 2-3 major players in 2023. Given the slow growth and high capital requirements, a base rate of 4-5 labs by 2025 is reasonable.

### 2. Comparison to Base Case
- **Similarities**: The AI industry remains capital-intensive with high barriers to entry, similar to the reference classes.
- **Differences**: Increased geopolitical interest and strategic investments, particularly from China, could lead to more entrants than in typical oligopolistic industries.

### 3. Evidence Evaluation (Likelihoods)
- **DeepSeek's Emergence**: Strong evidence of a new entrant, increasing the likelihood of more labs.
- **Regulatory and Capital Barriers**: These factors maintain a high barrier to entry, limiting the number of new labs.
- **Commercial Competition**: Incentivizes existing tech giants to maintain or increase their presence in frontier AI research.

### 4. Bayesian Updating (Posterior Distribution)
- **Prior**: 4-5 labs based on historical trends and industry characteristics.
- **Likelihood**: DeepSeek's emergence and geopolitical factors suggest a slight increase in the number of labs.
- **Posterior**: Adjust the prior upwards slightly to account for new entrants, resulting in a forecast of 5-6 labs.

### 5. Red Team Thinking
- **Overconfidence Check**: Consider the possibility of unforeseen entrants or exits due to regulatory changes or market dynamics.
- **Alternative Scenarios**: A scenario where regulatory barriers significantly deter new entrants could lower the count.
- **Rebuttal**: The strong evidence of DeepSeek's emergence and the strategic importance of AI suggest that the forecast is robust.

### 6. Final Percentiles
- **P10**: 4 labs
- **P20**: 5 labs
- **P40**: 5 labs
- **P60**: 6 labs
- **P80**: 6 labs
- **P90**: 7 labs

P10: 4  
P20: 5  
P40: 5  
P60: 6  
P80: 6  
P90: 7

```

#### Claude-3.7-Sonnet (OR)

- ok=True | t_ms=21051 | tokens: prompt=3920, completion=1029, total=4949 | cost=$0.027195

```md

# Forecasting the Number of Frontier AI Labs by Dec 31, 2025

## 1. Base Rate (Prior) Selection

For my base rate, I'll use the historical progression of frontier AI labs from 2023 to late 2025:
- As of April 2023: 2-3 labs (OpenAI, Google DeepMind, possibly Anthropic)
- As of May 2025: 4 confirmed labs (OpenAI, Google DeepMind, Anthropic, Meta AI)
- As of October 2025: 5 likely labs (the above plus China's DeepSeek "closing the gap")

I'll also consider the reference class of oligopolistic high-tech industries with extreme capital requirements, which typically sustain 3-7 major players.

The base rate distribution has a median of 5 labs, with a reasonable range of 4-7 labs.

## 2. Comparison to Base Case

**Similarities to base rate:**
- The extreme capital costs (~$1B+ per frontier model) continue to be a major barrier to entry
- Talent concentration remains a significant constraint
- Compute access limitations persist through 2025
- Regulatory scrutiny creates compliance costs that favor established players

**Differences from base rate:**
- National strategic interests have intensified, with China's DeepSeek now at the frontier
- The compute threshold has likely increased (from ~2e25 FLOPs in 2023 to ~2e26 FLOPs for SOTA, making the frontier threshold ~2e25 FLOPs)
- Only two months remain until resolution, limiting the possibility of surprise entrants

## 3. Evidence Evaluation

**Evidence for higher counts:**
- China's DeepSeek is explicitly mentioned as "closing the gap" in the October 2025 State of AI Report (+1 lab)
- National strategic interests could drive other countries to fund frontier labs (moderate likelihood)
- The May 2025 cheatsheet confirms Meta AI as a frontier player (already counted)

**Evidence for lower counts:**
- The California DOJ report (June 2025) focuses only on OpenAI, Google, and Anthropic, suggesting Meta might not be universally considered at the frontier
- Academic consortia like NYU's Global AI Frontier Lab lack the resources to train frontier models
- Platforms like Hugging Face enable access but don't train frontier models themselves

## 4. Bayesian Updating

Starting with my prior centered at 5 labs with a range of 4-7:

**Update on DeepSeek evidence:**
- The October 2025 State of AI Report provides strong evidence for DeepSeek joining the frontier
- This shifts my distribution slightly upward, reinforcing 5 as the most likely value

**Update on regulatory environment:**
- The California report suggests increasing regulatory friction
- This makes new entrants in the final two months less likely
- This narrows my distribution, reducing the probability of values above 6

**Update on time constraints:**
- With only two months until resolution, surprise entrants are unlikely
- Training runs at frontier scale take many months to complete
- This further narrows my distribution around 5

My posterior distribution now has a stronger peak at 5, with reduced probability mass at the tails.

## 5. Red Team Thinking

**Potential blind spots:**
- I may be underestimating the pace of progress at labs like Mistral, Cohere, or Inflection
- I haven't fully accounted for sovereign wealth fund investments (UAE, Saudi Arabia)
- Microsoft's independent efforts (separate from OpenAI) could qualify them as a distinct lab
- The resolution criteria depends on public estimation, which might miss secret capabilities

**Alternative scenario:**
A plausible alternative forecast would center on 6-7 labs, arguing that:
- Mistral or another European lab could announce a qualifying model
- A sovereign AI initiative (e.g., UAE's Technology Innovation Institute) could cross the threshold
- Microsoft might be counted separately from OpenAI
- Another Chinese competitor beyond DeepSeek might emerge

**Rebuttal:**
The short timeframe (2 months) makes these scenarios less likely, as frontier model training runs are typically known months in advance through supply chain signals and talent movements.

I'll widen my distribution slightly to account for these possibilities.

## 6. Final Percentiles

P10: 4
P20: 4
P40: 5
P60: 5
P80: 6
P90: 7

```

#### Gemini

- ok=True | t_ms=52590 | tokens: prompt=3744, completion=1340, total=9294 | cost=$0.018080

```md

Answering based on the provided research and context up to October 29, 2025, for a resolution on December 31, 2025.

### 1. Base Rate (Prior) Selection
The most appropriate reference class is oligopolistic, high-tech industries with extremely high capital barriers to entry. Examples include leading-edge semiconductor fabrication (TSMC, Samsung, Intel) or commercial airliner manufacturing (Boeing, Airbus). These industries typically support only a handful of players (2-5) due to immense R&D costs, specialized talent requirements, and massive capital expenditure.

Based on this, my prior distribution for the number of frontier AI labs is centered on a small integer.
- **Prior Median:** 4 labs.
- **Prior Spread:** A plausible range might be 3 to 7 labs (representing my P10 and P90), acknowledging that while consolidation is powerful, the geopolitical "AI race" could artificially support more players than a purely commercial market.

### 2. Comparison to Base Case
- **Similarities:** The development of frontier AI models perfectly mirrors the reference class. Training runs cost hundreds of millions to billions of dollars, require scarce world-class talent, and depend on a constrained supply chain for compute (i.e., high-end GPUs). These factors strongly favor consolidation.
- **Differences:** The primary difference is the intense geopolitical motivation. Unlike commercial jets, AI is considered a critical national security asset. This creates a powerful force for expansion, as nations like China, and potentially others (e.g., UAE, Saudi Arabia), will fund national champions to compete, even if it's not commercially optimal. This suggests the final number could be slightly higher than a pure commercial oligopoly.

### 3. Evidence Evaluation (Likelihoods)
The provided research is very recent, allowing for a significant update from the prior.

- **Evidence for a floor of 4:** The "AI Frontier Model Builders Cheatsheet" (May 2025) and the "California Report" (June 2025) firmly establish a core group of four US-based labs operating at or near the frontier: **OpenAI, Google DeepMind, Anthropic, and Meta AI**. This evidence is very strong and makes outcomes below 4 highly unlikely. This dramatically shifts the distribution's mass to ≥4.

- **Evidence for a 5th lab:** The "State of AI Report 2025" (October 2025) is the most critical piece of new information. It explicitly states that **China's DeepSeek** is "closing the gap with leaders." Given the report's timing (just two months before resolution) and the geopolitical driver of China's AI ambitions, this is a powerful signal that a fifth lab has crossed the frontier threshold. This evidence provides a strong likelihood ratio in favor of N=5 over N=4.

- **Evidence against more than 5 labs:** The same recent reports *do not* mention other contenders (e.g., Mistral, Cohere, other sovereign AI labs) in the same "closing the gap" context. Training a frontier model takes many months and massive, often public, resource acquisition. With only two months remaining, a complete surprise from a lab not already on the radar of major industry reports is highly improbable. This evidence trims the right tail of the distribution, making N≥7 very unlikely.

- **The rising compute threshold:** The SOTA compute has likely increased by ~10x since GPT-4 (2e25 FLOPs) in early 2023. The new threshold for a "frontier" lab is likely >2e25 FLOPs. This higher bar reinforces the consolidation effect, making it harder for new entrants to qualify and strengthening the case for a small number of labs.

### 4. Bayesian Updating (Posterior Distribution)
- **Prior:** Median of 4, with a wide tail up to 7.
- **Update 1 (Confirming 4 US labs):** The evidence for OpenAI, Google, Anthropic, and Meta is overwhelming. This truncates the left tail, making P(N<4) negligible. The distribution shifts right, with a mode at 4.
- **Update 2 (DeepSeek evidence):** The strong, recent signal for DeepSeek pushes the entire distribution further right. The likelihood of N=5 is now significantly higher than N=4. My posterior median shifts from 4 to 5.
- **Posterior Shape:** The final distribution is now sharply peaked at 5. The main source of remaining uncertainty is whether DeepSeek's model compute will be *credibly estimated* by a qualifying source by the deadline. This justifies a significant probability mass on N=4 as the primary alternative. There is a smaller, but non-trivial, probability of a sixth entrant (e.g., another state-backed lab), justifying some mass at N=6.

### 5. Red Team Thinking
- **Alternative Forecast (Higher Count):** A plausible alternative is that another Chinese lab (e.g., Zhipu AI) or a well-funded sovereign AI lab (e.g., in the UAE) makes a surprise announcement in the final two months that receives a credible compute estimate. The geopolitical race is accelerating, and secret projects could exist. This is the primary justification for a P90 of 6.
- **Alternative Forecast (Lower Count):** The "State of AI Report" says DeepSeek is "closing the gap," which is not definitive proof they have a qualifying model whose compute has been *publicly estimated*. If no credible source publishes a FLOP estimate for their latest model by Dec 31, they would not count, and the total would revert to 4. This is the most significant downside risk and why my distribution places substantial weight on N=4.
- **Conclusion:** My forecast is confident but not certain. The distribution is tight due to the short time horizon. The central outcome is 5, the main downside is 4, and the main upside is 6. The percentiles should reflect this structure.

### 6. Final Percentiles

P10: 4
P20: 4
P40: 5
P60: 5
P80: 5
P90: 6

```

#### Grok

- ok=False | t_ms=205 | tokens: prompt=0, completion=0, total=0 | cost=$0.000000

- error=HTTP 429: Your team 0101076a-72f1-4f25-a038-b346d74bbc8d has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.

### Aggregation (BMC)

- final_quantiles: P10=-983.606336920295, P50=29.553700756929178, P90=1055.3603479245241

- bmc_summary={"mean": 34.68383673238066, "p10": -983.606336920295, "p50": 29.553700756929178, "p90": 1055.3603479245241}