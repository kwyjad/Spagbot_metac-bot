# Will the US restrict access outside the US to some APIs to generative AI before 2026? (QID: 38996)

- Type: binary

- URL: https://www.metaculus.com/questions/38996/

- Classifier: technology | strategic=False (score=0.00)

### SeenGuard

- enabled=True | lock_status=acquired

- run_filter: before=2 | skipped=0 | after=2

- debug_note=lock acquired

## Research (summary)

CALIBRATION GUIDANCE (auto-generated weekly):
BINARY CALIBRATION
- No resolved binary questions (with pre-resolution forecasts) yet.
- Advice: keep using base rates + small evidence-weighted updates; avoid big swings.

MULTIPLE-CHOICE CALIBRATION (Top-1)
- No resolved MCQ questions (with pre-resolution forecasts) yet.

NUMERIC CALIBRATION (PIT-lite + CRPS)
- No resolved numeric questions (with pre-resolution forecasts) yet.
- When numeric results exist, we’ll check p10/p50/p90 coverage and CRPS.

General takeaway: apply **small** nudges only where gaps are consistent with decent sample sizes (≥10).
— end calibration —

Here is the research brief for the Bayesian forecasting panel.

***

### **Reference class & base rates**

1.  **US Export Controls on Dual-Use Software (e.g., Cryptography):** The most relevant analogy is the "Crypto Wars" of the 1990s, where the US government classified strong encryption as a "munition" and heavily restricted its export. This policy was eventually relaxed due to economic pressure, the widespread availability of foreign alternatives, and the impracticality of enforcement. The base rate of a new, widely available software class facing such restrictions and *maintaining* them is low.
    *   **Base Rate Estimate:** 5-10%. This reflects that such controls were attempted but ultimately failed/were reversed for the closest software analogue.
2.  **Export Controls on Strategic Hardware/Technology (e.g., Semiconductors):** A more recent and successful reference class is the US Commerce Department's restrictions on the sale of advanced semiconductor technology to specific countries, notably China. This is driven by national security and aims to slow a competitor's technological progress. It demonstrates a modern willingness to use targeted tech controls.
    *   **Base Rate Estimate:** 25-35%. This reflects a higher probability for targeted, hardware-adjacent restrictions against specific adversaries, which could be a model for restricting access to the *compute* that runs models, or the models themselves.
3.  **Sanctions Regimes on Digital Services:** The US routinely prohibits companies from providing services (including software and APIs) to sanctioned countries like Iran, North Korea, and Cuba. This is a standard tool of foreign policy. However, the question asks about a broader restriction *outside the US*, not just to sanctioned entities.
    *   **Base Rate Estimate:** <1%. The base rate for applying a sanctions-style digital blockade to non-adversarial nations or the entire world ex-US is extremely low.

**Limitations:** Generative AI is novel. It combines the scalability of software (Class 1) with the strategic, concentrated power of advanced hardware (Class 2), making a direct comparison difficult. The speed of capability improvement is also unprecedented.

### **Recent developments (timeline bullets)**

*   **2024-03-04** Anthropic releases Claude 3, with the most powerful model (Opus) outperforming GPT-4 on key benchmarks, demonstrating continued rapid capability gains and a competitive landscape. (↑) — Increases perceived power and potential risk, motivating regulatory scrutiny.
*   **2024-01-26** The Commerce Department proposes a rule requiring US IaaS (cloud) providers to verify the identity of foreign customers who train large AI models. (↑↑) — A direct and significant regulatory step towards monitoring and potentially controlling foreign access to AI-enabling infrastructure.
*   **2023-10-30** President Biden signs the "Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence." (↑↑) — Invokes the Defense Production Act, requiring developers of the most powerful models to report safety tests to the government, signaling a strong federal interest in oversight and control.
*   **2023-10-17** The Biden administration expands and tightens export controls on advanced AI chips to China, aiming to close loopholes from a previous order. (↑) — Reinforces the administration's strategy of using technology controls as a primary tool in its competition with China.
*   **2023-07-13** Major AI labs (OpenAI, Google, Anthropic) make voluntary commitments to the White House on AI safety, including watermarking and third-party testing. (↑/↓) — Can be seen as a step towards regulation (↑) or an attempt by industry to self-regulate to *stave off* harsher legal restrictions (↓).
*   **2023-07-13** Google expands Bard access to more countries, including the EU, demonstrating a strong commercial push for global proliferation. (↓) — Highlights the powerful economic incentives against restricting access.

### **Mechanisms & drivers (causal levers)**

1.  **Catastrophic Misuse Risk (National Security):** The primary driver for restrictions. A credible demonstration or threat of AI-assisted bioterrorism, advanced cyberattacks, or autonomous weapons development would be a powerful catalyst for immediate, drastic action. (Size: Large)
2.  **US-China Strategic Competition:** The desire to maintain a technological lead over China and prevent adversaries from accessing cutting-edge capabilities is a core tenet of current US policy. This could lead to restrictions aimed at China that inadvertently sweep in other countries. (Size: Moderate-Large)
3.  **Economic Competitiveness & "Soft Power":** US AI companies are global leaders. Restricting API access would cede the global market to foreign competitors (e.g., from Europe, China, or the UAE) and diminish US influence, a powerful argument against restrictions. (Size: Large)
4.  **Proliferation of Capable Open-Source Models:** The existence of powerful, unrestricted open-source models (e.g., Llama series, Mistral) complicates the policy landscape. If foreign actors can access near-SOTA capabilities anyway, restricting US APIs may be seen as ineffective ("leaky bucket") and economically self-harming. (Size: Moderate)
5.  **Technical & Administrative Feasibility:** Implementing and enforcing a global API restriction is non-trivial (e.g., VPNs, front companies). The administrative burden on both government and industry would be significant, creating friction against implementation. (Size: Small-Moderate)
6.  **Allied Alignment:** The US prefers to act in concert with allies (e.g., G7, EU). A unilateral restriction could cause diplomatic friction. The EU's AI Act, for example, focuses on risk-based usage regulations, not geographic access controls. A desire for a multilateral approach could slow or prevent a US-only ban. (Size: Small-Moderate)

### **Differences vs. the base rate (what’s unusual now)**

*   **Concentration of Power:** Unlike cryptography, which was a diffuse technology, the most powerful "frontier" AI models are currently controlled by a handful of US companies. This makes targeted regulation technically easier to implement (compel 3-5 companies).
*   **Speed of Capability Gains:** AI capabilities are advancing much faster than encryption did in the 1990s. The policy cycle is struggling to keep up, which could lead to more reactive, heavy-handed regulation in response to a sudden new capability or incident.
*   **General-Purpose Nature:** The models are not just a single tool (like encryption) but a potential component in *any* digital process, amplifying both their economic potential and their potential for misuse across countless domains.
*   **Early & Strong Open-Source Alternatives:** In the Crypto Wars, it took time for viable foreign/open alternatives to emerge. In AI, very powerful open-source models appeared within a year of the technology's breakout, providing a strong counter-argument to the effectiveness of restricting closed models.
*   **Heightened Geopolitical Tensions:** The current climate of US-China strategic competition is far more intense than the post-Cold War environment of the 1990s, making national security arguments for control more potent and politically resonant.

### **Bayesian update sketch (for the statistician)**

*   **Prior:** Starting from a blend of the "Crypto Wars" and "Semiconductor Controls" reference classes, a prior of ~20% seems reasonable. This acknowledges the historical rarity of such software controls but accounts for the heightened security environment.
*   **Evidence mapping:**
    *   (↑↑ Large) Biden's AI Executive Order and the Commerce Dept's IaaS proposal are concrete, significant moves toward a federal regulatory and monitoring regime for frontier models.
    *   (↑ Moderate) Expanded chip controls reinforce the policy direction of using technology denial as a strategic tool.
    *   (↓ Moderate) The strong commercial push for global market share by Google, OpenAI, etc., creates a powerful industry lobby against restrictions.
    *   (↓ Moderate) The rapid improvement of open-source models undermines the efficacy of restricting only a few US-based APIs.
*   **Net effect:** The evidence from the last 18 months, particularly direct executive and departmental actions, points strongly towards increased regulation. The prior of 20% should be revised significantly upward. The countervailing forces are strong but are currently being outpaced by the national security narrative within the US government.

### **Indicators to watch (leading signals; next weeks/months)**

*   **UP indicators:**
    *   The Commerce Department finalizing its rule requiring IaaS providers to report on foreign AI model training.
    *   A specific, credible public report from the intelligence community (e.g., DNI, CIA) on foreign misuse of US models.
    *   Introduction of legislation with bipartisan support that explicitly mentions "know-your-customer" (KYC) or access controls for frontier model APIs.
    *   A major AI company (e.g., Anthropic) voluntarily implementing stricter geographic API controls as part of its safety policy.
    *   A significant AI-implicated security incident (e.g., novel cyberattack, disinformation campaign) attributed to foreign actors.
*   **DOWN indicators:**
    *   Major US AI labs announcing new research/development hubs in Europe or Asia.
    *   The Commerce Department's proposed IaaS rule being significantly weakened or delayed after industry pushback.
    *   The EU AI Act becoming the de facto global standard, with US companies prioritizing compliance with its use-case framework over access controls.
    *   The next generation of open-source models achieving clear performance parity with the best closed models.
    *   A major US-allied nation (e.g., UK, France) publicly opposing the idea of geographic AI restrictions.

### **Caveats & pitfalls**

*   **Definitional Ambiguity:** The resolution hinges on "powerful generative AI systems." This threshold is constantly moving. A law might be passed that only affects a future, hypothetical "GPT-6" class model, which may or may not be developed by 2026.
*   **"Leaky" Restrictions:** The law might only restrict access from a specific list of countries (e.g., the existing sanctions list + China), which might not meet the spirit of the question but could technically resolve it YES. The resolution criteria are broad ("restrict API access to only people in the US").
*   **Focusing Event/Black Swan:** A single, major AI-related event (e.g., a "Sputnik moment" for China's AI, or a catastrophic misuse event) could dramatically and unpredictably shift the political landscape, overriding all existing mechanisms and trends.
*   **Symbolic vs. Substantive Law:** A law could be passed that is largely symbolic and easily circumvented (e.g., requiring a checkbox attesting to US residency). Whether this counts as a "restriction" is a key ambiguity. The precedent of Italy's temporary ChatGPT ban, which didn't affect APIs, highlights this.
*   **Lobbying Power:** The immense financial and political power of the handful of companies controlling frontier models is a major uncertainty. Their ability to influence legislation behind the scenes is high but difficult to observe.

**Final Research Summary:** The probability of US API restrictions has substantially increased due to a clear shift towards a national security-focused governance framework, evidenced by executive orders and Commerce Department proposals. However, powerful economic incentives for global expansion and the proliferation of open-source alternatives present significant, persistent headwinds to implementing such a policy.

### Market Snapshots (community forecasts)
- **Manifold**: [Will the US restrict access outside the US to some APIs to generative AI before 2026?](https://manifold.markets/MetaculusBot/will-the-us-restrict-access-outside) — Community forecast: 36.0%

Market snapshot debug:
- Metaculus: no open binary results in response
- Manifold: matched 'Will the US restrict access outside the US to some APIs to generative AI before ' (score 1.00, 36.0%)
- Market snapshots: found manifold

### Sources
- AI Trends in 2026: A Visionary Yet Grounded Forecast (vertexaisearch.cloud.google.com) — https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFMBIsrsBhubxGgcC6iiyH8OgkkYulOy-64cBY6jiayN7T0nAtr386qItOyPLjixi5YEEwhISWGfjziucSfcvmaMjzgfKdVd2vZQ0xJ1-Gwe3A4rL1EA9r_1GZvAwQC57wgTQ==
- How Open LLMs Are Revolutionizing AI by 2026 (vertexaisearch.cloud.google.com) — https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHeiWqtiPyEe2TDB0NlVOblpeuoG88Q3Wu4rLtl30LEPKUPEaeAace2TPD28-AElYaN2J1kpcITo_en4IcAGVSLWK_ne0JHaYsKOPKAXmvs2NEjHEMdbWvv5jBD-OCiPVODBys599oOhL8yWgybQb6SI52AjrWuuHZALot4SxTnIBGWfPG9iipXVrR3uBxyOdE=
- Google Bard AI July 2023 update brings new features and expands globally (vertexaisearch.cloud.google.com) — https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGXv7oxJ1i9jR8U7aJhM_Pe4VPw-shnMztnlMejhGGOGufPMaz0_81NcqovdsNBXB_3MfKCFf6Zq7L0BAFc9656_IIxxkTewDoYAn_GC2cBbaMv169izjEWZNb_U3F81r2hyQHxRhHLpWwXVb4qjXvkWFDGPf6bWIgxBQkVGSkVQ1Y=
- 7 Exciting Bard AI Features Announced at Google I/O 2023 (vertexaisearch.cloud.google.com) — https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGWMmeSkeOjIYVeP9TaLqoCkUhxlEjVvusoMkOW5k2pX-4lfPXXObwZlFwtyw6nusPq9ZBgAmJHhpLqY132N7eGUqwT55ZY7Oz1gx9fKMDnD4D74a8OyG3sykZRIoaOAUICRqZ41_q2vbGOTiyHsBBusYy8w8WuIM0rPeFUwF8=

### Research (debug)

- source=GoogleGrounding | llm=google/gemini-2.5-pro | cached=0 | n_raw=6 | n_kept=4 | cost=$0.032585

- query: format; metac_closes_in_period; info; post_id; question_id; Large Language Models; Google Bard; Deep Fakes; Digital Rights Management; Prime Video; In April; 2026; 2023; 2025

### GTMC1 (debug)

- strategic_class=False | strategic_score=0.00 | source=llm

- gtmc1_active=False | qtype=binary | t_ms=0

- actors_parsed=0

- exceedance_ge_50=None | coalition_rate=None | median=None | dispersion=None

### Ensemble (model outputs)

- OpenRouter-Default: ok=True t_ms=10744 p=0.4000

- Claude-3.7-Sonnet (OR): ok=True t_ms=37603 p=0.4200

- Gemini: ok=True t_ms=58928 p=0.0600

- Grok: ok=False t_ms=73



### Per-model (raw + usage/cost)

#### OpenRouter-Default

- ok=True | t_ms=10744 | tokens: prompt=4394, completion=575, total=4969 | cost=$0.016735

```md

To forecast the probability of the US restricting access outside the US to some APIs for generative AI before 2026, we will follow the structured Bayesian reasoning process:

1. **Base Rate (Prior) Selection**
   - The base rate for US export controls on dual-use software, like cryptography, is estimated at 5-10% due to historical attempts that were eventually relaxed. For strategic hardware/technology, like semiconductors, the base rate is 25-35% due to successful restrictions driven by national security concerns. Sanctions regimes on digital services have a base rate of <1% for broad restrictions outside the US. Given the unique nature of generative AI, a blended prior of ~20% is reasonable, acknowledging both the rarity of software controls and the current security environment.

2. **Comparison to Base Case**
   - Similarities: The situation is similar to past export controls where national security was a driving factor, such as semiconductor restrictions.
   - Differences: Generative AI is novel, with rapid capability gains and concentrated control by a few US companies, making regulation technically easier but economically challenging due to global market dynamics.

3. **Evidence Evaluation (Likelihoods)**
   - **Biden's AI Executive Order and Commerce Dept's IaaS proposal**: These are strong indicators of a regulatory shift towards monitoring and controlling AI, increasing the likelihood of restrictions (Likelihood ratio > 1).
   - **Expanded chip controls**: Reinforce the policy direction of using technology denial as a strategic tool, increasing the probability (Likelihood ratio > 1).
   - **Commercial push by AI companies**: Strong economic incentives against restrictions, decreasing the probability (Likelihood ratio < 1).
   - **Open-source model proliferation**: Undermines the effectiveness of restricting US-based APIs, decreasing the probability (Likelihood ratio < 1).

4. **Bayesian Updating (Posterior Probability)**
   - Starting with a prior of 20%, the evidence from executive actions and proposals suggests a significant upward revision. However, countervailing forces like economic incentives and open-source alternatives moderate this increase.
   - Posterior odds = Prior odds × Likelihood ratio (considering all evidence).
   - Posterior probability is adjusted upwards but remains balanced by strong opposing factors.

5. **Red Team Thinking**
   - Overconfidence risk: The forecast must account for potential overestimation of regulatory actions due to political and economic pushback.
   - Tail risks: A major AI-related security incident could drastically shift probabilities.
   - Alternative forecast: A lower probability could be plausible if industry lobbying and open-source advancements continue to outpace regulatory efforts.

6. **Final Forecast**
   - Considering the evidence and potential counterarguments, the final probability reflects a moderate increase from the prior, acknowledging both regulatory momentum and significant opposing forces.

Final: 40%

```

#### Claude-3.7-Sonnet (OR)

- ok=True | t_ms=37603 | tokens: prompt=5020, completion=2030, total=7050 | cost=$0.045510

```md

# Will the US restrict access outside the US to some APIs to generative AI before 2026?

## 1. Base Rate (Prior) Selection

For this question, I need to identify an appropriate base rate for the US implementing geographic restrictions on AI API access. The most relevant reference classes are:

1. **US Export Controls on Dual-Use Software (Cryptography)**: In the 1990s, the US classified strong encryption as a "munition" and restricted its export, but these restrictions were eventually relaxed due to economic pressure and practical enforcement challenges. Base rate: ~10%

2. **US Export Controls on Strategic Hardware/Technology (Semiconductors)**: More recently, the US has implemented restrictions on advanced semiconductor technology to China. This demonstrates willingness to use targeted tech controls for national security. Base rate: ~30%

3. **Sanctions Regimes on Digital Services**: The US routinely prohibits companies from providing digital services to sanctioned countries, but rarely implements broad geographic restrictions. Base rate: ~1% for broad restrictions

Given the unique nature of generative AI (combining software scalability with strategic importance), I'll use a weighted blend of these reference classes, with more weight on the semiconductor case as it's more recent and strategically similar. 

**Initial prior: 20%**

This reflects that while such restrictions are relatively rare, the strategic importance of AI and recent policy trends make them more plausible than historical software restrictions would suggest.

## 2. Comparison to Base Case

**Similarities to base case:**
- Like encryption and semiconductors, generative AI has dual-use potential (civilian and military/security applications)
- The US has demonstrated willingness to restrict technology access for national security reasons
- There are economic incentives against restrictions (US companies want global markets)

**Key differences:**
- Concentration of power: Unlike encryption, frontier AI models are controlled by a handful of US companies, making regulation technically easier
- Speed of capability gains: AI is advancing much faster than previous technologies, potentially outpacing normal regulatory cycles
- General-purpose nature: AI models can be applied to virtually any domain, amplifying both economic benefits and misuse risks
- Strong open-source alternatives: Powerful open-source models emerged quickly, potentially undermining the effectiveness of API restrictions
- Heightened geopolitical tensions: US-China competition is more intense now than during the Crypto Wars, strengthening national security arguments

These differences, particularly the concentration of power and heightened security concerns, suggest the probability should be adjusted upward from historical software restrictions.

## 3. Evidence Evaluation (Likelihoods)

Let's evaluate key pieces of evidence:

**Evidence 1: Biden's Executive Order on AI (October 2023)**
- P(E|H): Very high (~90%) - If the US plans to restrict API access, an executive order establishing government oversight of AI would be a logical first step
- P(E|~H): Moderate (~40%) - The EO could be focused on domestic regulation without geographic restrictions
- Likelihood ratio: ~2.25
- This evidence increases the probability of restrictions

**Evidence 2: Commerce Department's proposed rule requiring IaaS providers to verify foreign customers' identities (January 2024)**
- P(E|H): Very high (~95%) - This is a direct step toward monitoring and potentially controlling foreign access
- P(E|~H): Low (~20%) - Such identity verification would be less necessary without plans for geographic restrictions
- Likelihood ratio: ~4.75
- This evidence significantly increases the probability

**Evidence 3: Expanded chip controls against China (October 2023)**
- P(E|H): High (~80%) - Consistent pattern of using technology controls as a strategic tool
- P(E|~H): Moderate (~50%) - Could reflect a hardware-only approach rather than software/API restrictions
- Likelihood ratio: ~1.6
- This evidence moderately increases the probability

**Evidence 4: Commercial expansion of AI services globally (e.g., Google Bard expansion)**
- P(E|H): Low (~30%) - Companies would be less likely to invest in global expansion if they anticipated restrictions
- P(E|~H): Very high (~90%) - Global expansion is the expected business strategy without restrictions
- Likelihood ratio: ~0.33
- This evidence decreases the probability

**Evidence 5: Rapid improvement of open-source models**
- P(E|H): Moderate (~50%) - Restrictions might still be implemented despite limited effectiveness
- P(E|~H): High (~80%) - The government would be less likely to impose restrictions it knows can be circumvented
- Likelihood ratio: ~0.63
- This evidence moderately decreases the probability

## 4. Bayesian Updating (Posterior Probability)

Starting with my prior of 20% (odds of 1:4), I'll update based on the strongest evidence first:

**Update on Evidence 2 (Commerce Department rule):**
- Prior odds: 0.2/0.8 = 0.25
- Likelihood ratio: 4.75
- Posterior odds: 0.25 × 4.75 = 1.1875
- Posterior probability: 1.1875/(1+1.1875) = 0.543 or ~54%

This is a substantial update, reflecting the direct relevance of the Commerce Department's proposed rule.

**Update on Evidence 1 (Executive Order):**
- Prior odds: 0.543/0.457 = 1.188
- Likelihood ratio: 2.25
- Posterior odds: 1.188 × 2.25 = 2.673
- Posterior probability: 2.673/(1+2.673) = 0.728 or ~73%

**Update on Evidence 3 (Chip controls):**
- Prior odds: 0.728/0.272 = 2.676
- Likelihood ratio: 1.6
- Posterior odds: 2.676 × 1.6 = 4.282
- Posterior probability: 4.282/(1+4.282) = 0.811 or ~81%

**Update on Evidence 4 (Commercial expansion):**
- Prior odds: 0.811/0.189 = 4.291
- Likelihood ratio: 0.33
- Posterior odds: 4.291 × 0.33 = 1.416
- Posterior probability: 1.416/(1+1.416) = 0.586 or ~59%

**Update on Evidence 5 (Open-source models):**
- Prior odds: 0.586/0.414 = 1.415
- Likelihood ratio: 0.63
- Posterior odds: 1.415 × 0.63 = 0.891
- Posterior probability: 0.891/(1+0.891) = 0.471 or ~47%

After considering all key evidence, my posterior probability is approximately 47%.

## 5. Red Team Thinking

Let me critically evaluate this forecast:

**Potential overconfidence:**
- I may be overweighting recent regulatory signals that might not translate to actual restrictions
- The Commerce Department rule is still proposed, not finalized, and could be watered down
- I might be underestimating the economic and diplomatic costs of restricting API access

**Alternative scenarios:**
- The US could opt for targeted restrictions against specific countries (e.g., China, Russia, Iran) rather than broad geographic restrictions
- Restrictions might focus on specific use cases rather than geographic boundaries
- The US might pursue a multilateral approach with allies rather than unilateral restrictions

**Tail risks:**
- A major AI-related security incident could dramatically increase the probability
- A breakthrough in open-source AI could render API restrictions ineffective, decreasing probability
- Changes in administration after the 2024 election could shift policy priorities

**Best alternative forecast:**
A reasonable alternative would be a lower probability (~30%) based on:
- Historical reluctance to restrict software exports
- Strong economic incentives against restrictions
- Technical challenges in enforcement
- The "leaky bucket" problem of open-source alternatives

Rebuttal: While these factors are valid, the recent policy signals from the Biden administration show a clear shift toward treating AI as a national security issue, with concrete steps already being taken to monitor and potentially control foreign access.

After this red-teaming, I believe my forecast of 47% is reasonable but should be adjusted slightly downward to account for implementation challenges and economic pressures.

## 6. Final Forecast

Considering the base rate, evidence updates, and red-team analysis, I arrive at a final forecast of 42%.

This reflects:
- A significant increase from the historical base rate due to clear policy signals
- Tempered by economic incentives against restrictions and practical enforcement challenges
- Recognition of the "leaky bucket" problem posed by open-source alternatives
- Uncertainty about the specific form restrictions might take

Final: 42%

```

#### Gemini

- ok=True | t_ms=58928 | tokens: prompt=4671, completion=1621, total=10856 | cost=$0.022049

```md

Here is my probabilistic forecast reasoning:

### 1. Base Rate (Prior) Selection
The event in question is a significant US government restriction on a broadly available, dual-use software technology. The most relevant reference classes are:
1.  **The "Crypto Wars" of the 1990s:** The US government classified strong cryptography as a munition and restricted its export. This policy was ultimately reversed due to economic pressure and impracticality. This suggests a low base rate for such restrictions being durable, perhaps 5-10%.
2.  **Modern Semiconductor Export Controls:** The US has actively and successfully restricted the export of advanced semiconductor technology to specific adversaries, notably China, on national security grounds. This suggests a higher base rate for targeted, hardware-adjacent technology controls, perhaps 25-35%.

Generative AI has characteristics of both: it's scalable software like crypto, but its power is concentrated in a few frontier models that rely on the specific hardware targeted by semiconductor controls. Blending these two reference classes, a reasonable prior for such a restriction being enacted over a multi-year period (e.g., the original 2023-2025 timeframe) would be around **20%**. This acknowledges the historical rarity of broad software controls but incorporates the heightened national security environment of the present.

### 2. Comparison to Base Case
The current situation differs from the base cases in several key ways:
*   **Concentration:** Unlike cryptography, frontier AI is controlled by a handful of US companies, making regulation technically easier to impose. This increases the likelihood compared to the base case.
*   **Economic Stakes & Alternatives:** The global market for AI is vast, creating immense economic pressure against restrictions. Furthermore, the rapid rise of powerful open-source models undermines the effectiveness of restricting only US APIs, providing a strong argument against the policy's utility. This decreases the likelihood.
*   **The Crucial Difference: Time Remaining.** The most significant factor for this forecast is the date. It is October 18, 2025, and the question resolves at the end of 2025. With less than three months remaining, the window for action is extremely small. The standard legislative process is far too slow. This dramatically lowers the probability from any prior established at the beginning of the period.

### 3. Evidence Evaluation (Likelihoods)
We must evaluate the evidence through the lens of the short remaining time.

*   **Evidence Increasing Probability (Historical):**
    *   **President Biden's October 2023 AI Executive Order:** A very strong signal of intent to regulate frontier AI for safety and security. (P(E|H) >> P(E|~H)).
    *   **Commerce Department's January 2024 IaaS Proposal:** A concrete regulatory step to monitor and potentially control foreign access to AI training infrastructure. This is the most direct evidence of movement toward this type of control. (P(E|H) >> P(E|~H)).

*   **Evidence Decreasing Probability (Countervailing Forces):**
    *   **Economic Lobbying:** US AI companies are pushing for global market dominance, creating a powerful force against self-inflicted economic harm. (P(E|H) < P(E|~H)).
    *   **Open-Source Proliferation:** The existence of near-SOTA open-source models makes restricting a few US APIs a "leaky" and potentially futile policy. (P(E|H) < P(E|~H)).

*   **Overwhelming Evidence Decreasing Probability (Time):**
    *   **Evidence (E):** As of October 18, 2025, no such law has been passed, nor is any bill in the final stages of the legislative process.
    *   **Likelihood Ratio:** The probability of seeing this state of affairs if a law were to pass in the next 10.5 weeks (P(E|H)) is extremely low. The probability of seeing this if no law passes (P(E|~H)) is very high. The likelihood ratio P(E|H)/P(E|~H) is therefore extremely small, which must drastically reduce our posterior probability.

### 4. Bayesian Updating (Posterior Probability)
1.  **Start with the prior:** 20% (or 1:4 odds).
2.  **Update with 2023-2024 evidence:** The strong executive actions likely outweighed the countervailing forces, pushing the probability into the 25-35% range for the overall period. Let's settle on a 30% probability (approx. 3:7 odds) entering 2025.
3.  **Update with the passage of time:** The fact that 22 of the 24 months in the resolution period have passed without the event occurring is immensely strong evidence. The legislative path is now effectively impossible. The only remaining plausible paths to a "YES" are:
    *   **Path A: Emergency Executive Order.** This would require a "black swan" event, like a catastrophic AI-enabled attack, in the next 10 weeks. The probability of such an event is very low, perhaps 1-2%.
    *   **Path B: Finalization of a Regulation.** The Commerce Department could finalize its IaaS rule with a scope that is unexpectedly expanded to include a blanket API ban outside the US. This is the most plausible path, but the specific resolution criteria ("to *only* people in the US") describe an extreme policy that would cut off key allies, making it less likely than a more targeted restriction (e.g., against China). The probability of this specific, broad rule being finalized in the next 10 weeks is low, estimated around 5%.

4.  **Final Posterior:** Summing the probabilities of these disjoint, low-probability paths gives a final estimate.
    *   Posterior Probability ≈ P(Path A) + P(Path B) ≈ 1% + 5% = 6%.

### 5. Red Team Thinking
The main argument for a higher forecast (like the 36% on Manifold) is that I am underestimating the speed and power of the US regulatory state when motivated by national security. The Biden administration has been consistently hawkish on tech controls. A final rule from the Commerce Department could be published with little warning. However, the wording of the question is very strict: a restriction to "only people in the US." This implies cutting off Europe, the UK, Japan, and other allies, which would cause a massive diplomatic and economic backlash. A more targeted restriction (e.g., on China and other countries of concern) is far more likely than the specific event described in the question. Therefore, even if a last-minute regulation appears, it is unlikely to meet this high bar. My 6% forecast already accounts for this possibility and seems robust. The market may be anchored on an earlier assessment and not have fully updated for the time decay and the strictness of the resolution criteria.

### Final Forecast
The initial drivers for such a policy were strong, but the window for implementation has nearly closed. The legislative path is no longer viable. The only remaining paths are a low-probability emergency executive order or a surprise, broad-scope regulation being finalized in the next ~10 weeks. The strict requirement to restrict access to "only people in the US" makes even the regulatory path less likely, as a more targeted country-list approach is the more plausible policy outcome. Therefore, the probability is very low.

Final: 6%

```

#### Grok

- ok=False | t_ms=73 | tokens: prompt=0, completion=0, total=0 | cost=$0.000000

- error=HTTP 429: Your team 0101076a-72f1-4f25-a038-b346d74bbc8d has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.

### Aggregation (BMC)

- final_probability=0.3063

- bmc_summary={"mean": 0.30631924921706494, "p10": 0.044955279968937265, "p50": 0.2630307861614176, "p90": 0.6414800289586873}