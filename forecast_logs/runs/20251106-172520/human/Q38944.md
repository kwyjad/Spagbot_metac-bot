# What will be the largest number of digits of π to have ever been computed by December 31, 2025? (QID: 38944)

- Type: numeric

- URL: https://www.metaculus.com/questions/38944/

- Classifier: science | strategic=False (score=0.00)

### SeenGuard

- enabled=True | lock_status=acquired

- run_filter: before=1 | skipped=0 | after=1

- debug_note=lock acquired

## Research (summary)

CALIBRATION GUIDANCE (auto-generated weekly):
BINARY CALIBRATION
- No resolved binary questions (with pre-resolution forecasts) yet.
- Advice: keep using base rates + small evidence-weighted updates; avoid big swings.

MULTIPLE-CHOICE CALIBRATION (Top-1)
- No resolved MCQ questions (with pre-resolution forecasts) yet.

NUMERIC CALIBRATION (PIT-lite + CRPS)
- No resolved numeric questions (with pre-resolution forecasts) yet.
- When numeric results exist, we’ll check p10/p50/p90 coverage and CRPS.

General takeaway: apply **small** nudges only where gaps are consistent with decent sample sizes (≥10).
— end calibration —

### Reference class & base rates
-   **Reference Class 1: Post-2009 World Records.** Since 2009, there have been 10 new world records for π computation. The time between these records has averaged approximately 1.5 years (18 months). The number of digits has grown exponentially, with each new record being, on average, 2.5 times larger than the previous one.
    -   *Base Rate:* Based on this class, a new record is "due" as the last one was set in March 2024 (~20 months ago as of today). The trend would suggest a new record in the range of 250-300 trillion digits.
    -   *Limitations:* This is a small sample size (n=10), and the timing is irregular. It treats all record-setters as part of a homogenous group, ignoring the shift from individuals to corporate-backed teams.

-   **Reference Class 2: Corporate-Sponsored "Benchmark" Records.** The last three major records (Google 2019: 31.4T, Google 2022: 100T, StorageReview/Solidigm 2024: 105T) were set by corporations to showcase their cloud computing or hardware capabilities. The interval between these was ~3 years (2019-2022) and ~2 years (2022-2024).
    -   *Base Rate:* This class suggests a new record is plausible within a 2-3 year window. We are ~1.7 years into the current record's reign, placing a new attempt within the realm of possibility for the general timeframe, but not necessarily within the next two months.
    -   *Limitations:* This is an even smaller reference class (n=3), making the "average" interval highly uncertain.

### Recent developments (timeline bullets)
-   [2024-03-14] StorageReview announces a new record of 105 trillion digits, computed using Solidigm SSDs. — ↑ — This event sets the new floor for the resolution value, significantly higher than the 100T record from 2022 mentioned in the background sources.
-   [2024-03-27] Guinness World Records officially certifies the 105 trillion digit record. — ↑ — Official verification solidifies the 105T figure as the current, credible record to beat.
-   [2024-05-01 to 2025-11-06] No credible announcements of new, larger π computation attempts have been made by major cloud providers (Google, AWS, Azure), hardware companies, or known individual competitors. — ↓ — The lack of news about an ongoing attempt, which would likely take months, significantly lowers the probability of a new record being completed by the end of 2025.

### Mechanisms & drivers (causal levers)
-   **Computational Resource Advancement (Moore's Law):** The primary driver is the continual improvement in computing hardware, particularly CPU speeds, RAM capacity, and storage I/O performance. Faster and larger storage is a key enabler for recent records. (Large impact)
-   **Corporate Marketing & Benchmarking:** Tech companies (especially cloud and storage providers) use π computation as a high-profile stress test and marketing tool to demonstrate the power, stability, and scale of their infrastructure. (Large impact)
-   **Software Optimization:** The `y-cruncher` program, the de facto standard for these records, continues to be optimized, allowing for more efficient use of hardware and potentially reducing computation times. (Moderate impact)
-   **Computation Time:** A significant barrier. The 100T digit record took 157 days; the 105T record took 75 days (on more specialized hardware). A substantially larger record would likely require a multi-month, uninterrupted computation. (Large impact, acts as a constraint)
-   **Cost:** The financial cost of hardware, electricity, and engineering time is substantial, limiting the pool of potential competitors to well-funded corporations or highly dedicated individuals with access to sponsored hardware. (Moderate impact)

### Differences vs. the base rate (what’s unusual now)
-   **Extremely Short Horizon:** The most critical difference. With less than two months until the resolution date, and record-setting computations taking 2.5-5 months, it is practically impossible for a new, larger record attempt to start now and finish in time. Any such attempt would have needed to be well underway by mid-2025.
-   **Small Incremental Increase:** The last record jump (100T to 105T digits) was only a 5% increase, a stark departure from the typical 2-3x increases seen in the reference class. This may suggest that reaching the next major milestone (e.g., 200T+) requires a more significant leap in hardware or methodology, making it a longer-term project.
-   **Shift in Actors:** The 2024 record was not set by a cloud giant like Google but by a tech publication (StorageReview) in partnership with a hardware manufacturer (Solidigm). This broadens the potential field of actors but also suggests a focus on specific hardware benchmarking rather than general cloud capability.
-   **Absence of a Public "Race":** Unlike some historical periods, there is no publicly declared, active "race" to the next milestone. Announcements of new records in the modern era tend to be surprises, but the lack of any rumors or chatter is still notable.

### Bayesian update sketch (for the statistician)
-   **Prior:** A prior based on the historical log-linear trend (Reference Class 1) would suggest a new record in the 200-300T digit range is plausible for late 2025. This should be a weak prior (low "equivalent n") due to the small sample size and irregular timing.
-   **Evidence mapping:**
    -   (↑ Large) The floor was raised to 105 trillion digits in March 2024, invalidating any lower forecast.
    -   (↓ Very Large) The <2 month time horizon is a powerful constraint, making a new record completion almost impossible given recent computation times of 75-150+ days.
    -   (↓ Moderate) The lack of any announcements or rumors of an ongoing attempt from key players (Google, etc.) further reduces the likelihood of a surprise completion.
    -   (↔ Small) The small 5% increment of the last record is ambiguous; it could signal diminishing returns (making a new record harder) or an easy target for a more ambitious team (making a new record more likely, but not faster).
-   **Net effect:** The evidence overwhelmingly favors the status quo. The strong negative update from the short time horizon should collapse the prior distribution, moving the posterior to be very sharply peaked at the current record of 105 trillion digits.

### Indicators to watch (leading signals; next weeks/months)
-   **UP indicators:**
    -   A surprise press release from Google, AWS, Microsoft, Oracle Cloud, or a major hardware firm (NVIDIA, AMD, Solidigm) announcing a completed computation.
    -   A blog post from a known competitor (e.g., Emma Haruka Iwao, Alexander Yee) detailing a new, completed run.
    -   An update to the `y-cruncher` website or Wikipedia's "Chronology of computation of π" page with a new, credible entry.

-   **DOWN indicators:**
    -   The passage of time toward December 31, 2025, with no announcements (each passing day makes a new record less likely).
    -   Key actors like Google Cloud publishing major benchmarks on unrelated tasks (e.g., AI model training), suggesting their top-tier compute resources are allocated elsewhere.

### Caveats & pitfalls
-   **Secretive Computation:** A record attempt could have been running in secret for months and may be announced any day. The probability of this is low but non-zero.
-   **Verification Lag:** A computation might finish before the deadline but not be announced or verified by credible media until after. Per the resolution criteria, it would not count. The key is the date of the credible report.
-   **Rounding/Precision:** The question asks for the number of digits. The answer will be a specific integer, likely expressed in trillions (e.g., 105,000,000,000,000). Ensure the final number is precise.
-   **Outdated Information:** The provided background sources are from 2019 and 2022 and are outdated. The current record is 105 trillion digits (March 2024), not 100 trillion. This is the most critical fact for the forecast.

### Final Research Summary
The current world record is 105 trillion digits, set in March 2024. Given that recent record-setting computations have taken between 75 and 158 days, the extremely short time remaining until December 31, 2025, makes it highly improbable that a new, larger record will be completed and reported in time.

Market snapshot debug:
- Metaculus: no open binary results in response
- Manifold: no open binary results in response

### Sources
- Google Cloud team claims to have calculated 100trn digits of pi - Silicon Republic (www.siliconrepublic.com) — https://www.siliconrepublic.com/machines/google-cloud-pi-100trn-digits-emma-haruka-iwao
- Google Cloud employee beats record by calculating value of Pi to 100 trillion digit (www.thehindubusinessline.com) — https://www.thehindubusinessline.com/info-tech/google-cloud-employee-beats-record-by-calculating-value-of-pi-to-100-trillion-digit/article65526316.ece
- A Google employee just shattered the record for Pi calculations. Her name is Emma Haruka Iwao - The Washington Post (www.washingtonpost.com) — https://www.washingtonpost.com/technology/2019/03/14/google-employee-just-shattered-record-pi-calculations-her-name-is-emma-haruka-iwao/
- This Google employee just smashed the world record for calculating pi - CBS News (www.cbsnews.com) — https://www.cbsnews.com/news/pi-day-record-google-employee-emma-haruka-iwao-calculates-pi-to-31-trillion-digits/
- How Google's Emma Haruka Iwao Helped Set a New Record for Pi - The New Stack (thenewstack.io) — https://thenewstack.io/how-googles-emma-haruka-iwao-helped-set-a-new-record-for-pi/
- A Google Cloud employee set a new record calculating the 100 trillionth digit of pi (qz.com) — https://qz.com/a-google-cloud-employee-set-a-new-record-calculating-1849032213
- Emma Haruka Iwao - Wikipedia (en.wikipedia.org) — https://en.wikipedia.org/wiki/Emma_Haruka_Iwao
- Google's Emma Haruka Iwao breaks the world record for calculating pi - Engadget (www.engadget.com) — https://www.engadget.com/2019-03-14-google-pi-day-world-record.html
- Google Employee Emma Haruka Iwao Breaks Pi Day Record - InsideHook (www.insidehook.com) — https://www.insidehook.com/internet/google-employee-emma-haruka-iwao-breaks-pi-day-record

### Research (debug)

- source=GoogleGrounding | llm=google/gemini-2.5-pro | cached=0 | n_raw=11 | n_kept=9 | cost=$0.023652

- query: format; metac_closes_in_period; info; post_id; question_id; In March; Emma Haruka Iwao; 2019; 2025

### GTMC1 (debug)

- strategic_class=False | strategic_score=0.00 | source=llm

- gtmc1_active=False | qtype=numeric | t_ms=0

- actors_parsed=0

- exceedance_ge_50=None | coalition_rate=None | median=None | dispersion=None

### Ensemble (model outputs)

- OpenRouter-Default: ok=True t_ms=18398 P10=31.000, P50=2025.000, P90=1.000

- Claude-3.7-Sonnet (OR): ok=True t_ms=19400 P10=31.000, P50=2025.000, P90=1.000

- Gemini: ok=True t_ms=58727 P10=31.000, P50=2025.000, P90=1.000

- Grok: ok=False t_ms=275



### Per-model (raw + usage/cost)

#### OpenRouter-Default

- ok=True | t_ms=18398 | tokens: prompt=3475, completion=574, total=4049 | cost=$0.014427

```md

To forecast the largest number of digits of π to have been computed by December 31, 2025, we will follow a structured Bayesian reasoning process:

1. **Base Rate (Prior) Selection**
   - **Reference Class 1: Post-2009 World Records** provides a base rate of exponential growth in π computation, with records increasing by a factor of approximately 2.5 every 1.5 years. The mean of this distribution suggests a new record could be in the range of 250-300 trillion digits by late 2025.
   - **Reference Class 2: Corporate-Sponsored Records** suggests a more conservative growth pattern, with records set every 2-3 years. The current record is 105 trillion digits, set in March 2024.

2. **Comparison to Base Case**
   - The current situation aligns with the base case in terms of the exponential growth trend, but the extremely short time horizon (less than two months) makes it unlikely for a new record to be set by the end of 2025.

3. **Evidence Evaluation (Likelihoods)**
   - **Current Record (105 trillion digits):** Sets a floor for the forecast.
   - **Short Time Horizon:** Strongly suggests no new record will be set, as recent computations have taken 75-150+ days.
   - **Lack of Announcements:** Further decreases the likelihood of a new record, as no major attempts have been reported.

4. **Bayesian Updating (Posterior Distribution)**
   - **Prior:** Based on historical trends, a new record of 200-300 trillion digits is plausible.
   - **Likelihood:** The short time horizon and lack of announcements significantly reduce the probability of a new record.
   - **Posterior:** The evidence overwhelmingly supports the status quo, with the posterior distribution sharply peaked at 105 trillion digits.

5. **Red Team Thinking**
   - **Secretive Computation:** While possible, the probability is low given the lack of announcements.
   - **Verification Lag:** A computation might finish but not be reported in time, but this is unlikely to affect the resolution.
   - **Alternative Forecast:** A new record could be set if a secretive computation is completed, but this is highly improbable.

6. **Final Percentiles**
   - Given the evidence and analysis, the forecast is heavily weighted towards the current record of 105 trillion digits.

P10: 105,000,000,000,000
P20: 105,000,000,000,000
P40: 105,000,000,000,000
P60: 105,000,000,000,000
P80: 105,000,000,000,000
P90: 105,000,000,000,000

```

#### Claude-3.7-Sonnet (OR)

- ok=True | t_ms=19400 | tokens: prompt=3943, completion=971, total=4914 | cost=$0.026394

```md

# Forecasting the Largest Number of π Digits Computed by December 31, 2025

## 1. Base Rate (Prior) Selection

For my prior, I'll use the historical progression of π digit computation records, focusing on the most recent decade:

- 2019 (Google): 31.4 trillion digits
- 2022 (Google): 100 trillion digits
- 2024 (StorageReview/Solidigm): 105 trillion digits

The historical pattern shows exponential growth, with each record typically being 2-3x larger than the previous one. However, the most recent increase was only 5% (from 100T to 105T).

My base rate prior would suggest the next record should be around 200-300 trillion digits, following the historical growth pattern. However, this assumes sufficient time for computation and announcement.

## 2. Comparison to Base Case

**Similarities to base case:**
- Computing power continues to increase following Moore's Law
- Corporate interest in π computation as a benchmark remains
- The y-cruncher software continues to be the standard tool

**Critical differences:**
- Extremely short time horizon: Less than 2 months remain until December 31, 2025
- Recent computations took 75-158 days to complete
- The most recent record (March 2024) showed only a 5% increase, suggesting diminishing returns
- No public announcements of ongoing computation attempts by major players

## 3. Evidence Evaluation

**Evidence for higher values:**
- The current record of 105 trillion establishes a firm lower bound
- Computational power continues to increase, theoretically enabling larger computations
- Corporate interest in setting these records for marketing purposes remains

**Evidence for the status quo (105 trillion):**
- Computation time constraint: Recent records took 2.5-5 months to compute, making it practically impossible to start and complete a new record in the remaining time
- No public announcements or rumors of ongoing computation attempts
- The small increment in the last record (5%) suggests diminishing returns or technical barriers

## 4. Bayesian Updating

Starting with a prior centered around 200-300 trillion digits (based on historical growth patterns), I need to update based on the time constraint evidence:

P(New Record | Time Constraint) = P(Time Constraint | New Record) × P(New Record) / P(Time Constraint)

The likelihood P(Time Constraint | New Record) is extremely low - perhaps 0.01 or lower - because a new record computation would almost certainly require more time than remains before December 31, 2025.

This dramatically shifts my posterior distribution to be heavily concentrated at 105 trillion digits, with only a small probability mass for higher values.

## 5. Red Team Thinking

**Potential blind spots:**
- A computation could have been running in secret for months and might be announced soon
- A breakthrough in algorithm efficiency could enable faster computation
- A partial result (e.g., 150 trillion digits) might be announced even if verification is incomplete

**Alternative scenario:**
The most plausible alternative is that a major tech company (Google, Microsoft, etc.) has been running a computation since mid-2025 and will announce a new record of ~200 trillion digits in the next few weeks. However, the lack of any rumors or leaks makes this unlikely.

**Tail risks:**
- Extremely small chance (<1%) of a revolutionary algorithmic breakthrough enabling computation of 500+ trillion digits quickly
- Small chance (~5%) that a computation is already complete but announcement is being saved for a strategic date

## 6. Final Percentiles

Given the extremely short time horizon and the absence of any announcements about ongoing computations, I assign a very high probability (~90%) that the current record of 105 trillion will remain the largest by December 31, 2025. I allow for small probabilities of larger values to account for the possibility of secret computations nearing completion.

P10: 105,000,000,000,000
P20: 105,000,000,000,000
P40: 105,000,000,000,000
P60: 105,000,000,000,000
P80: 150,000,000,000,000
P90: 200,000,000,000,000

```

#### Gemini

- ok=True | t_ms=58727 | tokens: prompt=3881, completion=1515, total=10432 | cost=$0.020001

```md

Answering the question: What will be the largest number of digits of π to have ever been computed by December 31, 2025?

### 1. Base Rate (Prior) Selection
The appropriate reference class is the history of world records for the computation of π digits, particularly in the modern era (post-2009). Since 2009, there have been 11 records. In the most recent "corporate era" (since 2019), the time between new records has averaged about 15 months. The current record of 105 trillion digits was set in March 2024. As of November 2025, it has been 20 months, which is longer than the recent average.

Based on this, a prior distribution would suggest that a new record is plausible and perhaps even "due" around this time. The size of a new record, based on historical multiplicative jumps (averaging ~1.5x, though with high variance), might be in the 150-200 trillion digit range. This creates a bimodal prior: a high probability of the status quo (105T) and a smaller probability of a jump to a new, higher value. For instance, a prior might place a 60% chance on no new record and a 40% chance of a new record by the end of 2025.

### 2. Comparison to Base Case
The current situation is similar to the base case in that the underlying drivers—advances in computing hardware (Moore's Law), corporate marketing incentives, and algorithmic improvements (`y-cruncher` software)—remain active.

However, there is one critical difference: the time horizon. The question resolves on December 31, 2025, which is less than two months from the present date (November 6, 2025). This is a stark departure from a generic "what will the next record be?" scenario, as it imposes a very hard deadline.

### 3. Evidence Evaluation (Likelihoods)
I will evaluate the key pieces of evidence and their impact on the prior distribution.

*   **Evidence 1: The current record is 105 trillion digits, set in March 2024.** This is a foundational fact. It sets the absolute minimum possible value for the resolution. This evidence anchors the entire distribution at or above 105,000,000,000,000.

*   **Evidence 2: The time remaining is less than two months.** This is the most powerful piece of evidence. The last two record-setting computations took 75 days (for 105T digits) and 158 days (for 100T digits). A new, larger computation would likely take at least 2-3 months, if not longer. Therefore, it is practically impossible for a new record attempt to *begin now* and finish before the deadline. Any new record would have to come from a computation that is already complete or very near completion. This evidence provides a massive update, making the "no new record" scenario far more likely. The likelihood ratio of P(Time < 2mo | No New Record) / P(Time < 2mo | New Record) is extremely high.

*   **Evidence 3: There have been no public announcements or credible rumors of an ongoing record attempt.** While such attempts can be secret, a multi-month project using massive computational resources at a major tech company (the most likely actor) is difficult to keep entirely quiet. This lack of "chatter" further reduces the probability of a surprise announcement, acting as a moderate downward update on the likelihood of a new record.

### 4. Bayesian Updating (Posterior Distribution)
Let's conceptually update the prior.
*   **Prior**: P(New Record) ≈ 40%, P(No New Record) ≈ 60%.
*   **Likelihood from Evidence**: The time constraint evidence is overwhelmingly more consistent with the "No New Record" hypothesis than the "New Record" hypothesis. A new record being announced in the last 55 days of the year is a low-probability event, even if one was planned for 2025.
*   **Posterior**: Posterior ∝ Prior × Likelihood. The very low likelihood of the evidence under the "New Record" hypothesis drastically reduces its posterior probability. A rough calculation suggests the probability of a new record drops from 40% to less than 5%.

The resulting posterior distribution is a mixture model: a ~95-97% probability mass concentrated at exactly 105,000,000,000,000, and a very small (3-5%) probability mass spread across a range of higher values (e.g., 150T to 200T) representing a potential surprise announcement.

### 5. Red Team Thinking
The primary way this forecast could be wrong is if a major player like Google has been running a computation in secret for the past several months to reclaim the record, and is simply waiting for a convenient date before year-end to announce it. This is a low-probability but high-impact scenario. The motivation exists, as the last record was set by a hardware partnership (StorageReview/Solidigm), and Google may wish to re-establish its dominance in large-scale cloud computation.

However, even if I increase the probability of this surprise scenario to 5%, it does not change the lower and central percentiles of the distribution. The distribution's cumulative density function (CDF) would show a step from 0 to 0.95 at the value 105T. This means any percentile up to the 95th must be 105T. The requested percentiles (P10, P20, P40, P60, P80, P90) all fall within this range. Therefore, the forecast is robust to reasonable uncertainty about the tail risk. The forecast is necessarily "degenerate" for these percentiles due to the discrete nature of the potential outcomes.

### 6. Final Percentiles
The posterior distribution is overwhelmingly concentrated at the current record of 105 trillion. There is a small but non-zero probability of a new record, but this tail risk only affects the highest percentiles (e.g., P96 and above). All requested percentiles fall within the mass of probability at the current record.

P10: 105000000000000
P20: 105000000000000
P40: 105000000000000
P60: 105000000000000
P80: 105000000000000
P90: 105000000000000

```

#### Grok

- ok=False | t_ms=275 | tokens: prompt=0, completion=0, total=0 | cost=$0.000000

- error=HTTP 429: Your team 0101076a-72f1-4f25-a038-b346d74bbc8d has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.

### Aggregation (BMC)

- final_quantiles: P10=-989.1860566243694, P50=29.553700756929178, P90=1061.205681388584

- bmc_summary={"mean": 34.83219621421629, "p10": -989.1860566243694, "p50": 29.553700756929178, "p90": 1061.205681388584}