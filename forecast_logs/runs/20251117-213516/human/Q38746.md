# Will leading AI labs have their models evaluated for dangerous behavior before 2026? (C3.ai) (QID: 38746)

- Type: binary

- URL: https://www.metaculus.com/questions/38746/

- Classifier: technology | strategic=False (score=0.00)

### SeenGuard

- enabled=True | lock_status=acquired

- run_filter: before=1 | skipped=0 | after=1

- debug_note=lock acquired

## Research (summary)

CALIBRATION GUIDANCE (auto-generated weekly):
BINARY CALIBRATION
- No resolved binary questions (with pre-resolution forecasts) yet.
- Advice: keep using base rates + small evidence-weighted updates; avoid big swings.

MULTIPLE-CHOICE CALIBRATION (Top-1)
- No resolved MCQ questions (with pre-resolution forecasts) yet.

NUMERIC CALIBRATION (PIT-lite + CRPS)
- No resolved numeric questions (with pre-resolution forecasts) yet.
- When numeric results exist, we’ll check p10/p50/p90 coverage and CRPS.

General takeaway: apply **small** nudges only where gaps are consistent with decent sample sizes (≥10).
— end calibration —

### Reference class & base rates
1.  **Publicly Traded Enterprise AI Application Companies:** This is the most appropriate reference class. C3.ai's business model is providing a platform and applications for enterprise clients, not developing general-purpose, frontier foundation models. The base rate of companies in this class (e.g., Palantir, Snowflake) having a model evaluated by METR is **0%**.
2.  **Leading AI Labs Mentioned in AI Safety Discussions:** This class includes labs like OpenAI, Anthropic, Google DeepMind, and Meta AI, which are actively developing SOTA foundation models. Of this group of ~4-5 labs, two (OpenAI, Anthropic) have standing partnerships with METR. This suggests a base rate of **~40-50%**. However, C3.ai does not fit the primary characteristic of this class (frontier model development).
3.  **All AI Companies:** This is too broad to be useful. The base rate would be extremely low, approaching **<0.1%**, as it includes thousands of firms with no relevant models.

**Reasoning:** The key distinction is the type of AI being built. METR's focus is explicitly on "cutting-edge AI systems" that could pose "catastrophic risks." This concern is almost exclusively directed at large, general-purpose foundation models. C3.ai's business is in a different category, making Reference Class 1 the most relevant, suggesting a very low prior probability.

### Recent developments (timeline bullets)
*   **2024-09-09** METR submits comment to NIST — **↓** — The comment reinforces METR's focus on "dual-use foundation models" and "dangerous capabilities," explicitly citing their work with GPT-4 and Claude. This highlights the gap between their mission and C3.ai's product line.
*   **2024-05-21** Meta shuts down 'Workplace' — **~** — This source is irrelevant to the question about C3.ai and METR.
*   **General Observation (Q3-Q4 2025)** No public announcements — **↓** — The complete absence of any news, press releases, or even rumors connecting C3.ai with METR or the catastrophic risk evaluation ecosystem is the most significant recent evidence. Given the <2 month horizon, a partnership would likely need to be in advanced stages or already exist.

### Mechanisms & drivers (causal levers)
1.  **Mismatch of Model Type (Large, Down):** C3.ai provides an enterprise AI application platform, integrating various models (including third-party ones) for specific business tasks. METR evaluates novel, frontier-scale foundation models for emergent, dangerous capabilities. C3.ai does not appear to produce the type of artifact that METR evaluates.
2.  **Mismatch of Safety Culture & Focus (Large, Down):** C3.ai's public-facing "Responsible AI" efforts focus on traditional enterprise concerns like bias, fairness, and transparency. METR's entire purpose is to evaluate catastrophic and existential risks (e.g., self-replication, power-seeking), a topic C3.ai and its leadership do not publicly engage with.
3.  **Extremely Short Timeline (Large, Down):** With a resolution date of January 1, 2026, there is virtually no time for C3.ai to develop a relevant model, initiate a partnership with METR, conduct a lengthy and complex evaluation, and have it credibly reported.
4.  **Regulatory/Political Pressure (Small, Up):** A sudden, binding government mandate could theoretically compel a wider range of AI companies, including C3.ai, to undergo external safety evaluations. However, the likelihood of such a specific mandate being enacted and enforced on C3.ai within the next ~45 days is negligible.
5.  **Strategic Pivot by C3.ai (Small, Up):** C3.ai could announce a major strategic shift into frontier model development and, as a signal of goodwill and safety consciousness, partner with METR. This is highly improbable given their established business model and the immense capital required.

### Differences vs. the base rate (what’s unusual now)
This case is primarily unusual when compared to the *positive* examples in Reference Class 2 (OpenAI, Anthropic).
*   **Business Model:** C3.ai is an enterprise application provider. OpenAI and Anthropic are primarily foundation model developers whose business models depend on selling access to those models.
*   **Corporate Origin & Culture:** OpenAI and Anthropic have origins deeply rooted in the AGI and AI safety research communities. C3.ai originates from the enterprise software world of its CEO, Thomas Siebel (founder of Siebel Systems).
*   **Stated Goals:** C3.ai's goal is to solve enterprise problems and deliver ROI. The stated goals of labs like Anthropic explicitly include the safe development of AGI.
*   **Ecosystem:** C3.ai operates in the world of enterprise IT, competing with Oracle, SAP, and Palantir. METR's partners operate in the ecosystem of AI research, competing for talent, compute, and benchmark performance. There is virtually no overlap.

### Bayesian update sketch (for the statistician)
*   **Prior:** Based on the most relevant reference class (Enterprise AI Application Companies), the prior probability should be extremely low, perhaps **1%**, with a high equivalent n (e.g., n=100) reflecting the strong structural reasons for this low probability.
*   **Evidence mapping:**
    *   **↓ (Large):** Fundamental mismatch between C3.ai's enterprise application model and METR's focus on frontier foundation models.
    *   **↓ (Large):** Divergent cultures and stated priorities regarding AI risk (enterprise "Responsible AI" vs. "catastrophic risk").
    *   **↓ (Large):** The extremely short (<2 month) timeline to resolution makes a new partnership and evaluation logistically infeasible.
    *   **↓ (Moderate):** The complete lack of any public statements, personnel overlap, or other "soft" connections between the two organizations.
*   **Net effect:** The evidence strongly and uniformly reinforces the very low prior. The posterior should move down, to a value very close to 0%.

### Indicators to watch (leading signals; next weeks/months)
*   **UP indicators:**
    *   A joint press release from C3.ai and METR announcing a partnership.
    *   C3.ai announcing the creation of a new "AGI" or "Frontier Model" research division.
    *   Hiring of prominent AI safety researchers (e.g., from Anthropic, DeepMind, or METR) by C3.ai.
    *   A credible media report (e.g., from The Information, Reuters) stating that a partnership is under negotiation.
*   **DOWN indicators:**
    *   C3.ai's next earnings call or investor presentation continues to focus exclusively on their current enterprise application business model.
    *   METR announces a new partnership with a different frontier lab (e.g., Google, Meta), reinforcing their specific focus.
    *   Continued silence from both parties through December 2025.

### Caveats & pitfalls
*   **Private Evaluation:** A partnership and evaluation could be conducted entirely in secret. However, the resolution requires a "credible report," making a completely secret evaluation unlikely to trigger a "Yes" resolution.
*   **Definitional Ambiguity:** C3.ai could commission a "light" evaluation of a minor proprietary model and frame it as a "dangerous behavior" assessment. This is unlikely to meet the spirit of the question or METR's standards, but could create ambiguity.
*   **Acquisition/Merger:** A black swan event where C3.ai acquires a frontier model lab that already has a relationship with METR. This is exceptionally unlikely in the given timeframe.
*   **Information Asymmetry:** Our analysis is based entirely on public information. Private strategic decisions inside C3.ai are unknown.

**Final Research Summary:** C3.ai is an enterprise AI application company, not a frontier model developer, placing it outside METR's established mission of evaluating cutting-edge models for catastrophic risks. Given this fundamental strategic mismatch and the extremely short timeline, a partnership and evaluation before 2026 is highly improbable.

Market snapshot debug:
- Metaculus: no open binary results in response
- Manifold: no open binary results in response

### Sources
- ARC Evals is spinning out from ARC (www.metr.org) — https://www.metr.org/blog/arc-evals-is-spinning-out-from-arc
- Nobody Knows How to Safety-Test AI (time.com) — https://time.com/6959323/ai-safety-testing-metr/
- ARC Evals is now METR (metr.org) — https://metr.org/blog/arc-evals-is-now-metr
- Comment on NIST AI 800-1 (Managing Misuse Risk for Dual-Use Foundation Models) (www.regulations.gov) — https://www.regulations.gov/comment/NIST-2024-0002-0106
- Meta is Closing Workplace: All Your Questions Answered (yoobic.com) — https://yoobic.com/blog/meta-is-closing-workplace-all-your-questions-answered
- ARC Prize 2024 Technical Report (arcprize.org) — https://arcprize.org/blog/2024-technical-report
- ARC Prize - Competitions (arcprize.org) — https://arcprize.org/competitions

### Research (debug)

- source=GoogleGrounding | llm=google/gemini-2.5-pro | cached=0 | n_raw=12 | n_kept=7 | cost=$0.022511

- query: format; metac_closes_in_period; info; post_id; question_id; ARC Evals; 2024; 2023; 2026

### GTMC1 (debug)

- strategic_class=False | strategic_score=0.00 | source=llm

- gtmc1_active=False | qtype=binary | t_ms=0

- actors_parsed=0

- exceedance_ge_50=None | coalition_rate=None | median=None | dispersion=None

### Ensemble (model outputs)

- OpenRouter-Default: ok=True t_ms=22450 p=0.0000

- Claude-3.7-Sonnet (OR): ok=True t_ms=25591 p=0.0010

- Gemini: ok=True t_ms=54479 p=0.0100

- Grok: ok=False t_ms=138



### Per-model (raw + usage/cost)

#### OpenRouter-Default

- ok=True | t_ms=22450 | tokens: prompt=3410, completion=627, total=4037 | cost=$0.014795

```md

To forecast the probability that leading AI labs will have their models evaluated for dangerous behavior before 2026, specifically focusing on C3.ai, we will follow a structured Bayesian reasoning approach.

1. **Base Rate (Prior) Selection**
   - The most appropriate reference class for C3.ai is "Publicly Traded Enterprise AI Application Companies." The base rate of companies in this class having a model evaluated by METR is 0%. This is because C3.ai's business model focuses on providing enterprise AI applications, not developing general-purpose, frontier foundation models that METR evaluates.
   - Initial prior probability: 1% (reflecting a very low likelihood due to the structural mismatch).

2. **Comparison to Base Case**
   - Similarity: C3.ai, like other enterprise AI companies, focuses on specific business applications rather than developing cutting-edge AI models.
   - Difference: Unlike companies like OpenAI or Anthropic, C3.ai does not engage in developing models that pose catastrophic risks, which METR targets.

3. **Evidence Evaluation (Likelihoods)**
   - **Mismatch of Model Type (Large, Down):** C3.ai's focus on enterprise applications makes it unlikely to develop models that METR would evaluate. Likelihood ratio significantly less than 1.
   - **Mismatch of Safety Culture & Focus (Large, Down):** C3.ai's emphasis on traditional enterprise AI concerns contrasts with METR's focus on catastrophic risks. Likelihood ratio significantly less than 1.
   - **Extremely Short Timeline (Large, Down):** The short timeline to 2026 makes it logistically improbable for C3.ai to establish a partnership and complete an evaluation. Likelihood ratio significantly less than 1.
   - **Regulatory/Political Pressure (Small, Up):** While possible, the likelihood of a mandate affecting C3.ai is very low. Likelihood ratio slightly greater than 1.

4. **Bayesian Updating (Posterior Probability)**
   - Prior odds: 1% / 99% = 0.0101
   - Given the strong negative evidence, the likelihood ratios for the key pieces of evidence are significantly less than 1, reinforcing the low prior.
   - Posterior odds = Prior odds × (Likelihood ratio for all evidence combined)
   - Assuming a combined likelihood ratio of approximately 0.1 (due to strong negative evidence), the posterior odds become 0.00101.
   - Posterior probability = 0.00101 / (1 + 0.00101) ≈ 0.1%

5. **Red Team Thinking**
   - Consideration of alternative scenarios, such as a strategic pivot by C3.ai or a sudden regulatory change, remains highly improbable within the given timeframe.
   - The possibility of private evaluations or definitional ambiguities is noted but unlikely to affect the resolution criteria.

6. **Final Forecast**
   - Given the strong structural reasons against C3.ai having its models evaluated by METR and the reinforcing negative evidence, the probability remains extremely low.
   - Final: 0%

```

#### Claude-3.7-Sonnet (OR)

- ok=True | t_ms=25591 | tokens: prompt=3889, completion=1393, total=5282 | cost=$0.032562

```md

# Will C3.ai have their models evaluated for dangerous behavior before 2026?

## 1. Base Rate (Prior) Selection

For this question, I need to identify the appropriate reference class for C3.ai and determine a base rate for AI labs having their models evaluated by METR for dangerous behavior.

The most relevant reference classes to consider:
- **Leading AI frontier model developers**: OpenAI and Anthropic have partnerships with METR (2 out of ~5 major labs), suggesting a base rate of ~40%.
- **Enterprise AI application companies**: C3.ai falls into this category, providing AI solutions for businesses rather than developing frontier foundation models. The base rate for such companies being evaluated by METR is effectively 0%.

C3.ai is clearly in the second category. They develop enterprise AI applications and integrate various models (including third-party ones) for specific business tasks, not frontier foundation models that might pose catastrophic risks.

**Prior probability: 1%** (allowing for a small possibility despite the structural mismatch)

## 2. Comparison to Base Case

**Similarities to companies that get METR evaluations:**
- C3.ai is an AI company with a public profile
- They develop and deploy AI systems

**Critical differences:**
- C3.ai doesn't develop frontier foundation models (the type METR evaluates)
- C3.ai's business model focuses on enterprise applications, not general-purpose AI
- C3.ai's public AI safety discussions focus on traditional concerns (bias, fairness) rather than catastrophic risks
- No historical connection to the AI safety research community that METR emerged from
- No public statements indicating interest in METR-style evaluations

These differences are fundamental to the question. METR specifically evaluates "cutting-edge AI systems" for "catastrophic risks to civilization" - a focus that doesn't align with C3.ai's enterprise application business.

## 3. Evidence Evaluation

**Evidence 1: C3.ai's business model and product focus**
- P(E|H): If C3.ai were to get METR evaluations, they would likely need to be developing frontier models = Low (~0.1)
- P(E|~H): If C3.ai were not getting METR evaluations, they would continue their enterprise AI focus = High (~0.9)
- Likelihood ratio: ~0.11 (strong evidence against)

**Evidence 2: Extremely short timeline (less than 2 months to 2026)**
- P(E|H): If C3.ai were to get METR evaluations before 2026, we would expect to see preparations already = High (~0.9)
- P(E|~H): If C3.ai were not getting METR evaluations, we would see no such preparations = High (~0.95)
- Likelihood ratio: ~0.95 (slight evidence against)

**Evidence 3: Complete absence of public connections between C3.ai and METR**
- P(E|H): If C3.ai were planning METR evaluations, we might see some signals = Moderate (~0.5)
- P(E|~H): If C3.ai were not planning METR evaluations, we would see no signals = High (~0.95)
- Likelihood ratio: ~0.53 (moderate evidence against)

**Evidence 4: METR's continued focus on frontier model developers**
- P(E|H): If C3.ai were to get METR evaluations, METR might broaden their stated focus = Moderate (~0.6)
- P(E|~H): If C3.ai were not getting METR evaluations, METR would maintain their current focus = High (~0.9)
- Likelihood ratio: ~0.67 (moderate evidence against)

## 4. Bayesian Updating

Starting with prior odds of 1:99 (from 1% probability)

Update on Evidence 1 (business model mismatch):
- Posterior odds = 1:99 × 0.11 = 0.11:99 ≈ 0.0011:1
- Posterior probability ≈ 0.11%

Update on Evidence 2 (short timeline):
- Posterior odds = 0.0011:1 × 0.95 = 0.00105:1
- Posterior probability ≈ 0.105%

Update on Evidence 3 (absence of connections):
- Posterior odds = 0.00105:1 × 0.53 = 0.00056:1
- Posterior probability ≈ 0.056%

Update on Evidence 4 (METR's focus):
- Posterior odds = 0.00056:1 × 0.67 = 0.00037:1
- Posterior probability ≈ 0.037%

The evidence consistently points toward an extremely low probability.

## 5. Red Team Thinking

**Potential blind spots:**
- **Private evaluations**: C3.ai could be conducting evaluations with METR privately, though this seems unlikely given METR's transparency about partnerships.
- **Regulatory pressure**: A sudden regulatory mandate could force evaluations, but implementing this within 45 days is extremely unlikely.
- **Definition ambiguity**: The question requires a "credible report" of evaluation before commercial release. C3.ai might conduct some form of safety evaluation that doesn't fully match METR's approach.
- **Strategic pivot**: C3.ai could suddenly announce a major shift toward frontier model development, though this would be a dramatic departure from their established business.

**Alternative forecast**: One could argue for a slightly higher probability (~1-5%) based on the possibility of unknown private arrangements or a very rapid strategic shift. However, the fundamental business model mismatch and extremely short timeline make even this generous alternative difficult to justify.

After considering these factors, I believe my forecast remains appropriate, though I'll round up slightly to account for unknown possibilities.

## 6. Final Forecast

Final: 0.1%

```

#### Gemini

- ok=True | t_ms=54479 | tokens: prompt=3709, completion=1155, total=8937 | cost=$0.016186

```md

Here is a probabilistic forecast following the specified methodology.

### 1. Base Rate (Prior) Selection
The most appropriate reference class for C3.ai is "Publicly Traded Enterprise AI Application Companies," which includes firms like Palantir and Snowflake. These companies focus on providing AI platforms and applications for specific business tasks, rather than developing general-purpose, frontier foundation models. METR's mission is to evaluate the latter for catastrophic risks.

The base rate of a company in this reference class having a model evaluated by METR is 0%. To allow for uncertainty and the possibility of unforeseen events, I will start with a very low prior probability of **1%**. This translates to prior odds of 1:99.

### 2. Comparison to Base Case
C3.ai is highly similar to the base case of an enterprise AI company. Its business model, public statements on "Responsible AI" (focusing on bias, fairness, and transparency), and corporate culture align with its enterprise peers. It is fundamentally different from the labs that *have* partnered with METR (OpenAI, Anthropic), which originated in the AGI research community and are focused on building frontier models. There are no significant differences that would make C3.ai more likely than its peers to seek a catastrophic risk evaluation from METR.

### 3. Evidence Evaluation (Likelihoods)
Several key pieces of evidence must be considered:

*   **Evidence 1: Fundamental Mismatch of Model Type and Mission.** C3.ai does not appear to develop the kind of frontier-scale, general-purpose models that are METR's sole focus for evaluation. This is a structural mismatch. The likelihood of this mismatch existing *if* a partnership were to happen (P(E|H)) is very low, while the likelihood of it existing if no partnership happens (P(E|~H)) is very high. This evidence strongly decreases the probability (Likelihood Ratio << 1).

*   **Evidence 2: Extremely Short Timeline.** The resolution date is January 1, 2026, which is less than two months from the current date (November 17, 2025). A partnership would need to be negotiated, a complex evaluation conducted (these typically take months), and a credible report published within this short window. From a standing start, this is logistically infeasible. This is extremely strong evidence against the event occurring (Likelihood Ratio <<< 1).

*   **Evidence 3: Lack of Public Signals.** There are no rumors, press releases, high-profile hires from the AI safety community, or any other public indicators that a collaboration between C3.ai and METR is being considered. For a partnership to be announced and an evaluation completed in the next 45 days, some groundwork would almost certainly be visible. The absence of signals is strong evidence that nothing is planned (Likelihood Ratio << 1).

### 4. Bayesian Updating (Posterior Probability)
*   **Prior Odds:** 1:99 (1% probability)
*   **Likelihood Ratios:** All key evidence provides strong negative updates. The timeline constraint is particularly powerful. Let's conceptually model its impact. If we estimate that the chance of completing all necessary steps in under two months from zero is, very generously, 1 in 100, the likelihood ratio would be approximately 1/100.
*   **Update Step:**
    *   Posterior Odds = Prior Odds × Likelihood Ratio
    *   Posterior Odds ≈ (1:99) × (1:100) ≈ 1:9900
    *   Posterior Probability ≈ 1 / (1 + 9900) ≈ 0.01%

This single update, which is arguably conservative, drives the probability to a near-zero value. The other pieces of evidence reinforce this conclusion. The final posterior probability is extremely low.

### 5. Red Team Thinking
I must consider how this forecast could be wrong. A "Yes" resolution could occur if:
1.  A partnership and evaluation have been underway in secret for months, and an announcement is imminent. This is unlikely given C3.ai's established business model and the difficulty of keeping such a development secret.
2.  C3.ai commissions a superficial, "token" evaluation from METR for PR purposes, and METR agrees. This seems out of character for METR, as it could damage their credibility.
3.  A "black swan" event like a sudden acquisition of a frontier lab by C3.ai occurs in the next 45 days. This is exceptionally improbable.

The most plausible counter-argument is the secret partnership scenario. However, the lack of any corroborating evidence, combined with the fundamental strategic mismatch, makes this a very remote possibility. The current Metaculus community forecast for this specific question is 1%, which aligns with the conclusion that this event is highly unlikely.

### 6. Final Forecast
The base rate for a company like C3.ai partnering with METR is near zero. The available evidence, especially the extreme logistical constraints imposed by the short timeline, provides a strong and consistent update in a negative direction. The resulting posterior probability is very close to zero. I will set my forecast at 1% to account for the small, unquantifiable possibility of a major surprise, aligning with the floor of common forecasting practice and the community consensus.

Final: 1%

```

#### Grok

- ok=False | t_ms=138 | tokens: prompt=0, completion=0, total=0 | cost=$0.000000

- error=HTTP 429: Your team 0101076a-72f1-4f25-a038-b346d74bbc8d has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.

### Aggregation (BMC)

- final_probability=0.0351

- bmc_summary={"mean": 0.03513292619024313, "p10": 2.7197754822086263e-10, "p50": 0.00046873796055104674, "p90": 0.1105410235087478}