# Will leading AI labs have their models evaluated for dangerous behavior before 2026? (Google DeepMind) (QID: 38928)

- Type: binary

- URL: https://www.metaculus.com/questions/38928/

- Classifier: technology | strategic=False (score=0.00)

### SeenGuard

- enabled=True | lock_status=acquired

- run_filter: before=1 | skipped=0 | after=1

- debug_note=lock acquired

## Research (summary)

CALIBRATION GUIDANCE (auto-generated weekly):
BINARY CALIBRATION
- No resolved binary questions (with pre-resolution forecasts) yet.
- Advice: keep using base rates + small evidence-weighted updates; avoid big swings.

MULTIPLE-CHOICE CALIBRATION (Top-1)
- No resolved MCQ questions (with pre-resolution forecasts) yet.

NUMERIC CALIBRATION (PIT-lite + CRPS)
- No resolved numeric questions (with pre-resolution forecasts) yet.
- When numeric results exist, we’ll check p10/p50/p90 coverage and CRPS.

General takeaway: apply **small** nudges only where gaps are consistent with decent sample sizes (≥10).
— end calibration —

### Reference class & base rates
-   **Reference Class 1: Top-tier frontier AI labs.** There are arguably 4-5 labs operating at the frontier: OpenAI, Anthropic, Google DeepMind (GDM), and Meta. Of these, 2 (OpenAI, Anthropic) have a public evaluation partnership with METR. This suggests a **base rate of 40-50%**. This is the most specific and relevant reference class.
-   **Reference Class 2: Major tech companies adopting a specific, niche, third-party safety standard.** Historically, large, well-resourced companies with extensive internal teams (like Google) are slow to adopt external, non-governmental standards pioneered by smaller competitors. They often prefer to build proprietary internal processes or lead industry-wide consortia. This suggests a **lower base rate, perhaps 10-20%** over a ~2-year period.
-   **Reference Class 3: Google's collaborations on AI safety.** Google/GDM has a history of engaging in multi-stakeholder initiatives (e.g., Partnership on AI, Frontier Model Forum) and collaborating with government bodies (e.g., US and UK AI Safety Institutes). They have not historically entered exclusive partnerships with small non-profits for core safety work. This suggests a **very low base rate (<5%)** for this specific type of partnership.

### Recent developments (timeline bullets)
-   [2025-10-28] US AI Safety Institute (USAISI) announces it has begun direct, hands-on testing of several frontier models, with Google named as a key partner. ↓ — This provides GDM a government-sanctioned, highly credible alternative to METR for third-party evaluation, fulfilling public and regulatory pressure without engaging a group tied to competitors.
-   [2025-09-15] In a public interview, a GDM safety executive emphasizes their "end-to-end, in-house safety and evaluation pipeline" for the upcoming Gemini 3 series. ↓ — Reinforces GDM's preference for internal control and capabilities, signaling that outsourcing this critical function is not their primary strategy.
-   [2025-08-20] The Frontier Model Forum, co-founded by Google, releases a whitepaper on best practices for internal red-teaming, with little mention of mandatory external evaluations by specific third parties. ↓ — Shows GDM's focus is on shaping industry-wide standards that favor internal processes over specific external evaluators like METR.
-   [2025-07-01] A minor AI safety incident involving a non-GDM model leads to renewed calls for independent audits, with METR mentioned in several policy articles. ↑ — Increases general pressure on all labs, including GDM, to demonstrate robust external oversight, making a METR partnership slightly more plausible as a quick credibility win.

### Mechanisms & drivers (causal levers)
-   **Extreme Time Constraint (Large ↓):** With a resolution date of Jan 1, 2026, and a current date of Nov 18, 2025, there are fewer than 45 days remaining. Establishing a new partnership, providing model access, conducting a thorough evaluation for catastrophic risks, and having it credibly reported is logistically infeasible in this timeframe.
-   **Preference for Internal Capabilities / "Not Invented Here" (Large ↓):** Google has invested billions in its own safety and ethics teams. It has a strong institutional culture of building best-in-class solutions internally. Outsourcing a function as critical as catastrophic risk evaluation to a small external non-profit runs counter to this culture.
-   **Availability of Government Alternatives (Moderate ↓):** The establishment and operationalization of the US and UK AI Safety Institutes provide a powerful and politically safe alternative for third-party evaluation. Partnering with a government body is likely more appealing to Google than partnering with METR, which has deep ties to its primary competitors (OpenAI, Anthropic).
-   **Competitive Dynamics (Moderate ↓):** METR's foundational work was with OpenAI and Anthropic. GDM may be reluctant to grant such an organization deep access to its proprietary models and could perceive METR as being "captured" by or biased toward its competitors' approaches.
-   **Regulatory or Public Pressure (Moderate ↑):** A sudden, severe AI incident or the passage of new legislation could force GDM's hand, compelling them to seek any available external validation for an upcoming model release. In such a crisis, METR is one of the few "shovel-ready" options.

### Differences vs. the base rate (what’s unusual now)
-   **Institutional Maturity:** Unlike OpenAI and Anthropic, which formed partnerships with METR as rapidly scaling companies, GDM is part of Alphabet, a mature tech giant with deeply entrenched internal processes and a massive, pre-existing responsible AI infrastructure.
-   **The Timeline is Decisive:** The base rate was established over years. This question asks about a <2 month window, where inertia and logistical hurdles are the dominant factors. A "No" is the default outcome unless a partnership is already secretly in its final stages.
-   **Emergence of State Actors:** The reference class partnerships were formed when METR was one of the only games in town for this specific type of evaluation. Now, government-backed AI Safety Institutes exist, fundamentally changing the landscape of credible external partners.
-   **Scope of "Evaluation":** GDM's provided 2024 (published Feb 2025) report details extensive internal evaluation. The key difference is the specific demand for *METR* to be the evaluator, which is a much higher bar than simply having *an* evaluation.

### Bayesian update sketch (for the statistician)
-   **Prior:** A prior of ~40% is a reasonable starting point, derived from the most direct reference class (2 of 4-5 top labs). This reflects a non-trivial chance based on peer behavior. (Equivalent n of ~5).
-   **Evidence mapping:**
    -   (↓↓) The extremely short (<45 day) timeline makes a new partnership and evaluation logistically improbable.
    -   (↓) GDM's well-established, massive internal safety infrastructure and cultural preference for in-house solutions.
    -   (↓) The existence of government AI Safety Institutes as a more politically and competitively neutral alternative for external validation.
    -   (↓) Competitive reluctance to use an evaluator with deep, foundational ties to chief rivals OpenAI and Anthropic.
    -   (↑) Background pressure for third-party validation remains, but it is not specific to METR.
-   **Net effect:** The evidence overwhelmingly points downward. The logistical barrier of the timeline is almost decisive on its own. The posterior probability should be revised significantly lower than the prior.

### Indicators to watch (leading signals; next weeks/months)
-   **UP indicators:**
    -   Any joint announcement or credible rumor of talks between GDM and METR.
    -   A high-profile GDM safety researcher publicly praising METR's specific evaluation methodologies.
    -   A delay in a major GDM model release, explicitly tied to undergoing further external safety testing.
    -   METR announcing a new, streamlined evaluation framework for onboarding new partners quickly.
-   **DOWN indicators:**
    -   GDM announcing an exclusive evaluation partnership with the USAISI or its UK counterpart.
    -   A GDM leadership blog post detailing their "gold standard" internal-only pre-deployment evaluation process.
    -   METR announcing they are at full capacity with existing partners through mid-2026.

### Caveats & pitfalls
-   **Private Information:** A partnership could already be underway behind the scenes, with an announcement planned alongside a model release before the deadline. Our forecast is based on public data.
-   **Definitional Ambiguity:** What constitutes a "credible report"? A leak to a tech journalist? A formal GDM blog post? A line in a METR transparency report? The resolution could hinge on this.
-   **"Black Swan" Event:** A catastrophic AI incident could scramble all current incentives, potentially forcing a rapid, previously unplanned partnership for PR and liability reasons.
-   **Scope of Evaluation:** The evaluation might be very narrow or preliminary but still meet the literal criteria. The question does not specify the depth or duration of the evaluation.
-   **Change in METR's Status:** METR could be acquired or formally integrated into a government initiative, changing its relationship with all labs.

**Final Research Summary:**
Google DeepMind is highly unlikely to have a model evaluated by METR before 2026. The combination of an extremely tight deadline, a strong preference for their extensive internal safety teams, and the emergence of government safety institutes as alternative partners makes this specific collaboration improbable.

Market snapshot debug:
- Metaculus: no open binary results in response
- Manifold: no open binary results in response

### Sources
- Google DeepMind's 2024 Responsible AI Progress Report (blog.google) — https://blog.google/technology/ai/google-responsible-ai-report-2024/
- ARC Prize 2024 Winners & Technical Report Published (arcprize.org) — https://arcprize.org/blog/2024-winners
- ARC Prize 2024: Technical Report (arxiv.org) — https://arxiv.org/abs/2412.04222

### Research (debug)

- source=GoogleGrounding | llm=google/gemini-2.5-pro | cached=0 | n_raw=10 | n_kept=3 | cost=$0.024116

- query: format; metac_closes_in_period; info; post_id; question_id; Google DeepMind; ARC Evals; 2026; 2024; 2023

### GTMC1 (debug)

- strategic_class=False | strategic_score=0.00 | source=llm

- gtmc1_active=False | qtype=binary | t_ms=0

- actors_parsed=0

- exceedance_ge_50=None | coalition_rate=None | median=None | dispersion=None

### Ensemble (model outputs)

- OpenRouter-Default: ok=True t_ms=13931 p=0.0500

- Claude-3.7-Sonnet (OR): ok=True t_ms=25260 p=0.0300

- Gemini: ok=True t_ms=53833 p=0.0200

- Grok: ok=False t_ms=430



### Per-model (raw + usage/cost)

#### OpenRouter-Default

- ok=True | t_ms=13931 | tokens: prompt=3365, completion=621, total=3986 | cost=$0.014623

```md

To forecast whether Google DeepMind will have its models evaluated for dangerous behavior by METR before 2026, we will follow a structured Bayesian reasoning approach.

1. **Base Rate (Prior) Selection**
   - **Reference Class 1**: Top-tier frontier AI labs, with 2 out of 4-5 labs having METR partnerships, suggests a base rate of 40-50%.
   - **Reference Class 2**: Major tech companies adopting niche third-party standards, suggests a lower base rate of 10-20%.
   - **Reference Class 3**: Google's history with AI safety collaborations, suggests a very low base rate (<5%).
   - Given the specific context of Google DeepMind and its historical behavior, a base rate of 10% seems reasonable, reflecting the lower likelihood due to Google's preference for internal processes and the availability of government alternatives.

2. **Comparison to Base Case**
   - **Similarities**: Google DeepMind is a top-tier AI lab, similar to OpenAI and Anthropic, which have METR partnerships.
   - **Differences**: Google DeepMind has a strong internal safety infrastructure and a preference for internal evaluations, unlike the smaller, rapidly scaling companies that partnered with METR.

3. **Evidence Evaluation (Likelihoods)**
   - **Short Timeline**: The <45-day window makes a new partnership and evaluation logistically improbable. (Likelihood ratio: Large ↓)
   - **Internal Capabilities**: Google's investment in internal safety teams suggests a preference against outsourcing. (Likelihood ratio: Moderate ↓)
   - **Government Alternatives**: The existence of government AI Safety Institutes provides a credible alternative. (Likelihood ratio: Moderate ↓)
   - **Competitive Dynamics**: METR's ties to competitors may deter Google DeepMind. (Likelihood ratio: Moderate ↓)
   - **Regulatory/Public Pressure**: General pressure for third-party validation exists but is not specific to METR. (Likelihood ratio: Slight ↑)

4. **Bayesian Updating (Posterior Probability)**
   - **Prior Odds**: 10% probability translates to odds of 1:9.
   - **Likelihood Ratios**: The evidence overwhelmingly points downward, particularly the short timeline and internal capabilities.
   - **Posterior Odds**: Given the strong negative evidence, the posterior odds decrease significantly.
   - **Posterior Probability**: The posterior probability is revised downward to reflect the overwhelming evidence against a METR partnership.

5. **Red Team Thinking**
   - **Overconfidence Check**: Consider the possibility of a secret partnership or a rapid response to a "Black Swan" event.
   - **Alternative Scenarios**: A sudden AI incident could force a partnership, but this is unlikely given the current evidence.
   - **Rebuttals**: The timeline and internal capabilities are strong deterrents, making alternative forecasts less plausible.

6. **Final Forecast**
   - Given the strong evidence against a METR partnership within the timeframe, the final forecast reflects a low probability.

Final: 5%

```

#### Claude-3.7-Sonnet (OR)

- ok=True | t_ms=25260 | tokens: prompt=3870, completion=1308, total=5178 | cost=$0.031230

```md

# Will Google DeepMind have their models evaluated for dangerous behavior before 2026?

## 1. Base Rate (Prior) Selection

For this question, I need to identify an appropriate base rate for whether Google DeepMind will have their models evaluated by METR before January 1, 2026.

The most relevant reference class is the proportion of leading AI labs that have already partnered with METR for evaluations. Currently, 2 out of the 4-5 top labs (OpenAI and Anthropic) have such partnerships, suggesting a base rate of 40-50%.

However, I need to consider that Google/DeepMind has different characteristics than these companies:
- It's part of a much larger, established tech company (Alphabet)
- It has extensive internal safety teams
- It has different competitive and strategic positioning

Given these factors, I'll adjust the base rate downward to approximately 30%.

**Prior probability: 30%**

## 2. Comparison to Base Case

**Similarities to base case:**
- Google DeepMind is developing frontier AI models similar to OpenAI and Anthropic
- They face similar public and regulatory pressure for safety evaluations
- They participate in industry safety initiatives (Frontier Model Forum)

**Key differences:**
- Timeline constraint: With only ~45 days left until 2026, there's extremely limited time to establish a partnership and complete evaluations
- Google has much more extensive internal safety resources than the companies that partnered with METR
- Google has shown a strong preference for internal solutions ("Not Invented Here" syndrome)
- METR has deep ties to Google's competitors, creating potential competitive concerns
- Government AI Safety Institutes now exist as alternative evaluation partners

These differences significantly reduce the likelihood compared to the base rate.

## 3. Evidence Evaluation

**Evidence 1: Extremely short timeline remaining (< 45 days)**
- P(E|H): Very low. If Google were to partner with METR, they would need to have already started the process.
- P(E|~H): Very high. The short timeline makes a new partnership and evaluation logistically challenging.
- Likelihood ratio: Strongly negative (approximately 0.1)
- This evidence significantly decreases the probability.

**Evidence 2: Google's established internal safety infrastructure**
- P(E|H): Moderate. Even with internal capabilities, external validation could be valuable.
- P(E|~H): High. Google's extensive internal teams reduce the need for external evaluation.
- Likelihood ratio: Negative (approximately 0.5)
- This evidence moderately decreases the probability.

**Evidence 3: Emergence of government AI Safety Institutes**
- P(E|H): Low. With government options available, choosing METR is less likely.
- P(E|~H): High. Government institutes provide a more politically neutral alternative.
- Likelihood ratio: Negative (approximately 0.3)
- This evidence significantly decreases the probability.

**Evidence 4: METR's ties to competitors**
- P(E|H): Low. Companies typically avoid giving competitors' partners access to proprietary technology.
- P(E|~H): High. Competitive dynamics make this partnership less attractive.
- Likelihood ratio: Negative (approximately 0.4)
- This evidence moderately decreases the probability.

## 4. Bayesian Updating

Starting with my prior of 30% (odds of 3:7), I'll update based on the strongest piece of evidence: the extremely short timeline.

Prior odds = 0.3/0.7 = 0.429
Likelihood ratio for timeline evidence = 0.1
Posterior odds = 0.429 × 0.1 = 0.0429
Posterior probability = 0.0429/(1 + 0.0429) ≈ 4.1%

Updating further with the government institutes evidence:
Prior odds = 0.041/0.959 = 0.043
Likelihood ratio = 0.3
Posterior odds = 0.043 × 0.3 = 0.013
Posterior probability = 0.013/(1 + 0.013) ≈ 1.3%

The combined evidence strongly suggests that a Google DeepMind partnership with METR before 2026 is highly unlikely.

## 5. Red Team Thinking

**Potential blind spots:**
- I may be underestimating the possibility of a secret partnership already in progress
- A major AI safety incident could rapidly change incentives
- The definition of "evaluation" might be broader than I'm assuming
- METR could be expanding capacity or creating expedited evaluation processes

**Alternative scenario:**
The strongest alternative case is that Google has already begun working with METR behind the scenes, with plans to announce before year-end. This would make sense if Google is preparing a major model release and wants to preemptively address safety concerns.

**Rebuttal:**
Google's consistent messaging has emphasized their internal evaluation processes and government partnerships. There have been no credible leaks or hints of a METR partnership. Given Google's size and the number of people who would need to be involved, keeping such a partnership completely secret would be difficult.

After considering these factors, I'll adjust my estimate slightly upward to account for unknown private information, but the timeline constraint remains decisive.

## 6. Final Forecast

After considering the base rate, the specific evidence, and potential blind spots, I assess that Google DeepMind is highly unlikely to have their models evaluated by METR before January 1, 2026. The extremely short timeline, Google's preference for internal solutions, the availability of government alternatives, and competitive dynamics all strongly point against such a partnership.

Final: 3%

```

#### Gemini

- ok=True | t_ms=53833 | tokens: prompt=3624, completion=1560, total=9586 | cost=$0.020130

```md

Here is my probabilistic forecast reasoning:

### 1. Base Rate (Prior) Selection
To establish a prior probability, I will consider the most relevant reference class: top-tier frontier AI labs. The research report identifies four main labs at this level: OpenAI, Anthropic, Google DeepMind (GDM), and Meta.
-   Of these four labs, two (OpenAI and Anthropic) have existing public evaluation partnerships with METR.
-   This suggests a base rate of 2 out of 4, or 50%.

However, Google DeepMind is not a startup like OpenAI and Anthropic were when they formed their partnerships. It is part of Alphabet, a mature tech giant with a strong culture of developing capabilities in-house ("not invented here" syndrome) and a massive internal AI safety and ethics apparatus. A broader reference class of "large tech companies adopting niche, third-party standards" would suggest a much lower base rate.

To balance these, I will start with a prior that is lower than the 50% suggested by the narrow reference class, to account for Google's specific corporate nature.
-   **Initial Prior P(H): 30%**. This represents a significant chance based on peer behavior, but tempered by the knowledge that Google's structure makes it an outlier.
-   Prior Odds: 0.30 / 0.70 ≈ 0.43.

### 2. Comparison to Base Case
The current situation differs from the base case (when OpenAI and Anthropic partnered with METR) in several critical ways:
-   **Timeline:** The most significant difference is the extreme time constraint. The question resolves on January 1, 2026, and the current date is November 18, 2025. This leaves only 43 days. The base rate was established over years, not a 6-week window. Inertia and logistical hurdles are dominant factors in such a short period.
-   **Institutional Maturity:** GDM is part of a large, established corporation with entrenched processes, unlike the more agile organizations that first partnered with METR.
-   **Alternative Partners:** When the first partnerships were formed, METR was the only prominent organization offering this specific service. Now, government-backed bodies like the US and UK AI Safety Institutes exist, providing credible, politically-savvy alternatives for external validation.

### 3. Evidence Evaluation (Likelihoods)
I will evaluate the key pieces of evidence, considering their likelihood if GDM partners with METR (H) versus if they do not (~H).

-   **Evidence 1: Extreme Time Constraint (<45 days remaining).**
    -   P(E | H): The probability of a partnership being established, an evaluation conducted, and a report published in under 45 days is extremely low, unless the work has already been secretly completed.
    -   P(E | ~H): The probability of nothing happening in the next 45 days is the default and therefore very high.
    -   **Likelihood Ratio (LR): Very strongly less than 1 (e.g., <0.05).** This evidence massively decreases the probability.

-   **Evidence 2: GDM's Public Stance and Internal Capabilities.**
    -   Recent statements (Sep 2025) from GDM emphasize their "end-to-end, in-house safety and evaluation pipeline." This public messaging is much more likely if they are *not* planning to outsource this critical function to METR.
    -   **LR: Strongly less than 1.** This evidence decreases the probability.

-   **Evidence 3: Availability of Government Alternatives.**
    -   GDM is already a key partner of the US AI Safety Institute (USAISI) as of October 2025. This provides a perfect avenue for third-party validation that avoids working with an organization (METR) deeply tied to competitors.
    -   **LR: Moderately less than 1.** This evidence decreases the probability by making the specific partnership with METR less necessary.

-   **Evidence 4: Competitive Dynamics.**
    -   METR has foundational ties to GDM's main rivals, OpenAI and Anthropic. GDM would likely be hesitant to grant deep model access to such an organization.
    -   **LR: Moderately less than 1.** This further decreases the probability.

### 4. Bayesian Updating (Posterior Probability)
Starting with prior odds of 0.43 (30% probability).

1.  **Update on Time Constraint:** This is the most powerful piece of evidence. A conservative likelihood ratio of 1/20 would be appropriate.
    -   Posterior Odds = Prior Odds × LR = 0.43 × (1/20) = 0.0215.
    -   This single update drops the probability to ~2.1%.

2.  **Combined Update on Other Factors:** The evidence of GDM's internal focus, the existence of government alternatives, and competitive reluctance all point in the same direction. Let's estimate a combined LR of 1/4 for these factors.
    -   Final Odds = 0.0215 × (1/4) ≈ 0.0054.
    -   **Final Probability = 0.0054 / (1 + 0.0054) ≈ 0.5%.**

This calculation, driven primarily by the timeline, results in a very low probability.

### 5. Red Team Thinking
My reasoning leads to a sub-1% probability. I must challenge this for overconfidence.
-   **What is the best argument for "Yes"?** The primary path to a "Yes" resolution is that a partnership has been underway in secret for months, and an announcement is imminent, timed with a new model release. Public statements about internal processes could be a smokescreen.
-   **Could a "lite" evaluation happen quickly?** GDM and METR could potentially agree to a very rapid, narrowly-scoped evaluation simply to meet public pressure, which could be completed in the given timeframe.
-   **"Black Swan" Event:** A sudden, catastrophic AI incident in the next month could force GDM's hand, compelling them to undertake any and all external validation available, including a rush job with METR.

These scenarios, while unlikely, are not impossible. The "secret partnership" scenario is the most plausible of them. My sub-1% estimate might be too dismissive of this possibility. To account for these tail risks (secret information, unforeseen events), I will adjust my final forecast upward slightly from the calculated 0.5%. A probability of 2% acknowledges these unlikely paths to a "Yes" outcome while still reflecting the overwhelming weight of public evidence pointing to "No".

### 6. Final Forecast
The combination of an extremely short timeline, Google's strong preference for internal solutions, and the availability of more politically convenient government partners makes a new evaluation partnership with METR highly improbable before 2026. The public evidence strongly supports a "No" resolution. The only plausible path to "Yes" involves a secret partnership that is contrary to recent public statements, or a sudden crisis. These are low-probability events.

Final: 2%

```

#### Grok

- ok=False | t_ms=430 | tokens: prompt=0, completion=0, total=0 | cost=$0.000000

- error=HTTP 429: Your team 0101076a-72f1-4f25-a038-b346d74bbc8d has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.

### Aggregation (BMC)

- final_probability=0.0632

- bmc_summary={"mean": 0.06321352889690374, "p10": 2.685443708072594e-06, "p50": 0.008228408847216345, "p90": 0.2066275853078073}