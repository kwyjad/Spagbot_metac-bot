# What will be the largest number of digits of π to have ever been computed by December 31, 2025? (QID: 38944)

- Type: numeric

- URL: https://www.metaculus.com/questions/38944/

- Classifier: science | strategic=False (score=0.00)

### SeenGuard

- enabled=True | lock_status=acquired

- run_filter: before=1 | skipped=0 | after=1

- debug_note=lock acquired

## Research (summary)

CALIBRATION GUIDANCE (auto-generated weekly):
BINARY CALIBRATION
- No resolved binary questions (with pre-resolution forecasts) yet.
- Advice: keep using base rates + small evidence-weighted updates; avoid big swings.

MULTIPLE-CHOICE CALIBRATION (Top-1)
- No resolved MCQ questions (with pre-resolution forecasts) yet.

NUMERIC CALIBRATION (PIT-lite + CRPS)
- No resolved numeric questions (with pre-resolution forecasts) yet.
- When numeric results exist, we’ll check p10/p50/p90 coverage and CRPS.

General takeaway: apply **small** nudges only where gaps are consistent with decent sample sizes (≥10).
— end calibration —

### Research Plan

**1. Core Questions to Investigate:**
*   What is the current, verified world record for the number of digits of π? The provided sources are from 2022 and may be outdated.
*   What is the historical trend of record-breaking computations, specifically focusing on the frequency and magnitude of new records in the last 10-15 years?
*   Who are the key actors (individuals, corporations, academic institutions) currently competing to set new records?
*   What are the technical requirements (time, hardware, storage) for setting a new record, based on the most recent attempts?
*   Given the short time horizon (less than 2 months), is there any public information about an ongoing record attempt that could conclude before December 31, 2025?
*   What are the primary drivers and constraints for new records (e.g., Moore's Law, storage costs, algorithmic improvements, motivation)?

**2. Search Queries:**
*   `"world record" "pi digits" computed 2023 2024 2025`
*   `"chronology of computation of pi" wikipedia`
*   `y-cruncher pi record list`
*   `StorageTeam pi calculation 105 trillion`
*   `Emma Haruka Iwao pi next record`
*   `"pi computation" ongoing attempt`
*   `"Chudnovsky algorithm" performance improvements`
*   `cost of petabyte storage trends`

**3. Synthesis of Findings:**
*   The current record is **105 trillion digits**, set by Jordan Ranous and the StorageTeam. The computation finished on December 27, 2023, and was announced on Pi Day, March 14, 2024. This supersedes the 100 trillion digit record from 2022 mentioned in the provided sources.
*   The historical trend shows records are broken every 1-2 years. The magnitude of the increase has varied, from a 5% jump (100T to 105T) to a ~3x jump (31.4T to 100T).
*   Key actors include Google (Emma Haruka Iwao), enthusiast groups (StorageTeam), and academic institutions (UAS Grisons). This indicates a competitive, multi-polar environment.
*   The 105T record took 75 days of computation and used approximately 1.3 petabytes of storage. The 100T record took 157 days. This implies a new record attempt would require a multi-month commitment.
*   There are no public announcements of an ongoing record attempt scheduled to finish before the end of 2025. The lead time for preparation, computation, and verification makes a surprise record highly improbable within the remaining timeframe.
*   Drivers are continued improvements in CPU performance and, critically, the falling cost and increasing speed of mass storage (NVMe SSDs). The main constraint is the sheer time and resources required for computation and verification.

**4. Drafting the Research Brief:**
The synthesized findings will now be structured into the required format for the Bayesian forecasting panel.

***

### Reference class & base rates
*   **Reference Class 1: Post-2010 Pi Computation Records.** Since 2010, a new record has been set approximately every 1.5 years (8 records in ~13 years). The number of digits typically increases by a factor of 1.05x to 3.2x over the previous record. This class suggests a new record is plausible within the 2024-2025 timeframe.
*   **Reference Class 2: Major Corporate/Enthusiast "Big Data" Demonstrations.** Projects like large-scale protein folding, prime number searches, or Pi computation serve as benchmarks for hardware and cloud infrastructure. These occur irregularly but are often timed for publicity (e.g., Pi Day). The base rate is low for any specific short window but confirms the existence of motivated actors.
*   **Limitations:** These base rates describe the general frequency over years, not the probability within a specific <2 month window. The timing is driven by project readiness, not a fixed schedule.

### Recent developments (timeline bullets)
*   [2024-03-14] StorageTeam announces a new record of 105 trillion digits, computed in late 2023. (↓) — This resets the clock on a "new" record and establishes a higher, more difficult-to-beat benchmark. The current record is more recent than the 2022 record mentioned in the background.
*   [2024-Present] No public announcements of a new, ongoing record attempt by major players like Google, StorageTeam, or others. (↓↓) — Given the multi-month computation time, the lack of news about an attempt that would have started mid-2025 makes a new record by year-end extremely unlikely.

### Mechanisms & drivers (causal levers)
*   **Computation Time (Large):** The most significant driver. Recent records took 75-157 days for the main computation, plus verification. This makes a new record impossible to start and finish before the resolution date.
*   **Hardware Advancement (Moderate):** Continuous improvements in CPU core counts and, more importantly, the speed and cost-per-terabyte of SSD storage, steadily lower the barrier to entry and enable larger calculations. This is a constant upward pressure over time.
*   **Actor Motivation (Moderate):** Competition between corporate entities (Google showcasing cloud prowess) and dedicated enthusiast groups (StorageTeam demonstrating hardware limits) drives attempts. A lull in motivation or a shift in priorities could delay the next attempt.
*   **Algorithmic Efficiency (Small):** The dominant Chudnovsky algorithm is highly optimized. Further gains are marginal and unlikely to dramatically shorten computation times in the short term.
*   **Publicity & Funding (Small):** The desire for a Pi Day (March 14) announcement can influence the timing of projects, suggesting a lower likelihood of a major announcement in late Q4.

### Differences vs. the base rate (what’s unusual now)
*   **Extreme Time Constraint:** The primary difference. The base rate suggests a record is "due," but the specific <2 month window to resolution is shorter than the known computation time of recent records.
*   **Known Current Record is Recent:** The current 105T digit record was announced in March 2024. While the base rate is ~1.5 years, an interval of 21 months (Mar 2024 - Dec 2025) is on the longer side but not exceptionally so. The key is that any attempt to break it would have already needed to be well underway.
*   **Shift to "Prosumer" Hardware:** The 105T record was set not on a massive cloud platform but on high-end, commercially available hardware. This potentially increases the number of actors who could attempt a record, but doesn't change the fundamental time constraints.

### Bayesian update sketch (for the statistician)
*   **Prior:** Start with a prior centered on the current record of **105 trillion digits**. Based on the historical frequency and growth rate, assign a non-trivial probability (perhaps 30-40%) to a new record being set sometime between the last one (announced Mar 2024) and the resolution date (Dec 2025). The distribution for a new record might be centered around 120-150 trillion.
*   **Evidence mapping:**
    *   (↓↓↓) The resolution date is less than two months away, which is shorter than the 75+ day computation time required.
    *   (↓↓) There have been no announcements of an ongoing attempt that would conclude in Q4 2025. Such projects are rarely kept entirely secret.
    *   (↓) The current record of 105T was set relatively recently (announced Mar 2024), slightly reducing the immediate pressure for a new one.
    *   (↑) Moore's Law and storage cost reductions are a constant, gentle tailwind making a new record easier than the last.
*   **Net effect:** The evidence overwhelmingly points to the current record holding. The posterior probability should collapse almost entirely onto the current record of 105 trillion. The initial prior allowing for a new record should be revised sharply downwards.

### Indicators to watch (leading signals; next weeks/months)
*   **UP indicators:**
    *   A surprise announcement from any major actor (Google, StorageTeam, a university) of a nearly-completed computation.
    *   A major, unexpected breakthrough in integer multiplication algorithms is announced (extremely unlikely).
    *   A credible rumor on a high-performance computing forum (e.g., HWBOT) about a secret ongoing run.
*   **DOWN indicators:**
    *   Continued silence from all known actors in the field.
    *   Pi Day (March 14, 2026) is mentioned by a key actor as the target for their *next* announcement.
    *   Each passing day without an announcement further solidifies the status quo.

### Caveats & pitfalls
*   **Secret Computation:** The largest risk is a well-kept secret. An individual or group could be running a computation privately to make a surprise announcement. However, the scale of the hardware required makes this difficult to hide completely.
*   **Resolution Criteria Ambiguity:** The question resolves based on "credible media reports." A record could be *achieved* on Dec 30, 2025, but not verified and reported until Jan 2026. This would likely resolve to the old value.
*   **Failed Attempt:** A team could have been attempting a record with the goal of finishing by year-end, only for it to fail due to hardware or other issues, and we would not hear about it.
*   **Typo/Data Entry Error:** The current record is 105 trillion (1.05 x 10^14). Ensure this exact number is used, not the outdated 100 trillion figure.

**Final Research Summary:**
The current record is 105 trillion digits, announced in March 2024. Given that recent record-breaking computations require over 75 days, and there are no public announcements of an ongoing attempt, it is overwhelmingly likely that the 105 trillion digit record will remain the largest number computed by December 31, 2025.

Market snapshot debug:
- Metaculus: no open binary results in response
- Manifold: no open binary results in response

### Sources
- Pi calculated to 100 trillion digits by Google engineer (bandwidthblog.com) — https://bandwidthblog.com/2022/06/21/pi-calculated-to-100-trillion-digits-by-google-engineer/
- This Google employee just smashed the world record for calculating pi () — https.www.cbsnews.com/news/pi-world-record-google-employee-emma-haruka-iwao-just-smashed-the-guinness-world-record-of-calculating-pi/
- Google Celebrates Pi Day As Employee Calculates New World Record (time.com) — https://time.com/5551344/pi-day-record-google-emma-haruka-iwao/
- Emma Haruka beats world record for most-calculated digits of pi (cfo.co.za) — https://cfo.co.za/articles/emma-haruka-beats-world-record-for-most-calculated-digits-of-pi/
- Google developer Emma Haruka Iwao has calculated Pi to 100 trillion digits (www.engadget.com) — https://www.engadget.com/google-pi-100-trillion-digits-144946191.html
- A Google Cloud employee set a new record calculating the 100 trillionth digit of pi (www.fastcompany.com) — https://www.fastcompany.com/90758486/a-google-cloud-employee-set-a-new-record-calculating-the-100-trillionth-digit-of-pi
- A Google employee just shattered the record for Pi calculations. Her name is Emma Haruka Iwao (www.washingtonpost.com) — https://www.washingtonpost.com/nation/2019/03/14/google-employee-just-shattered-record-pi-calculations-her-name-is-emma-haruka-iwao/
- Emma Haruka Iwao - Wikipedia (en.wikipedia.org) — https://en.wikipedia.org/wiki/Emma_Haruka_Iwao
- The longest version of the number Pi is calculated (www.reporteri.net) — https://www.reporteri.net/lifestyle/e-majtas-llogaritet-versioni-me-i-gjate-i-numrit-pi
- Pi calculated to 100 trillion digits by Google Cloud employee (www.tribuneindia.com) — https://www.tribuneindia.com/news/science-technology/pi-calculated-to-100-trillion-digits-by-google-cloud-employee-403001
- Pi dream: Google employee sets record for most digits calculated (www.cnet.com) — https://www.cnet.com/tech/computing/pi-dream-google-employee-sets-record-for-most-digits-calculated/

### Research (debug)

- source=GoogleGrounding | llm=google/gemini-2.5-pro | cached=0 | n_raw=12 | n_kept=11 | cost=$0.026760

- query: format; metac_closes_in_period; info; post_id; question_id; In March; Emma Haruka Iwao; 2025; 2019

### GTMC1 (debug)

- strategic_class=False | strategic_score=0.00 | source=llm

- gtmc1_active=False | qtype=numeric | t_ms=0

- actors_parsed=0

- exceedance_ge_50=None | coalition_rate=None | median=None | dispersion=None

### Ensemble (model outputs)

- OpenRouter-Default: ok=True t_ms=20085 P10=31.000, P50=2025.000, P90=1.000

- Claude-3.7-Sonnet (OR): ok=True t_ms=23144 P10=31.000, P50=2025.000, P90=1.000

- Gemini: ok=True t_ms=59887 P10=6.000, P50=2025.000, P90=1.000

- Grok: ok=False t_ms=75



### Per-model (raw + usage/cost)

#### OpenRouter-Default

- ok=True | t_ms=20085 | tokens: prompt=3781, completion=515, total=4296 | cost=$0.014603

```md

To forecast the largest number of digits of π to have ever been computed by December 31, 2025, we will follow the structured Bayesian reasoning process:

1. **Base Rate (Prior) Selection**
   - The base rate is derived from the historical trend of π digit computation records. Since 2010, records have been set approximately every 1.5 years, with increases ranging from 1.05x to 3.2x over the previous record. The current record is 105 trillion digits, set in December 2023. The base rate suggests that a new record could be set within the 2024-2025 timeframe, with a potential increase to around 120-150 trillion digits.

2. **Comparison to Base Case**
   - Similarities: The historical pattern of record-setting continues, with advancements in hardware and storage making larger computations feasible.
   - Differences: The time constraint is significant, as the computation and verification process takes over 75 days, and no ongoing attempts have been publicly announced. This makes it unlikely for a new record to be set by the end of 2025.

3. **Evidence Evaluation (Likelihoods)**
   - The recent record of 105 trillion digits was announced in March 2024, resetting the clock for a new record.
   - No public announcements of ongoing attempts suggest a low likelihood of a new record by year-end.
   - Hardware and storage improvements provide a constant upward pressure but are insufficient to overcome the time constraint.

4. **Bayesian Updating (Posterior Distribution)**
   - Prior centered on 105 trillion digits, with some probability for a new record (120-150 trillion).
   - Evidence overwhelmingly supports the current record holding due to the time constraint and lack of announcements.
   - Posterior distribution shifts towards the current record, with a very low probability of a new record.

5. **Red Team Thinking**
   - Consider the possibility of a secret computation, but the scale of resources required makes this unlikely.
   - The resolution criteria require credible media reports, reducing the risk of a surprise record.
   - Adjust the forecast to account for these considerations, maintaining a focus on the current record.

6. **Final Percentiles**
   - Given the overwhelming evidence and constraints, the forecast is heavily weighted towards the current record of 105 trillion digits.

P10: 105 trillion
P20: 105 trillion
P40: 105 trillion
P60: 105 trillion
P80: 105 trillion
P90: 105 trillion

```

#### Claude-3.7-Sonnet (OR)

- ok=True | t_ms=23144 | tokens: prompt=4291, completion=1227, total=5518 | cost=$0.031278

```md

# Forecasting the Largest Number of Digits of π Computed by December 31, 2025

## 1. Base Rate (Prior) Selection

For my prior, I'll use the chronology of π computation records, focusing on recent history:

- March 2019: 31.4 trillion digits (Emma Haruka Iwao, Google)
- August 2021: 62.8 trillion digits (University of Applied Sciences, Switzerland)
- June 2022: 100 trillion digits (Emma Haruka Iwao, Google)
- March 2024: 105 trillion digits (Jordan Ranous, StorageTeam)

The historical pattern shows:
- Records typically increase by 1.05x to 3.2x
- New records occur approximately every 1.5-2 years
- The current record is 105 trillion digits (1.05 × 10^14)

This gives me a prior distribution centered at 105 trillion digits, with some probability mass on higher values (120-150 trillion) based on historical growth rates.

## 2. Comparison to Base Case

**Similarities to base case:**
- The computational challenge remains similar (using Chudnovsky algorithm)
- Hardware improvements continue to follow Moore's Law
- Storage costs continue to decrease
- Motivated actors (corporations, enthusiasts, academics) still exist

**Differences from base case:**
- Time constraint: Less than 2 months remain until December 31, 2025
- Recent record: The current record was set relatively recently (announced March 2024)
- No public announcements: No known ongoing computation attempts

## 3. Evidence Evaluation (Likelihoods)

**Evidence 1: Computation time requirements**
- Recent records required 75-157 days for computation alone
- Likelihood ratio: Strongly favors no new record (~100:1) as there's insufficient time to start and complete a new record before December 31, 2025

**Evidence 2: Absence of announcements**
- No public announcements of ongoing record attempts
- Such large-scale projects are rarely kept completely secret
- Likelihood ratio: Strongly favors no new record (~20:1)

**Evidence 3: Recency of current record**
- The 105 trillion digit record was announced in March 2024
- While the typical cycle is 1.5-2 years, this puts the next expected record around September 2025-March 2026
- Likelihood ratio: Moderately favors no new record (~3:1)

**Evidence 4: Hardware advancement**
- Continuous improvements in CPU performance and storage costs
- Makes future records easier but doesn't overcome time constraints
- Likelihood ratio: Slightly favors a new record (~1:1.2) in general, but not within the timeframe

## 4. Bayesian Updating (Posterior Distribution)

Starting with my prior centered at 105 trillion with some probability of higher values:

P(105T) = 0.6
P(>105T) = 0.4

Applying Evidence 1 (computation time):
P(105T | Evidence 1) ∝ 0.6 × 1 = 0.6
P(>105T | Evidence 1) ∝ 0.4 × 0.01 = 0.004
Normalizing: P(105T) ≈ 0.993, P(>105T) ≈ 0.007

Applying Evidence 2 (no announcements):
P(105T | Evidence 2) ∝ 0.993 × 1 = 0.993
P(>105T | Evidence 2) ∝ 0.007 × 0.05 = 0.00035
Normalizing: P(105T) ≈ 0.9996, P(>105T) ≈ 0.0004

The posterior distribution is now heavily concentrated at 105 trillion digits, with only a tiny probability of a higher value.

## 5. Red Team Thinking

**Potential blind spots:**
- Secret computation: A team could be working in secret, though this is unlikely given hardware requirements
- Algorithmic breakthrough: An unexpected mathematical advancement could reduce computation time
- Misunderstanding the question: The question asks for the largest number "ever computed," not necessarily a new record

**Alternative scenarios:**
- A team started a computation months ago and is about to announce completion
- A record was already set but not yet publicly announced
- A smaller record (e.g., 110 trillion) might be achievable in less time

**Adjustments:**
I should slightly increase the probability of values above 105 trillion to account for unknown unknowns and the possibility of secret computations. However, the time constraint remains the dominant factor.

## 6. Final Percentiles

Given the overwhelming evidence that the current record of 105 trillion digits will remain the largest by December 31, 2025, my distribution is heavily concentrated at this value, with small probability mass for higher values to account for unknown possibilities:

P10: 105000000000000
P20: 105000000000000
P40: 105000000000000
P60: 105000000000000
P80: 105000000000000
P90: 110000000000000

```

#### Gemini

- ok=True | t_ms=59887 | tokens: prompt=4249, completion=1607, total=10882 | cost=$0.021381

```md

Answering based on the provided information and general knowledge as of November 6, 2025.

### 1. Base Rate (Prior) Selection
The base rate is derived from the historical frequency and magnitude of new records for the computation of π digits, particularly in the modern computing era (post-2010).

*   **Reference Class:** World records for π digits computed since 2010.
*   **Reasoning:** This period is characterized by intense competition between corporate entities (like Google) and enthusiast groups, driven by rapid advances in processing power and, crucially, mass storage technology.
*   **Distribution:**
    *   **Frequency:** A new record has been set approximately every 1-1.5 years. The last record was computed in late 2023 (announced March 2024). The resolution date is Dec 31, 2025, roughly two years after the last computation finished. Based on frequency alone, a new record within this window is plausible.
    *   **Magnitude:** Jumps typically range from a factor of 1.05x to over 1.6x the previous record.
    *   **Prior:** My prior distribution, based on this historical pattern, would be bimodal. There would be a significant probability (perhaps 40%) that the record remains at the current value of 105 trillion, and a corresponding probability (60%) of a new record, likely in the range of 115 to 170 trillion digits.

### 2. Comparison to Base Case
*   **Similarities:** The underlying drivers of technological improvement (faster CPUs, cheaper/faster SSDs) and motivation (corporate PR, enthusiast competition) are still present, consistent with the reference class.
*   **Differences:** The most critical difference is the **extremely short time horizon**. The forecast is for December 31, 2025, which is less than two months from the current date (November 6, 2025). This is the single most important factor distinguishing this specific forecast from the general base rate.

### 3. Evidence Evaluation (Likelihoods)
I will evaluate the evidence to update my prior belief.

*   **Evidence 1: Remaining Time vs. Computation Time (Extremely Strong, Downward Pressure)**
    *   The current record of 105 trillion digits took 75 days to compute. The prior record of 100 trillion took 157 days.
    *   There are only about 55 days remaining until December 31, 2025.
    *   **Likelihood:** This evidence makes a new record extremely unlikely. A new computation cannot be started *now* and finished before the deadline. The only possibility for a new record is if a computation started many months ago and is coincidentally scheduled to finish, be verified, and be reported in the next 55 days. This evidence makes the "no new record" outcome vastly more likely than the "new record" outcome. The likelihood ratio P(Evidence | New Record) / P(Evidence | No New Record) is vanishingly small.

*   **Evidence 2: Lack of Public Announcements (Strong, Downward Pressure)**
    *   Major π computation projects, due to their scale and resource requirements, are rarely kept completely secret until the moment of announcement. There are no public statements or credible rumors from key players (Google, StorageTeam, etc.) about an ongoing attempt set to conclude in late 2025.
    *   **Likelihood:** This silence is highly consistent with no record attempt being close to completion. It makes the "secret project" scenario less probable.

*   **Evidence 3: Verification and Reporting Lag (Moderate, Downward Pressure)**
    *   The resolution requires the record to be established "according to credible media reports." After a computation finishes, it must be verified, and a press release or announcement must be prepared and disseminated. This process adds days or even weeks to the timeline, further constraining the possibility of a new record resolving by the deadline.

### 4. Bayesian Updating (Posterior Distribution)
*   **Conceptual Update:** Posterior ∝ Prior × Likelihood
*   **Step:** My prior placed a ~60% chance on a new record. The evidence, particularly the time constraint, provides an extremely strong likelihood ratio in favor of the "no new record" hypothesis.
*   **Calculation Sketch:**
    *   Prior Odds = P(New Record) / P(No New Record) ≈ 60/40 = 1.5
    *   Likelihood Ratio for Time Constraint ≈ P(<2mo left | New Record) / P(<2mo left | No New Record) ≈ 0.01 / 1 = 0.01 (being generous)
    *   Posterior Odds ≈ 1.5 * 0.01 = 0.015
    *   Posterior P(New Record) ≈ 0.015 / (1 + 0.015) ≈ 1.5%
*   **Posterior Shape:** The update collapses the probability mass almost entirely onto a single value: the current record of 105 trillion digits. The bimodal prior becomes a unimodal posterior with a massive spike at 105 trillion and a tiny, long tail for the remote possibility of a surprise.

### 5. Red Team Thinking
*   **Alternative Scenario:** A well-funded, completely secret project using a next-generation supercomputer or a revolutionary new algorithm has been running and is about to finish.
*   **Plausibility:** This is the primary upside risk. However, the secrecy required is difficult, and a revolutionary algorithm would likely be major academic news on its own. The hardware would have to be significantly faster to reduce the multi-month computation time to something that could have been started later in 2025.
*   **Rebuttal:** Even in this scenario, the verification and reporting lag makes a resolution by Dec 31, 2025, challenging. The time constraint is a hard physical barrier that is difficult to argue against.
*   **Conclusion:** The probability of a new record is not zero, but it is very low, likely in the low single digits. My forecast should reflect this high degree of confidence. A distribution where even the 90th percentile is the current record is the most honest representation of this belief.

### 6. Final Percentiles
My posterior distribution is overwhelmingly concentrated at exactly 105,000,000,000,000. The probability of a new record being computed, verified, and reported in the less than two months remaining is exceedingly small. Therefore, the vast majority of the probability mass falls on the current record.

P10: 105,000,000,000,000
P20: 105,000,000,000,000
P40: 105,000,000,000,000
P60: 105,000,000,000,000
P80: 105,000,000,000,000
P90: 105,000,000,000,000

```

#### Grok

- ok=False | t_ms=75 | tokens: prompt=0, completion=0, total=0 | cost=$0.000000

- error=HTTP 429: Your team 0101076a-72f1-4f25-a038-b346d74bbc8d has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.

### Aggregation (BMC)

- final_quantiles: P10=-996.4721320429649, P50=20.950350797224736, P90=1052.0594019184223

- bmc_summary={"mean": 26.49969621421629, "p10": -996.4721320429649, "p50": 20.950350797224736, "p90": 1052.0594019184223}