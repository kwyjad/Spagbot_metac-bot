# What will be the best score by an AI on the full Humanity's Last Exam (HLE) before 2026? (QID: 38767)

- Type: numeric

- URL: https://www.metaculus.com/questions/38767/

- Classifier: technology | strategic=False (score=0.00)

### SeenGuard

- enabled=True | lock_status=acquired

- run_filter: before=1 | skipped=0 | after=1

- debug_note=lock acquired

## Research (summary)

CALIBRATION GUIDANCE (auto-generated weekly):
BINARY CALIBRATION
- No resolved binary questions (with pre-resolution forecasts) yet.
- Advice: keep using base rates + small evidence-weighted updates; avoid big swings.

MULTIPLE-CHOICE CALIBRATION (Top-1)
- No resolved MCQ questions (with pre-resolution forecasts) yet.

NUMERIC CALIBRATION (PIT-lite + CRPS)
- No resolved numeric questions (with pre-resolution forecasts) yet.
- When numeric results exist, we’ll check p10/p50/p90 coverage and CRPS.

General takeaway: apply **small** nudges only where gaps are consistent with decent sample sizes (≥10).
— end calibration —

### Reference class & base rates
-   **Reference Class 1: Major Leaps on Broad Academic Benchmarks (e.g., MMLU).** When a new generation of models is released (e.g., GPT-3 to GPT-4), performance on complex, multi-subject benchmarks can jump dramatically. GPT-4 achieved a score of 86.4% on MMLU, a ~30 percentage point increase over its predecessors. Applying this to HLE, a next-generation model could plausibly achieve a similar leap from the current 26.6% SOTA.
    -   *Base Rate Range:* A 25-40 percentage point increase following a frontier model release. This would imply a potential score of **51% to 66%**.
    -   *Limitations:* HLE is designed to be significantly harder than MMLU, testing frontier knowledge and multimodality, which may not scale as easily.
-   **Reference Class 2: Progress on Specialized Reasoning Benchmarks (e.g., MATH).** For benchmarks requiring deep, multi-step reasoning, progress is often slower and more incremental. Top models improved on the MATH benchmark from ~53% (GPT-4) to ~60% (Claude 3 Opus) over a year, a ~7 percentage point gain. This class suggests progress on HLE might be more constrained.
    -   *Base Rate Range:* A 5-10 percentage point increase per year from SOTA models within the same architectural generation. Given the <3 month horizon, this implies a much smaller gain.
    -   *Limitations:* This reference class may understate the potential for a discontinuous leap from a completely new model architecture, which is a real possibility in the current environment.

### Recent developments (timeline bullets)
-   [2025-09-18] Report highlights transformer capabilities doubling every seven months since 2023. (↑) — Suggests underlying scaling trends are strong, making a large performance jump plausible if a new model is released.
-   [2025-09-16] Article notes some experts predict AGI arrival as early as 2025, citing near-human performance. (↑) — Reflects extremely bullish sentiment and high investment in achieving breakthroughs, increasing the odds of a major release before year-end.
-   [2025-09-04] Consulting report predicts AGI as early as 2026, automating 60-80% of activities. (↑) — Reinforces the narrative of rapid, imminent progress from industry observers, which can be a leading indicator of private-sector capabilities.
-   [2025-08-XX] OpenAI's "Deep Research" team achieves the current 26.6% SOTA on HLE. (↑) — Establishes a baseline and indicates that specialized, post-hoc reasoning systems (not just base models) are already being deployed on this benchmark.

### Mechanisms & drivers (causal levers)
-   **Release of a Next-Generation Frontier Model (e.g., GPT-5, Gemini 3):** The single most important driver. A new, larger, and architecturally improved model could obsolete the current SOTA overnight. (Impact: Large)
-   **Advanced Post-Training Enhancements:** The current SOTA was set by a "Deep Research" team, implying methods beyond simple prompting (e.g., search, tool use, complex reasoning chains). A concerted effort by a top lab to "solve" HLE with such a system is a key mechanism. (Impact: Moderate-Large)
-   **Architectural Innovations in Reasoning/Multimodality:** Breakthroughs in areas like process supervision, tree-of-thought, or native multimodal understanding could unlock performance on HLE's specific challenges. (Impact: Moderate)
-   **Scaling of Compute and Data:** The continuous increase in training resources allows for larger models and more extensive training runs, which generally correlates with higher capability. (Impact: Moderate)
-   **Benchmark-Specific Optimization:** A dedicated team focusing on HLE could develop specialized fine-tuning datasets or prompting strategies, yielding significant gains on the existing SOTA model. (Impact: Small-Moderate)

### Differences vs. the base rate (what’s unusual now)
-   **Extreme Time Compression:** The forecast horizon is less than three months. Reference class jumps (like on MMLU) occurred over 1-2 year periods of model development and optimization. A major leap *before 2026* requires a new model to be released and benchmarked almost immediately.
-   **Designed Difficulty:** Unlike MMLU, which was created and then surpassed, HLE was explicitly designed to be a "final" exam at the frontier of knowledge. It may have a much steeper difficulty curve with fewer "easy" gains.
-   **Frontier Knowledge Requirement:** HLE tests cutting-edge scientific knowledge. This makes it resistant to models trained on older data corpuses and puts a premium on real-time synthesis or reasoning from first principles, a known weakness.
-   **Intense, Focused Competition:** The hype and capital in the AI space are at an all-time high. The competitive pressure to release a new SOTA model and dominate benchmarks is immense, potentially accelerating release timelines compared to historical precedent.

### Bayesian update sketch (for the statistician)
-   **Prior:** Based on the MMLU reference class, a prior centered around **55%** seems plausible, assuming a next-generation model is released. The high uncertainty and difficulty of HLE suggest this prior should be weak (equivalent n≈5).
-   **Evidence mapping:**
    -   (↑ Large) The cadence of major model releases (GPT-4 in early 2023) makes a successor release by end-of-2025 highly plausible. Industry sentiment is at a fever pitch.
    -   (↓ Large) The <3 month time horizon is extremely constraining. It leaves little time for a new model to be released, let alone for teams to fully optimize its performance on a novel, difficult benchmark.
    -   (↓ Moderate) The nature of HLE (frontier knowledge, designed difficulty) suggests gains will be harder-won than on previous benchmarks. The jump from 26.6% may not be as clean as MMLU's jump from a lower baseline.
    -   (↑ Small) The current 26.6% SOTA was achieved by a specialized system, showing that powerful post-hoc methods are allowed and effective. There is room to improve these systems even on current models.
-   **Net effect:** The strong upward pressure from an anticipated model release is significantly counteracted by the severe time constraint and the benchmark's inherent difficulty. The posterior should be revised downwards from the initial 55% prior, likely landing in the **40-50%** range.

### Indicators to watch (leading signals; next weeks/months)
-   **UP indicators:**
    -   Any credible leak or official announcement of a GPT-5, Gemini 3, or Claude 4 release before mid-December 2025.
    -   A new paper demonstrating a breakthrough in scientific reasoning or multimodal understanding that significantly outperforms SOTA on related benchmarks (e.g., MATH, ScienceQA).
    -   Major AI labs begin teasing "superhuman performance" on a range of cognitive tasks.
    -   The HLE leaderboard (agi.safe.ai) shows a new, intermediate score (e.g., 35%) from a major lab, indicating active work.
-   **DOWN indicators:**
    -   Official statements from OpenAI, Google, or Anthropic delaying their next model release until Q1 2026 or later.
    -   Key AI researchers publishing analyses suggesting fundamental roadblocks to further scaling or reasoning improvements.
    -   No significant AI model releases or announcements by the first week of December 2025.

### Caveats & pitfalls
-   **Release Timing is Everything:** The forecast is almost entirely dependent on the binary event of a next-generation model being released *and benchmarked* within the next ~10 weeks. This is a massive source of uncertainty.
-   **Definition of "Post-training enhancements":** This is a crucial ambiguity. A highly agentic system with web access and self-correction loops could achieve a very high score, while a more constrained system might not. The precedent of "Deep Research" suggests complex systems are allowed.
-   **S-Curve Dynamics:** Performance on benchmarks often follows an S-curve. It is unknown if 26.6% is at the bottom of a steep incline (poised

Market snapshot debug:
- Metaculus: no open binary results in response
- Manifold: no open binary results in response

### Sources
- Report: Artificial General Intelligence marks the next era of AI (vertexaisearch.cloud.google.com) — https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEDxAMvpTpNcakhCanr6RnmFqjgFWZw31LycWIKyHxX8awO6JXCO5VqOlibWCJhKBP9zYCTX-iP8fohxkkSbRGlDU69kH31ZLaYbD_uD8qZ8j0lNDAUZ_8vXAxH9fnFDkfpTL0UE8uLgFLpBkeV7XGK-wm7lW5FNFNd6zuQeE27ydCGr5IUOeuU4a6iUfYbcPr0nHS_TxOVV3Ss3FAVmDHOaHQj
- The Ultimate Guide to Recent AGI Research and Roadmaps 2023 to 2025 - Medium (vertexaisearch.cloud.google.com) — https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEo9Akp38vhYDG_ghlkiDucesRenQJ0G_-I6NP8VfLtfArgob_AgvuwVgHzaej_Q0VvC8LCF3YqubOjWshMb6QUD39gG8ggK8T8gQVLk1p5BqcqCNGExd_6MFCltC36UD2EwDCvGf3us68bcMNVLsXbJMEskrI9oGZKBGeqv4vGiNANRSxfU5M8lraJyvG57g-jXPGSVFliL-zNNE5ikNv-kS_KXcSZj1jW
- The AGI Revolution: Why 2025 Could Be The Year Everything Changes (vertexaisearch.cloud.google.com) — https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFWkVIkMGWKgKe5pFzr5YgiwRPaJvBt7pXXwXVLUTC4NJrPYYcPmf9vzOUf4M1p8sn9tsYOtDQ_hJRetu7AZBKCtsssaZqAbkawhWqna8KX4pyD2PNbPqkoAbDHUmi9Q49KIjSdiXhZ-qIhI43bv3hYWpu8f3s0XhCj9Clo952fFoPxb2CyoJG2CH7XV4IGkLXjrOASuHp2ZiLm2onJZN5t4ySq_VJ7lDD0MQ==

### Research (debug)

- source=GoogleGrounding | llm=google/gemini-2.5-pro | cached=0 | n_raw=5 | n_kept=3 | cost=$0.022393

- query: artificial general intelligence.; format; metac_closes_in_period; info; post_id; question_id; Last Exam; Deep Research; 2026; 2025

### GTMC1 (debug)

- strategic_class=False | strategic_score=0.00 | source=llm

- gtmc1_active=False | qtype=numeric | t_ms=0

- actors_parsed=0

- exceedance_ge_50=None | coalition_rate=None | median=None | dispersion=None

### Ensemble (model outputs)

- OpenRouter-Default: ok=True t_ms=24434 P10=2026.000, P50=1.000, P90=1.000

- Claude-3.7-Sonnet (OR): ok=True t_ms=22852 P10=2026.000, P50=1.000, P90=1.000

- Gemini: ok=True t_ms=52367 P10=2026.000, P50=1.000, P90=5.000

- Grok: ok=False t_ms=290



### Per-model (raw + usage/cost)

#### OpenRouter-Default

- ok=True | t_ms=24434 | tokens: prompt=3743, completion=592, total=4335 | cost=$0.015277

```md

To produce a probabilistic forecast for the best score by an AI on the full Humanity's Last Exam (HLE) before 2026, we will follow the outlined steps:

1. **Base Rate (Prior) Selection**
   - **Reference Class 1: Major Leaps on Broad Academic Benchmarks (e.g., MMLU).** Historical data shows that new model generations can lead to significant performance jumps on complex benchmarks. The base rate range for such a leap is a 25-40 percentage point increase, suggesting a potential score of 51% to 66%.
   - **Reference Class 2: Progress on Specialized Reasoning Benchmarks (e.g., MATH).** Progress is slower and more incremental, with a 5-10 percentage point increase per year. Given the short time frame, this suggests a smaller gain.
   - **Chosen Base Rate:** Given the potential for a new model release, the base rate is centered around 55%, with a wide variance due to the uncertainty of a new model release and the benchmark's difficulty.

2. **Comparison to Base Case**
   - **Similarities:** The rapid pace of AI development and the potential for a new model release align with historical patterns of significant performance jumps.
   - **Differences:** The short time frame (<3 months) and the designed difficulty of HLE make immediate gains less likely compared to past benchmarks.

3. **Evidence Evaluation (Likelihoods)**
   - **Positive Evidence:**
     - The cadence of major model releases and industry sentiment suggest a new model release is plausible.
     - The current SOTA was achieved by a specialized system, indicating room for improvement.
   - **Negative Evidence:**
     - The short time frame is a major constraint.
     - HLE's designed difficulty and frontier knowledge requirements make gains harder to achieve.

4. **Bayesian Updating (Posterior Distribution)**
   - **Prior:** Centered around 55% with high uncertainty.
   - **Likelihoods:** The strong upward pressure from a potential model release is counteracted by the time constraint and benchmark difficulty.
   - **Posterior:** Revised downwards to a range of 40-50%, reflecting the balance of evidence.

5. **Red Team Thinking**
   - **Overconfidence Check:** The forecast accounts for the possibility of no new model release and the inherent difficulty of HLE.
   - **Tail Risks:** Consideration of a major breakthrough or delay in model release.
   - **Alternative Forecast:** A more conservative forecast might center around 35%, but given the evidence, 40-50% is more plausible.

6. **Final Percentiles**
   - Based on the analysis, the percentiles are adjusted to reflect the posterior distribution.

P10: 35  
P20: 40  
P40: 45  
P60: 50  
P80: 55  
P90: 60

```

#### Claude-3.7-Sonnet (OR)

- ok=True | t_ms=22852 | tokens: prompt=4316, completion=1089, total=5405 | cost=$0.029283

```md

# Forecast: Best Score on Humanity's Last Exam (HLE) Before 2026

## 1. Base Rate (Prior) Selection

I'll use two reference classes to establish a prior:

**Reference Class 1: Performance jumps on academic benchmarks**
- MMLU saw a jump from ~50% to 86.4% when GPT-4 was released
- This suggests potential jumps of 25-40 percentage points are possible with new model generations
- Current SOTA on HLE is 26.6% from OpenAI's Deep Research

**Reference Class 2: Progress on specialized reasoning benchmarks**
- On difficult reasoning benchmarks like MATH, progress is slower (~7 percentage points/year)
- HLE is explicitly designed to be more difficult than previous benchmarks

Considering both reference classes, I'll center my prior at **45%**, representing a significant but not extreme jump from the current 26.6%. This assumes a reasonable probability of a new frontier model release before 2026, but acknowledges HLE's designed difficulty.

## 2. Comparison to Base Case

**Similarities to reference classes:**
- Like MMLU, HLE tests broad academic knowledge across many domains
- Like specialized reasoning benchmarks, HLE requires deep reasoning and frontier knowledge

**Key differences:**
- **Extremely short timeframe**: Less than 3 months remain until 2026, severely constraining the window for a new model release
- **Designed difficulty**: HLE was explicitly created to be a "final" benchmark at the frontier of human knowledge
- **Multimodal requirements**: HLE includes image-based questions, adding complexity
- **Intense competition**: The AI race has accelerated, potentially compressing development timelines

## 3. Evidence Evaluation

**Evidence for higher scores:**
- The current SOTA (26.6%) was achieved by a specialized "Deep Research" system, suggesting post-hoc reasoning systems are allowed and effective (moderate upward push)
- Industry sentiment and investment are at all-time highs, increasing the likelihood of a major model release (moderate upward push)
- Transformer capabilities have been doubling approximately every 7 months, suggesting underlying scaling trends remain strong (small upward push)

**Evidence for lower scores:**
- The extremely short timeframe (~10 weeks) severely limits the probability of a new frontier model being released AND fully optimized for HLE (strong downward push)
- HLE's design as a "final exam" at the frontier of knowledge makes it inherently more resistant to rapid progress (moderate downward push)
- Frontier knowledge requirements may limit performance of models trained on older data (small downward push)

## 4. Bayesian Updating

Starting with my prior of 45%, I'll update based on the evidence:

**Update 1: Time constraint**
- The extremely short timeframe (~10 weeks) is the most significant factor
- This reduces the probability of a major new model release being fully optimized for HLE
- Likelihood ratio: ~0.7 (significantly favors lower scores)
- Updated estimate: ~38%

**Update 2: Benchmark difficulty**
- HLE's designed difficulty as a "final exam" makes rapid progress harder
- Likelihood ratio: ~0.9 (moderately favors lower scores)
- Updated estimate: ~35%

**Update 3: Competitive pressure**
- The intense competition in AI may accelerate release timelines
- Likelihood ratio: ~1.1 (slightly favors higher scores)
- Updated estimate: ~37%

My posterior distribution is now centered around 37%, with significant uncertainty.

## 5. Red Team Thinking

**Potential blind spots:**
- I may be underestimating the probability of a major model release in the next 10 weeks
- I might be overestimating HLE's difficulty relative to previous benchmarks
- The definition of "post-training enhancements" is ambiguous - highly agentic systems with web access could achieve much higher scores

**Alternative scenarios:**
- **Upside scenario**: A new frontier model (e.g., GPT-5) is released in November and achieves 55%+ on HLE
- **Downside scenario**: No new frontier models are released before 2026, limiting improvement to incremental gains on existing architectures (~30-32%)

**Adjustments needed:**
- I should widen my distribution to account for the binary nature of a major model release
- The upper tail should extend higher to account for the possibility of a breakthrough model + aggressive optimization

## 6. Final Percentiles

After considering all factors, I'll provide my calibrated forecast:

P10: 28.5
P20: 31.0
P40: 35.0
P60: 40.0
P80: 48.0
P90: 55.0

```

#### Gemini

- ok=True | t_ms=52367 | tokens: prompt=4006, completion=1387, total=9943 | cost=$0.018878

```md

Here is a probabilistic forecast for the best score by an AI on the full Humanity's Last Exam (HLE) before 2026.

### 1. Base Rate (Prior) Selection

The primary uncertainty is whether a next-generation frontier AI model (e.g., GPT-5, Gemini 3) will be released and benchmarked before 2026. This suggests a mixture model for the prior, combining two reference classes:

1.  **Major Benchmark Leaps (e.g., MMLU):** When GPT-4 was released, it achieved a ~30 percentage point jump on MMLU over its predecessors. This class applies if a new model is released. Starting from the current 26.6% SOTA, this suggests a potential score in the **50-65%** range.
2.  **Incremental Progress on Hard Benchmarks (e.g., MATH):** On difficult reasoning tasks, progress between major releases is slower, often 5-10 percentage points per year. This class applies if no new model is released, suggesting a score in the **30-40%** range, driven by improved post-hoc reasoning systems on current models.

Initially, I'll assign a 60% probability to the "new model" scenario given the intense competitive pressure, leading to a prior centered around **49%** but with a very wide, bimodal distribution.

### 2. Comparison to Base Case

*   **Similarities:** The AI development environment is characterized by intense competition and rapid scaling, similar to the period preceding the GPT-4 release. This supports the possibility of a large leap.
*   **Differences:**
    *   **Extreme Time Constraint:** The forecast horizon is less than three months (from mid-October 2025 to year-end). This is an exceptionally short window for a new model to be released, let alone fully optimized on a new benchmark.
    *   **Designed Difficulty:** HLE was explicitly designed to be a "final exam" testing frontier knowledge. This suggests that gains may be harder to achieve than on previous benchmarks like MMLU, and the performance S-curve may be less steep.

### 3. Evidence Evaluation (Likelihoods)

*   **(↓ Large) Short Time Horizon:** The <3 month window is the most significant piece of evidence. It substantially lowers the probability that a new model will be released *and* benchmarked in time. This is the strongest factor pushing the forecast down.
*   **(↓ Moderate) HLE's Inherent Difficulty:** The benchmark's focus on frontier, multimodal knowledge suggests that even a next-generation model might see a smaller performance jump than the one observed on MMLU. A 20-25 point gain may be more realistic than a 30-40 point one.
*   **(↑ Moderate) Intense Competition:** The timeline of bullish reports and the general market environment create immense pressure on labs like OpenAI, Google, and Anthropic to release new models. This could accelerate timelines, making a release before 2026 more likely than historical cadences would suggest.
*   **(↑ Small) SOTA from "Deep Research":** The current 26.6% score was achieved by a specialized system, not a base model. This confirms that complex reasoning methods are effective and allowed. It also implies that other labs could apply similar or better systems to their own SOTA models (e.g., Claude 3, Gemini 1.5) to achieve significant gains even without a brand-new architecture.

### 4. Bayesian Updating (Posterior Distribution)

My conceptual update process is as follows:

*   **Prior:** A bimodal distribution centered around 49%, with a 60% chance of a "new model" world (median ~57%) and a 40% chance of an "incremental" world (median ~37%).
*   **Update with Evidence:**
    1.  The **short time horizon** is powerful evidence against the "new model" scenario occurring in time. I'll flip the probabilities: the chance of a new model being released *and benchmarked* in time is now closer to 40%.
    2.  The **difficulty of HLE** reduces the expected gain *if* a new model is released. I'll lower the median of the "new model" world from ~57% to a more conservative ~47% (a ~20 point jump).
    3.  The "incremental" world's median of ~37% (a ~10 point jump) remains plausible.
*   **Posterior Calculation:** The updated posterior is a mixture of these two scenarios: (40% * 47%) + (60% * 37%) = 18.8% + 22.2% = **41%**. My central estimate shifts significantly downward from the prior, and the distribution remains wide and right-skewed.

### 5. Red Team Thinking

*   **Overconfidence in Slowdown:** Am I underestimating the pace of AI development? A new model could be announced any day. The bullish sentiment from the research report is strong. This suggests I should have more mass in the upper tail.
*   **Overconfidence in Progress:** Am I underestimating the difficulty of HLE? Perhaps it's fundamentally resistant to current architectures, and even a GPT-5 would only score a few points higher. This suggests the jump in the "new model" scenario might be much smaller, pulling the median down.
*   **Blind Spot: Data Contamination:** As HLE is new, there's a non-zero tail risk that its contents could leak and contaminate the training data of a new model, leading to an artificially high score (e.g., 80%+). This adds to the right skew.

Considering these points, I will adjust my median slightly upward to ~43% to account for the possibility of a surprise accelerated release, and I will ensure the P80-P90 range is wide enough to capture the high-end possibilities of a major breakthrough or data issues. The lower end of the distribution is well-anchored by the likelihood of at least some incremental progress.

### 6. Final Percentiles

P10: 34
P20: 37
P40: 40
P60: 46
P80: 56
P90: 64

```

#### Grok

- ok=False | t_ms=290 | tokens: prompt=0, completion=0, total=0 | cost=$0.000000

- error=HTTP 429: Your team 0101076a-72f1-4f25-a038-b346d74bbc8d has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.

### Aggregation (BMC)

- final_quantiles: P10=-1017.4710900447335, P50=0.9614413586829434, P90=1032.0523996805098

- bmc_summary={"mean": 6.167289591792484, "p10": -1017.4710900447335, "p50": 0.9614413586829434, "p90": 1032.0523996805098}