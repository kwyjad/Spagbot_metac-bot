# How large will the largest cluster used to train an AI model be, by the end of the following years? (2025) (QID: 38884)

- Type: numeric

- URL: https://www.metaculus.com/questions/38884/

- Classifier: technology | strategic=False (score=0.00)

### SeenGuard

- enabled=True | lock_status=acquired

- run_filter: before=1 | skipped=0 | after=1

- debug_note=lock acquired

## Research (summary)

CALIBRATION GUIDANCE (auto-generated weekly):
BINARY CALIBRATION
- No resolved binary questions (with pre-resolution forecasts) yet.
- Advice: keep using base rates + small evidence-weighted updates; avoid big swings.

MULTIPLE-CHOICE CALIBRATION (Top-1)
- No resolved MCQ questions (with pre-resolution forecasts) yet.

NUMERIC CALIBRATION (PIT-lite + CRPS)
- No resolved numeric questions (with pre-resolution forecasts) yet.
- When numeric results exist, we’ll check p10/p50/p90 coverage and CRPS.

General takeaway: apply **small** nudges only where gaps are consistent with decent sample sizes (≥10).
— end calibration —

### Reference class & base rates
-   **Reference Class 1: Top-tier AI training clusters.** This is the most relevant class. The largest clusters have grown exponentially.
    -   ~2020 (GPT-3): ~10,000 Nvidia A100s.
    -   ~2023 (GPT-4 / Google A3): ~25,000-26,000 A100/H100 GPUs.
    -   **Base Rate:** This implies a doubling time of roughly 2-3 years. Extrapolating from a ~25k baseline in early 2023 to the end of 2025 (a ~2.75-year gap) suggests a range of **50,000 to 100,000 GPUs**.
    -   *Limitation:* Exponential trends cannot continue indefinitely as they hit physical or economic limits. This forecast horizon is short enough that the trend may hold, but we are entering a scale where new constraints (power, cooling) become dominant.

-   **Reference Class 2: Frontier-scale general supercomputers.** The world's top supercomputers provide a loose upper bound on what is technically feasible to build and power.
    -   Frontier (2022): ~38,000 AMD GPUs.
    -   Aurora (2023): ~64,000 Intel Max GPUs.
    -   **Base Rate:** This class suggests that clusters in the **40,000-70,000** accelerator range are the current state-of-the-art for massive, publicly-funded scientific computing.
    -   *Limitation:* These systems are not always optimized for single, large-scale AI training runs, and the interconnect architecture may differ significantly from bespoke AI clusters built by hyperscalers.

### Recent developments (timeline bullets)
-   [2025-10-06] AMD and OpenAI announce a strategic partnership for 6 gigawatts of AMD GPUs. **(Neutral/↓ for 2025)** — While a massive long-term signal, the first 1GW deployment is scheduled for H2 2026, meaning it will not contribute to the 2025 total and may divert planning resources.
-   [2025-06-15] (Hypothetical but plausible) Reports emerge of major cloud providers (e.g., Microsoft, AWS) completing the first large-scale deployments of Nvidia's Blackwell GB200 NVL72 systems. **(↑)** — The Blackwell architecture is designed for exascale AI, and its availability in rack-scale systems (72 GPUs per rack) is a key enabler for building >50k GPU clusters efficiently.
-   [2025-02-28] AMD announces RDNA 4-based Radeon RX 9000 series GPUs. **(Neutral)** — These are consumer/gaming GPUs. They are irrelevant to the data center-scale Instinct line used in training clusters.
-   [2024-03-18] Nvidia announces the Blackwell B200 GPU and GB200 Superchip, claiming significant performance gains and scalability for LLM training. **(↑)** — This announcement set the technical foundation for the next generation of massive clusters, with systems like the DGX SuperPOD with GB200 designed to scale to "tens of thousands" of chips.

### Mechanisms & drivers (causal levers)
-   **Competitive Arms Race (Large ↑):** Intense competition between OpenAI/Microsoft, Google, Meta, and Anthropic/Amazon to train next-generation foundation models (e.g., GPT-5, Gemini-Next) is the primary driver for scaling cluster size.
-   **Hardware & Interconnect Innovation (Large ↑):** New architectures like Nvidia's Blackwell (GB200) are designed as large, tightly-coupled systems (e.g., NVL72 racks), making it technically more feasible to build and efficiently utilize clusters of 50k-100k+ GPUs.
-   **Power & Infrastructure Bottlenecks (Large ↓):** A 100,000 H100-class GPU cluster requires 70-100 megawatts of power, the equivalent of a small city. Securing this power and building the corresponding data centers is a multi-year process, acting as a powerful brake on growth.
-   **Capital Expenditure (Moderate ↓):** These clusters represent investments of $5-20 billion. While funding has been abundant, a significant economic downturn or a shift in investor sentiment towards profitability could slow the pace of these mega-projects.
-   **Software & Algorithmic Scalability (Moderate ↓):** There are diminishing returns when scaling a single training run across an ever-larger number of GPUs due to communication overhead. Overcoming these challenges is a significant software engineering feat.

### Differences vs. the base rate (what’s unusual now)
-   **Shift from GPU Count to Power as the Limiting Factor:** Unlike in 2020-2023 where the main constraint was acquiring enough GPUs, the primary bottleneck for the largest new projects is securing gigawatt-scale power and the physical infrastructure to support it.
-   **Emergence of a Viable Competitor:** The base rate period was characterized by near-total Nvidia dominance. The AMD/OpenAI deal, though post-2025, signals the start of a more competitive market, which could increase the total long-term supply of high-end accelerators.
-   **National Strategic Importance:** Building leading-edge AI is now a matter of national security and economic competitiveness, leading to sovereign AI initiatives and potentially larger, state-backed investments than seen in the purely corporate-led base rate period.
-   **System-Level Architecture:** The unit of scale is shifting from the individual GPU to the rack-scale system (e.g., Nvidia NVL72). This pre-integration simplifies the construction of massive clusters but also locks builders

Market snapshot debug:
- Metaculus: no open binary results in response
- Manifold: no open binary results in response

### Sources
- AMD Radeon RX 8000 series: All of the rumors so far (www.xda-developers.com) — https://www.xda-developers.com/amd-radeon-rx-8000-series-rumors/
- Leak Points to 4 SKUs for AMD Radeon RX 8000 laptop GPUs (www.extremetech.com) — https://www.extremetech.com/gaming/leak-points-to-4-skus-for-amd-radeon-rx-8000-laptop-gpus
- AMD RDNA 4 release date and announcement leak alleges "at least" one next-gen RX 8000 series GPU launch for January (www.notebookcheck.net) — https://www.notebookcheck.net/AMD-RDNA-4-release-date-and-announcement-leak-alleges-at-least-one-next-gen-RX-8000-series-GPU-launch-for-January.897007.0.html
- AMD and OpenAI announce strategic partnership to deploy 6 gigawatts of AMD GPUs (www.amd.com) — https://www.amd.com/en/newsroom/press-releases/2025-10-6-amd-and-openai-announce-strategic-partnership.html
- AMD RDNA 4 and Radeon RX 9000-series GPUs start at $549: Specifications, release date, pricing, and more revealed (www.tomshardware.com) — https://www.tomshardware.com/pc-components/gpus/amd-rdna-4-and-radeon-rx-9000-series-gpus-start-at-dollar549-specifications-release-date-pricing-and-more-revealed
- AMD Reports Fourth Quarter and Full Year 2023 Financial Results (ir.amd.com) — https://ir.amd.com/news-events/press-releases/detail/1181/amd-reports-fourth-quarter-and-full-year-2023-financial
- AMD confirms Radeon RX 9070 XT and RDNA 4 launch date (www.tweaktown.com) — https://www.tweaktown.com/news/101031/amd-confirms-radeon-rx-9070-xt-and-rdna-4-launch-date/index.html
- AMD Announces FidelityFX Super Resolution 3 (FSR 3) Fluid Motion Rivaling DLSS 3, Broad Hardware Support (www.techpowerup.com) — https://www.techpowerup.com/312821/amd-announces-fidelityfx-super-resolution-3-fsr-3-fluid-motion-rivaling-dlss-3-broad-hardware-support

### Research (debug)

- source=GoogleGrounding | llm=google/gemini-2.5-pro | cached=0 | n_raw=9 | n_kept=8 | cost=$0.015745

- query: format; metac_closes_in_period; info; post_id; question_id; AMD GPUs; 2025; 2023

### GTMC1 (debug)

- strategic_class=False | strategic_score=0.00 | source=llm

- gtmc1_active=False | qtype=numeric | t_ms=0

- actors_parsed=0

- exceedance_ge_50=None | coalition_rate=None | median=None | dispersion=None

### Ensemble (model outputs)

- OpenRouter-Default: ok=True t_ms=9444 P10=2025.000, P50=1.000, P90=1.000

- Claude-3.7-Sonnet (OR): ok=True t_ms=22426 P10=2025.000, P50=1.000, P90=2020.000

- Gemini: ok=True t_ms=55364 P10=1.000, P50=2020.000, P90=3.000

- Grok: ok=False t_ms=302



### Per-model (raw + usage/cost)

#### OpenRouter-Default

- ok=True | t_ms=9444 | tokens: prompt=2897, completion=641, total=3538 | cost=$0.013652

```md

To produce a probabilistic forecast for the largest cluster used to train an AI model by the end of 2025, we will follow the structured Bayesian reasoning process outlined.

### 1. Base Rate (Prior) Selection
- **Reference Class 1: Top-tier AI training clusters.** The largest clusters have grown exponentially, with a doubling time of roughly 2-3 years. Starting from a baseline of ~25,000 GPUs in early 2023, we extrapolate to a range of **50,000 to 100,000 GPUs** by the end of 2025.
- **Reference Class 2: Frontier-scale general supercomputers.** These suggest a range of **40,000-70,000** GPUs as the current state-of-the-art for massive computing systems.

### 2. Comparison to Base Case
- **Similarity:** The trend of exponential growth in AI training clusters is expected to continue in the short term, supported by competitive pressures and technological advancements.
- **Difference:** New constraints such as power and infrastructure bottlenecks are becoming more significant, potentially limiting growth.

### 3. Evidence Evaluation (Likelihoods)
- **Nvidia Blackwell Architecture (↑):** The introduction of Nvidia's Blackwell architecture, designed for exascale AI, supports the feasibility of building clusters with >50k GPUs.
- **Power & Infrastructure Bottlenecks (↓):** The requirement for significant power and infrastructure acts as a constraint, making extremely large clusters less likely.
- **Competitive Arms Race (↑):** Intense competition among major AI players drives the push for larger clusters.
- **Capital Expenditure & Economic Conditions (↓):** Economic downturns or shifts in investor sentiment could slow the pace of mega-projects.

### 4. Bayesian Updating (Posterior Distribution)
- **Prior:** Assume a normal distribution centered around 75,000 GPUs with a standard deviation of 15,000, based on the base rate extrapolation.
- **Likelihood Adjustment:** The evidence of new architectures and competitive pressures increases the likelihood of larger clusters, shifting the mean upward slightly. However, power and infrastructure constraints temper this shift.
- **Posterior:** Adjust the mean to 80,000 GPUs with a slightly increased standard deviation of 20,000 to account for increased uncertainty due to constraints.

### 5. Red Team Thinking
- **Overconfidence Check:** Consider the possibility of unforeseen technological breakthroughs or setbacks.
- **Tail Risks:** A significant economic downturn or geopolitical event could drastically alter the landscape.
- **Alternative Forecast:** A more conservative estimate might center around 60,000 GPUs, considering the constraints.
- **Rebuttal:** The competitive drive and technological advancements make larger clusters plausible.

### 6. Final Percentiles
- **P10:** 55,000
- **P20:** 65,000
- **P40:** 75,000
- **P60:** 85,000
- **P80:** 95,000
- **P90:** 105,000

These percentiles reflect a balanced view, considering both the drivers for growth and the constraints that could limit it.

```

#### Claude-3.7-Sonnet (OR)

- ok=True | t_ms=22426 | tokens: prompt=3412, completion=1081, total=4493 | cost=$0.026451

```md

# Forecasting the Largest AI Training Cluster Size by End of 2025

## 1. Base Rate (Prior) Selection

For my prior distribution, I'll use the historical growth pattern of top-tier AI training clusters:
- ~2020 (GPT-3): ~10,000 Nvidia A100s
- ~2023 (GPT-4/Google A3): ~25,000-26,000 GPUs

This shows a doubling time of approximately 2-3 years. Since we're forecasting from early 2023 to end of 2025 (about 2.75 years), a simple extrapolation suggests a range of 50,000-100,000 GPUs.

I'll also consider the frontier of general supercomputers as an upper bound:
- Frontier (2022): ~38,000 AMD GPUs
- Aurora (2023): ~64,000 Intel Max GPUs

Based on these reference classes, I'll use a prior distribution with:
- Median (P50): 60,000 GPUs
- Spread: Roughly log-normal with substantial variance to reflect uncertainty

## 2. Comparison to Base Case

**Similarities to base case:**
- The competitive dynamics between major AI labs remain intense
- Hardware capabilities continue to advance with new architectures (Blackwell)
- Economic incentives for building larger models persist

**Key differences:**
- Power and infrastructure have become the primary bottlenecks rather than GPU availability
- The unit of scale is shifting from individual GPUs to rack-scale systems
- National security interests are increasingly driving investment in AI infrastructure
- We're reaching scales where diminishing returns on parallelization become more pronounced

## 3. Evidence Evaluation

**Evidence pushing estimates higher:**
1. Nvidia's Blackwell architecture announcement (March 2024) enables more efficient scaling to "tens of thousands" of chips (likelihood ratio ~1.5x for higher values)
2. The intense competitive arms race between OpenAI/Microsoft, Google, Meta, and Anthropic/Amazon creates strong incentives for larger clusters (likelihood ratio ~2x for higher values)
3. National strategic importance of AI may lead to larger state-backed investments (likelihood ratio ~1.2x for higher values)

**Evidence pushing estimates lower:**
1. Power and infrastructure bottlenecks - 100,000 H100-class GPUs require 70-100 megawatts (likelihood ratio ~2x for lower values)
2. The AMD/OpenAI partnership announcement indicates first 1GW deployment only in H2 2026, suggesting longer timelines for massive deployments (likelihood ratio ~1.3x for lower values)
3. Diminishing returns on parallelization create software engineering challenges (likelihood ratio ~1.2x for lower values)

## 4. Bayesian Updating

Starting with my prior centered at 60,000 GPUs, I'll update based on the most significant evidence:

First update - Power constraints (strongest downward factor):
- Prior: P50 = 60,000 GPUs
- Evidence: Power infrastructure limitations (2x likelihood for lower values)
- Posterior: Shift downward to P50 ≈ 50,000 GPUs

Second update - Competitive dynamics (strongest upward factor):
- Prior: P50 = 50,000 GPUs
- Evidence: Intense competition (2x likelihood for higher values)
- Posterior: Shift upward to P50 ≈ 55,000 GPUs

The remaining factors roughly balance each other, with a slight downward pressure due to the practical challenges of building and operating such massive systems.

## 5. Red Team Thinking

**Potential blind spots:**
- I may be underestimating the pace of innovation in power-efficient architectures
- I might be overlooking potential breakthroughs in distributed training algorithms
- The forecast doesn't fully account for geopolitical factors that could accelerate or decelerate development

**Alternative scenarios:**
1. **Higher scenario (90,000+ GPUs)**: A major breakthrough in distributed training efficiency combined with national security imperatives leads to accelerated deployment of mega-clusters.
2. **Lower scenario (30,000-40,000 GPUs)**: Economic downturn or shift in AI strategy toward more efficient, smaller models reduces incentives for massive clusters.

**Adjustments for uncertainty:**
- Widening my distribution to account for these scenarios
- Slightly lowering my median estimate to reflect the practical challenges that emerge at these scales

## 6. Final Percentiles

P10: 32,000
P20: 38,000
P40: 48,000
P60: 58,000
P80: 75,000
P90: 95,000

```

#### Gemini

- ok=True | t_ms=55364 | tokens: prompt=3313, completion=1588, total=9301 | cost=$0.020021

```md

### 1. Base Rate (Prior) Selection
The primary reference class is the growth of top-tier AI training clusters.
-   **~2020 (GPT-3):** ~10,000 Nvidia V100-class GPUs.
-   **~2023 (GPT-4):** ~25,000 Nvidia A100-class GPUs.
This represents a 2.5x increase in approximately 3 years, which corresponds to a doubling time of roughly 2.5 years. The question asks for the largest cluster by the end of 2025, which is about 2.75 years from the early 2023 baseline of ~25,000 GPUs.

A simple exponential extrapolation provides a starting point:
25,000 * (2^(2.75 / 2.5)) = 25,000 * (2^1.1) ≈ 53,500 GPUs.

A secondary reference class is the largest general-purpose supercomputers.
-   **Aurora (2023):** ~64,000 Intel Max GPUs.
This demonstrates that clusters in the 60k+ range are technically feasible as of 2023, providing a strong anchor for what is possible.

Combining these, my prior distribution is centered around **55,000 - 65,000 GPUs**, with significant uncertainty. I'll model this as a log-normal distribution, reflecting the nature of exponential growth.

### 2. Comparison to Base Case
The current situation shares the primary driver of the base rate period: an intense competitive "arms race" among major AI labs (OpenAI/Microsoft, Google, Meta, etc.) and now nations (sovereign AI). However, there are key differences:

-   **Similar:** The fundamental motivation to build larger and more capable models by scaling compute remains the dominant force.
-   **Different (Slowing Factor):** As cluster sizes approach 100k GPUs, they require power on the scale of a small city (70-100+ MW). Securing this power and building the necessary data center infrastructure is a multi-year bottleneck that is more severe now than it was for the 10k -> 25k jump. This acts as a powerful brake on the exponential trend.
-   **Different (Accelerating Factor):** New system architectures, like Nvidia's Blackwell GB200 NVL72, are designed as rack-scale building blocks (72 GPUs per rack). This simplifies the engineering and deployment of massive clusters, making it logistically easier to scale to 50k-100k GPUs once the physical infrastructure is in place.

### 3. Evidence Evaluation (Likelihoods)
-   **Competitive Arms Race & Sovereign AI (Strong Upward Pressure):** The immense commercial and geopolitical value placed on leading-edge AI models makes it highly likely that multiple actors are willing to spend billions ($5-20B) on next-generation clusters. This strongly supports a significant increase over the 25k baseline.
-   **Power & Infrastructure Bottlenecks (Strong Downward Pressure / Cap):** This is the most significant constraint. It makes extremely large numbers (>150k) very unlikely by EOY 2025, as such projects would have needed to secure land and power years in advance. This tempers the upper tail of the distribution.
-   **Nvidia Blackwell Architecture (Strong Upward Pressure / Enabler):** The announcement in March 2024 and expected availability in 2025 of systems designed for exascale AI makes a 50k-120k cluster a concrete engineering goal, not just a theoretical one. This makes higher outcomes much more plausible than they would be with older H100 technology.
-   **Aurora Supercomputer at 64k GPUs (Anchor / Upward Pressure):** The existence of a 64k GPU scientific computing cluster in 2023 provides a strong proof-of-concept for this scale. It makes it less likely that the largest *AI-specific* cluster, which is the focus of the most intense commercial competition, would be smaller than this. This raises the floor of my forecast.

### 4. Bayesian Updating (Posterior Distribution)
-   **Prior:** A distribution centered around 55k-65k, based on trend extrapolation.
-   **Update 1 (Constraints):** The power and infrastructure bottleneck is a powerful piece of evidence that pulls the distribution down and trims the extreme upper tail. A pure exponential trend is unlikely to hold. This might shift the median from ~60k down towards ~55k.
-   **Update 2 (Enablers):** The combination of the competitive arms race, the existence of the 64k-GPU Aurora, and the new Blackwell architecture strongly counteracts the constraints. The Aurora system in particular makes a median below 60k feel too conservative. The intense competition suggests at least one player will push to beat that benchmark. This evidence shifts the median back up, likely into the **65k-75k range**.
-   **Posterior Shape:** The final distribution is right-skewed. Delays in construction or commissioning are common (capping the upside), but there is also a non-trivial chance of a surprise announcement of a very large, state-backed or hyperscaler project that was kept secret, creating a fat upper tail. The median should be above the 64k Aurora benchmark.

### 5. Red Team Thinking
-   **Alternative Scenario (Stagnation):** Could algorithmic efficiency gains (e.g., Q*, better MoE) reduce the demand for massive dense clusters? Or could the economic reality of the immense cost and power requirements cause all players to pause and consolidate? In this world, the largest cluster might only be a modest increment to ~45,000 GPUs. This suggests my P10 and P20 should not be too high.
-   **Alternative Scenario (Moonshot):** Am I underestimating the "whatever it takes" attitude of Microsoft, Google, or a sovereign wealth fund? A project may have been secretly in the works since 2022, timed perfectly for Blackwell, to build a >120,000 GPU cluster. The NVL72 architecture makes deployment rapid once the facility is ready. This suggests the P90 should be well over 100,000.
-   **My central estimate seems robust.** It sits just above the last known major data point (Aurora), which is a common pattern in competitive technological races. My final distribution will account for the stagnation and moonshot scenarios in the tails.

### 6. Final Percentiles
My final distribution is centered around a median of ~68,000 GPUs. It acknowledges the powerful constraints by keeping the median from reaching the ~100k level, but it has a long right tail to account for the intense competitive pressure and new technological enablers.

P10: 42000
P20: 48000
P40: 57000
P60: 74000
P80: 94000
P90: 115000

```

#### Grok

- ok=False | t_ms=302 | tokens: prompt=0, completion=0, total=0 | cost=$0.000000

- error=HTTP 429: Your team 0101076a-72f1-4f25-a038-b346d74bbc8d has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.

### Aggregation (BMC)

- final_quantiles: P10=-803.1896054503585, P50=513.0837126976794, P90=2420.129880173401

- bmc_summary={"mean": 678.5059534496028, "p10": -803.1896054503585, "p50": 513.0837126976794, "p90": 2420.129880173401}