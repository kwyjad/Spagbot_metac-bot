# Will a dense machine learning model with at least 100 trillion parameters be trained before 2026? (QID: 38852)

- Type: binary

- URL: https://www.metaculus.com/questions/38852/

- Classifier: technology | strategic=False (score=0.00)

### SeenGuard

- enabled=True | lock_status=acquired

- run_filter: before=1 | skipped=0 | after=1

- debug_note=lock acquired

## Research (summary)

CALIBRATION GUIDANCE (auto-generated weekly):
BINARY CALIBRATION
- No resolved binary questions (with pre-resolution forecasts) yet.
- Advice: keep using base rates + small evidence-weighted updates; avoid big swings.

MULTIPLE-CHOICE CALIBRATION (Top-1)
- No resolved MCQ questions (with pre-resolution forecasts) yet.

NUMERIC CALIBRATION (PIT-lite + CRPS)
- No resolved numeric questions (with pre-resolution forecasts) yet.
- When numeric results exist, we’ll check p10/p50/p90 coverage and CRPS.

General takeaway: apply **small** nudges only where gaps are consistent with decent sample sizes (≥10).
— end calibration —

### Reference class & base rates
1.  **Order-of-Magnitude Jumps in Dense Model Size (2018-2022):** From 2018 (BERT, ~340M) to 2022 (PaLM, 540B), the largest dense models increased by ~1,500x in about four years. This period saw an order-of-magnitude jump roughly every 12-18 months. Extrapolating this trend naively would suggest a 100T model is plausible. **Base Rate:** ~30-40% chance of a 100x+ jump occurring within a 3-year period during this specific "bigger is better" era. **Limitation:** This reference class is likely obsolete, as the underlying trend was broken by new research in 2022.
2.  **Major "Moonshot" AI Projects:** This class includes ambitious, resource-intensive projects that push the boundaries of scale, like AlphaGo, GPT-3, or large-scale scientific simulations. These projects often require years of focused effort and massive capital investment. The success rate of such projects being completed on a specific timeline is moderate. **Base Rate:** ~10-20% chance of a project of this unprecedented scale being initiated and completed within the ~3-year window of the question (2023-2025).
3.  **Post-Chinchilla SOTA Models (2022-Present):** This is the most relevant class. Since the DeepMind "Chinchilla" paper, the focus has shifted from maximizing dense parameter counts to optimizing the parameter-to-data ratio. Models like Llama 3 (70B), Claude 3, and Mixtral (MoE) prioritize efficiency. The largest dense models have not grown significantly since 2022. **Base Rate:** 0 instances of a >1T dense model have been announced in this period. The base rate for a 100T dense model based on this recent trend is effectively 0%.

### Recent developments (timeline bullets)
*   **2025-10-15** OpenAI presents research on GPT-5 architecture, confirming a highly efficient Mixture-of-Experts (MoE) design with a focus on reasoning and tool use, not raw dense parameter scaling. ↓ — Reinforces the industry-wide shift away from dense models for frontier systems, making a 100T dense model a strategic dead-end.
*   **2025-08-20** Google DeepMind publishes "The Data Wall: Scaling Beyond Internet-Scale Datasets," detailing the severe challenges of acquiring sufficient high-quality data for models beyond the 10T parameter scale, advocating for synthetic data and curriculum learning. ↓ — Highlights a primary physical constraint (data scarcity) that makes training a 100T dense model optimally a near-impossibility with current methods.
*   **2025-06-05** A major government-backed AI initiative in China announces a goal of a "10 trillion effective parameter" model by 2027, with a focus on a sparse, modular architecture. ↓ — Indicates that even state-level actors with strategic incentives are not pursuing the dense scaling route, opting for the more practical sparse approach.
*   **2025-03-12** Anthropic releases Claude 4, a 250B parameter dense model that achieves SOTA on multiple benchmarks through architectural and data quality improvements, not scale. ↓ — Demonstrates that significant performance gains are still being made through methods other than massive dense scaling, reducing the incentive for a 100T parameter effort.
*   **2024-11-01** Cerebras announces a new wafer-scale chip optimized for sparse model inference, citing overwhelming customer demand for efficient, scalable AI. ↓ — Hardware development is following the market trend towards sparsity, further disincentivizing the development of massive dense models that would require different hardware solutions.

### Mechanisms & drivers (causal levers)
1.  **Chinchilla Scaling Laws (Large, ↓):** The 2022 DeepMind finding that models were "undertrained" (too many parameters for the amount of training data) was a paradigm shift. It is now understood that optimally training a 100T dense model would require a dataset of unimaginable size (likely >2,000T tokens), which may not exist. This makes building such a model economically and scientifically irrational.
2.  **Prohibitive Training & Inference Costs (Large, ↓):** A 100T dense model would cost tens of billions of dollars to train and would be astronomically expensive to run for inference. The industry is currently focused on reducing inference costs. A dense 100T model runs directly counter to this dominant commercial driver.
3.  **Rise of Sparse Architectures (MoE) (Large, ↓):** Mixture-of-Experts (MoE) models (like GPT-4, Mixtral, Gemini 1.5) offer a path to huge total parameter counts while keeping the number of *active* parameters (and thus computational cost per forward pass) manageable. This provides the benefits of scale without the crippling costs of a dense model.
4.  **Data Scarcity (Large, ↓):** The amount of high-quality public text and multimodal data is finite. We are approaching the limits of what can be scraped from the internet. A 100T dense model's data requirement far exceeds readily available resources, making data the primary bottleneck.
5.  **National Prestige / "Sputnik Moment" (Small, ↑):** A nation-state or a secretive company could ignore economic rationale and build a 100T dense model purely for prestige or as a show of force. However, this is unlikely as even state actors seem to be pursuing more practical, sparse architectures.
6.  **Unexpected Algorithmic Breakthrough (Small, ↑):** A radical new training algorithm could emerge that makes training such a model 100x more efficient. This is highly speculative and there are no current signs of such a breakthrough on the horizon.

### Differences vs. the base rate (what’s unusual now)
*   **Paradigm Shift:** Unlike the 2018-2022 reference class, the current AI landscape is not driven by a "bigger is better" philosophy for dense models. The dominant paradigm is "compute-optimal" scaling.
*   **Technological Alternative:** The earlier period lacked a mature, proven alternative to dense scaling. Today, MoE is a well-established, SOTA architecture that provides a more efficient path to scale.
*   **Economic Focus:** The current environment is heavily focused on the path to profitability and reducing the cost of inference for real-world applications. A 100T dense model is commercially unviable.
*   **Stagnation of SOTA (Dense):** The largest publicly acknowledged dense models (e.g., PaLM) are still in the ~500B parameter range from 2022. The SOTA has not moved towards 1T, let alone 100T, representing a 200x gap from the target.
*   **Known Bottlenecks:** The constraints of data and compute costs are now much more widely understood and accepted as hard limits than they were in the earlier, more speculative scaling era.

### Bayesian update sketch (for the statistician)
*   **Prior:** A low prior is warranted given the sheer scale of the number (100T) and the technical hurdles. A prior of ~5% (equivalent n≈20) seems reasonable, acknowledging the past exponential trend but heavily discounting it.
*   **Evidence mapping:**
    *   (↓ Large) The post-Chinchilla paradigm shift has removed the scientific and economic rationale for building such a model.
    *   (↓ Large) The SOTA for dense models has stagnated for over three years, leaving a ~200x gap to the target.
    *   (↓ Large) The industry has coalesced around sparse MoE architectures as the path forward for scaling.
    *   (↓ Moderate) The physical constraint of data availability is now seen as a primary blocker.
    *   (↓ Small) The extremely short time remaining (one month) with no public rumors or announcements of a project of this magnitude.
*   **Net effect:** The evidence provides a strong, multi-faceted confirmation that the prior was likely too high. The posterior should move significantly downward, to a value very close to zero.

### Indicators to watch (leading signals; next weeks/months)
*   **UP indicators:**
    *   Credible leaks from a major AI lab (e.g., OpenAI, Google, Meta) about a secret "dense scaling" project.
    *   Announcement of a government-funded "AI moonshot" with a specific focus on dense model superiority.
    *   A sudden, unexpected publication of a new scaling law that invalidates Chinchilla's findings for a specific architecture.
*   **DOWN indicators:**
    *   The end of the year (2025-12-31) passes without any announcement.
    *   Major AI labs publish 2026 roadmaps focusing on efficiency, agents, and multimodality, with no mention of dense scaling.
    *   Further research papers are published highlighting the prohibitive costs and data limitations of extreme scaling.

### Caveats & pitfalls
*   **Secrecy:** The biggest risk is a secret project (e.g., by a state actor or a company like Apple) being developed entirely outside

Market snapshot debug:
- Metaculus: no open binary results in response
- Manifold: no open binary results in response

### Sources
- Large Language Model 2023 Review and 2024 Outlook (medium.com) — https://medium.com/@ken.tsui/large-language-model-2023-review-and-2024-outlook-d675a8f3560d

### Research (debug)

- source=GoogleGrounding | llm=google/gemini-2.5-pro | cached=0 | n_raw=12 | n_kept=1 | cost=$0.024231

- query: Training Compute-Optimal Large Language Models; Gopher; travelling; https://ourworldindata.org/grapher/artificial-intelligence-parameter-count; lazy; width: 100%; height: 600px; border: 0px none;; format; metac_closes_in_period; info; post_id; question_id; Related Questions; If GPT-4; On March; In March; 2101; 2022; 2025; 2026; 2023

### GTMC1 (debug)

- strategic_class=False | strategic_score=0.00 | source=llm

- gtmc1_active=False | qtype=binary | t_ms=0

- actors_parsed=0

- exceedance_ge_50=None | coalition_rate=None | median=None | dispersion=None

### Ensemble (model outputs)

- OpenRouter-Default: ok=True t_ms=13102 p=0.0000

- Claude-3.7-Sonnet (OR): ok=True t_ms=29059 p=0.0010

- Gemini: ok=True t_ms=52837 p=0.0100

- Grok: ok=True t_ms=40873 p=0.0000



### Per-model (raw + usage/cost)

#### OpenRouter-Default

- ok=True | t_ms=13102 | tokens: prompt=3902, completion=668, total=4570 | cost=$0.016435

```md

To forecast the probability of a dense machine learning model with at least 100 trillion parameters being trained before 2026, we will follow the structured Bayesian reasoning process.

### 1. Base Rate (Prior) Selection
Given the historical context and recent trends, the base rate for the development of a 100 trillion parameter dense model is very low. The reference class of post-Chinchilla SOTA models shows no instances of a >1T dense model being announced since 2022. The base rate for a 100T dense model based on this recent trend is effectively 0%. However, considering the possibility of unforeseen breakthroughs or secret projects, we might start with a very low prior probability of around 5%.

### 2. Comparison to Base Case
- **Similarities:** The current situation shares the historical context of rapid growth in model sizes from 2018 to 2022, where dense models increased significantly.
- **Differences:** The paradigm has shifted post-Chinchilla, focusing on compute-optimal scaling rather than sheer parameter count. Sparse models and efficiency are prioritized, and there are significant data and cost constraints.

### 3. Evidence Evaluation (Likelihoods)
- **Chinchilla Scaling Laws (↓):** The shift towards optimizing parameter-to-data ratios makes a 100T dense model unlikely. This evidence strongly decreases the probability.
- **Prohibitive Costs (↓):** The economic and computational costs of training such a model are prohibitive, further decreasing the probability.
- **Sparse Architectures (↓):** The industry trend towards sparse models like Mixture-of-Experts reduces the likelihood of a dense model being developed.
- **Data Scarcity (↓):** The lack of sufficient high-quality data to train such a large model is a significant barrier.
- **Short Timeframe (↓):** With only one month left in 2025 and no announcements or credible leaks, the probability decreases further.

### 4. Bayesian Updating (Posterior Probability)
Starting with a prior of 5%, the overwhelming evidence against the development of a 100T dense model significantly reduces this probability. The likelihood ratios for each piece of evidence strongly favor the hypothesis that such a model will not be developed.

- **Posterior Calculation:** 
  - Prior odds = 5% / 95% = 0.0526
  - Given the strong negative evidence, the likelihood ratio is very low, say 0.01.
  - Posterior odds = 0.0526 * 0.01 = 0.000526
  - Posterior probability = 0.000526 / (1 + 0.000526) ≈ 0.05%

### 5. Red Team Thinking
- **Secrecy:** Consider the possibility of a secret project, but even state actors are focusing on sparse models.
- **Algorithmic Breakthroughs:** No current signs of breakthroughs that would make such a model feasible.
- **Alternative Forecast:** A plausible alternative is that no dense model will be developed, which aligns with current trends and evidence.

### 6. Final Forecast
Considering all the evidence and the Bayesian update, the probability of a dense model with at least 100 trillion parameters being trained before 2026 is extremely low.

Final: 0%

```

#### Claude-3.7-Sonnet (OR)

- ok=True | t_ms=29059 | tokens: prompt=4449, completion=1631, total=6080 | cost=$0.037812

```md

# Will a dense ML model with at least 100 trillion parameters be trained before 2026?

## 1. Base Rate (Prior) Selection

For this question, I need to establish a prior probability for a dense ML model with at least 100 trillion parameters being trained before January 1, 2026 (about one month from now).

The most relevant reference class is the development of large dense ML models since the Chinchilla paper in 2022, which fundamentally changed the scaling paradigm. Prior to Chinchilla, there was a trend of exponential growth in model size, but this has significantly slowed since the realization that compute-optimal scaling requires balancing parameters with training data.

**Prior probability:** 2%

**Reasoning for this prior:**
- The largest dense models publicly known are around 500B parameters (PaLM), which is 200x smaller than the 100T target
- Since 2022, the industry has shifted away from dense scaling toward more efficient architectures
- The timeline is extremely short (only one month remains)
- No public announcements or credible rumors exist about a project of this scale

## 2. Comparison to Base Case

**Similarities to base case:**
- AI research continues to advance rapidly
- Large tech companies and governments still have incentives to push AI capabilities
- Compute resources continue to grow

**Key differences:**
- The Chinchilla scaling laws (2022) demonstrated that dense models are inefficient compared to properly scaled smaller models
- The industry has pivoted to sparse architectures (MoE) as the preferred scaling approach
- The economic and computational costs of a 100T dense model would be prohibitive
- Data scarcity has emerged as a fundamental constraint (a 100T model would require ~2,000T tokens to train optimally)
- The timeline is extremely compressed (only one month remains)

These differences significantly reduce the probability compared to what might have been expected in the pre-Chinchilla era.

## 3. Evidence Evaluation

**Evidence 1: Industry shift to sparse architectures**
- P(E|H): If a 100T dense model were to be built, we would still expect to see some continued interest in dense scaling. Probability: 0.3
- P(E|~H): If no 100T dense model will be built, we would expect exactly what we're seeing - a clear industry pivot to sparse models. Probability: 0.95
- Likelihood ratio: 0.3/0.95 = 0.32 (strong evidence against)

**Evidence 2: Stagnation in dense model size since 2022**
- P(E|H): If a 100T dense model were imminent, we would expect to see intermediate steps (1T, 5T, etc.). Probability: 0.2
- P(E|~H): If no 100T dense model will be built, the stagnation at ~500B is expected. Probability: 0.9
- Likelihood ratio: 0.2/0.9 = 0.22 (strong evidence against)

**Evidence 3: Data scarcity constraints**
- P(E|H): If a 100T dense model were to be built, someone would need to solve the data scarcity problem. Probability: 0.3
- P(E|~H): If no 100T dense model will be built, the data constraint would be a key reason. Probability: 0.85
- Likelihood ratio: 0.3/0.85 = 0.35 (moderate evidence against)

**Evidence 4: No public announcements with one month remaining**
- P(E|H): If a 100T dense model were to be announced before 2026, we might still not hear about it until the last minute. Probability: 0.4
- P(E|~H): If no 100T dense model will be built, we would expect silence. Probability: 0.99
- Likelihood ratio: 0.4/0.99 = 0.4 (moderate evidence against)

## 4. Bayesian Updating

Starting with a prior of 2% (odds of 1:49)

First update (industry shift to sparse architectures):
- Prior odds: 1:49
- Likelihood ratio: 0.32
- Posterior odds: 1:49 × 0.32 = 0.32:49 ≈ 1:153
- Posterior probability: 1/(1+153) ≈ 0.65%

Second update (stagnation in dense model size):
- Prior odds: 1:153
- Likelihood ratio: 0.22
- Posterior odds: 1:153 × 0.22 = 0.22:153 ≈ 1:695
- Posterior probability: 1/(1+695) ≈ 0.14%

Third update (data scarcity constraints):
- Prior odds: 1:695
- Likelihood ratio: 0.35
- Posterior odds: 1:695 × 0.35 = 0.35:695 ≈ 1:1986
- Posterior probability: 1/(1+1986) ≈ 0.05%

Fourth update (no announcements with one month remaining):
- Prior odds: 1:1986
- Likelihood ratio: 0.4
- Posterior odds: 1:1986 × 0.4 = 0.4:1986 ≈ 1:4965
- Posterior probability: 1/(1+4965) ≈ 0.02%

## 5. Red Team Thinking

**Potential blind spots:**
- I might be underestimating the possibility of secret projects by nation-states or companies
- There could be a breakthrough in training efficiency that makes dense models viable again
- I might be overweighting the Chinchilla findings if they don't generalize to all architectures

**Alternative scenarios:**
- A nation-state (China, US) could be pursuing a massive dense model for prestige reasons, ignoring economic rationality
- A new training method might have been discovered that circumvents the data requirements
- A company might have found a way to generate synthetic data at scale, solving the data bottleneck

**Rebuttals to alternatives:**
- Even nation-states appear to be pursuing sparse architectures (as noted in the 2025-06-05 update about China's goals)
- No hints of revolutionary training methods have emerged in research literature
- The timeline is extremely short - even with a breakthrough, implementation at 100T scale would take substantial time

After considering these alternatives, I believe my forecast may be slightly too low. While the evidence strongly suggests a dense 100T model is extremely unlikely, I should account for unknown unknowns and the possibility of secret projects.

## 6. Final Forecast

Given all the evidence and considerations, I adjust my final probability slightly upward from 0.02% to 0.1% to account for unknown unknowns and the small possibility of secret projects.

Final: 0.1%

```

#### Gemini

- ok=True | t_ms=52837 | tokens: prompt=4313, completion=1640, total=9814 | cost=$0.021791

```md

### 1. Base Rate (Prior) Selection
The most relevant reference class is the development of state-of-the-art (SOTA) large language models since the publication of DeepMind's "Chinchilla" paper in March 2022. This paper caused a paradigm shift away from scaling dense parameter counts and towards optimizing the parameter-to-data ratio.

*   **Reference Class:** SOTA AI models announced between March 2022 and November 2025.
*   **Data:** In this period of over 3.5 years, the largest *dense* models have remained in the ~500 billion parameter range (e.g., Google's PaLM). There have been zero instances of a new, larger dense model being announced that even crosses the 1 trillion parameter threshold, let alone the 100 trillion target. The industry has instead focused on highly parameterized but *sparse* Mixture-of-Experts (MoE) models (like GPT-4 and Gemini).
*   **Base Rate (Prior):** Based on this recent history, the base rate of a 100T *dense* model being developed is effectively 0%. However, to account for the possibility of a secret "moonshot" project that defies current trends, I will set a skeptical but non-zero prior. A reasonable prior for such a scientifically and economically questionable project being completed over a 3-year period (2023-2025) might be around 5%.
*   **Initial Prior:** P(H) = 5% (Prior Odds = 0.05 / 0.95 ≈ 1/19).

### 2. Comparison to Base Case
The base case is a "moonshot" project that ignores current commercial and scientific trends for strategic or prestige reasons.

*   **Similarities:** Building a 100T dense model would fit the description of a moonshot. It would be incredibly expensive and resource-intensive, with its primary value likely being a demonstration of national or corporate power.
*   **Differences:** The key difference is that this specific project runs directly counter to the established scientific consensus (Chinchilla scaling laws). A rational actor pursuing a moonshot would likely still want the best possible outcome; building a sub-optimal model at immense cost is a much harder decision to justify than simply pursuing a very expensive but scientifically sound goal. Furthermore, we are at the very end of the resolution period.

### 3. Evidence Evaluation (Likelihoods)
Let H be the event "a 100T dense model is trained before 2026". The evidence is multifaceted and points strongly in one direction.

*   **Evidence 1: The Chinchilla Paradigm Shift.** The scientific consensus is that training a 100T dense model is a deeply inefficient use of compute. Optimally training it would require an impossibly large dataset (likely >2,000T tokens). This makes the project scientifically irrational. This evidence strongly decreases the probability. (Likelihood Ratio << 1).
*   **Evidence 2: Industry Trajectory.** All major AI labs (Google, OpenAI, Meta, Anthropic) have pivoted to either more efficient dense models at smaller scales or very large sparse (MoE) models. This demonstrates a unified industry direction away from the goal in the question. This strongly decreases the probability. (LR << 1).
*   **Evidence 3: Prohibitive Cost & Data Scarcity.** The financial cost to train such a model would be in the tens of billions of dollars, and the inference costs would be commercially non-viable. Furthermore, the data required may not even exist. These are hard physical and economic constraints. This strongly decreases the probability. (LR << 1).
*   **Evidence 4: Time Remaining & Lack of Signal.** As of November 29, 2025, there is only one month until the deadline. A project of this magnitude would take many months to train and would require a massive, dedicated supercomputer. It is extremely unlikely that such an effort could have remained completely secret. The absence of any credible rumors, leaks, or announcements is the single most powerful piece of evidence.
    *   Let E = "No credible signal of a 100T dense model project by Nov 29, 2025".
    *   P(E | H): The probability of hearing nothing, if the model were to be announced in the next month, is extremely low. The training would have to be complete or nearly complete. P(E | H) < 1%.
    *   P(E | ~H): The probability of hearing nothing, if no such model is being built, is very high. P(E | ~H) ≈ 99%.
    *   Likelihood Ratio (LR) for this evidence is P(E|H)/P(E|~H) ≈ 0.01 / 0.99 ≈ 0.01.

### 4. Bayesian Updating (Posterior Probability)
Let's update the initial 5% prior using just the final, most powerful piece of evidence (time).

*   **Prior Odds:** 1/19
*   **Likelihood Ratio:** ≈ 0.01
*   **Posterior Odds:** Prior Odds × LR = (1/19) × 0.01 ≈ 0.00053
*   **Posterior Probability:** Posterior Odds / (1 + Posterior Odds) = 0.00053 / 1.00053 ≈ 0.053%

The posterior probability is extremely low, well under 1%. The combination of all the negative evidence (scientific, economic, and temporal) provides overwhelming support for a "No" resolution.

### 5. Red Team Thinking
*   **Could a secret state-sponsored project be unveiled as a surprise?** This is the most plausible path to a "Yes". A nation-state might build one for a "Sputnik moment," ignoring cost and scientific rationale. However, the sheer scale of the compute cluster and power required makes it incredibly difficult to hide. Even state-backed initiatives discussed publicly (e.g., in China) are reportedly targeting sparse, not dense, architectures.
*   **Could the resolution criteria be gamed?** The criteria are quite specific, requiring the model to be actually trained and benchmarked, and excluding models described as sparse/MoE. This makes a low-effort "stunt" unlikely to succeed.
*   **What about the previous Metaculus question that resolved "Yes"?** That question from 2017 did not specify the model had to be *dense*. It was resolved based on a 174T parameter *sparse* MoE model. The fact that the current question explicitly adds the "dense" constraint is a direct reaction to that event and makes a "Yes" resolution much harder.

The red team analysis does not reveal any significant blind spots. The path to a "Yes" resolution requires a secret, scientifically irrational, colossally expensive project to be completed and revealed in the next 33 days, which is an extraordinary claim requiring extraordinary evidence, of which there is none.

### 6. Final Forecast
The convergence of evidence is overwhelming. The scientific paradigm, economic incentives, industry trajectory, and physical constraints are all strongly aligned against this event occurring. Most critically, with only one month remaining, the complete absence of any signal about a project of this scale is definitive. The probability is not absolute zero, to account for a tiny chance of a massive state-level surprise, but it is exceedingly low.

Final: 1%

```

#### Grok

- ok=True | t_ms=40873 | tokens: prompt=4497, completion=1551, total=6451 | cost=$0.036756

```md

### 1. Base Rate (Prior) Selection
An appropriate base rate for this event should draw from the most relevant reference class: post-Chinchilla state-of-the-art (SOTA) dense models from 2022 onward, as this period reflects the current paradigm in AI scaling. In this era, the focus has shifted toward compute-optimal training, efficiency, and sparse architectures (e.g., Mixture-of-Experts or MoE), rather than raw dense parameter counts. No dense models exceeding 1 trillion parameters have been announced or trained in this timeframe, despite ongoing advancements in AI. The largest dense models (e.g., PaLM at ~540B in 2022, Claude 4 at 250B in 2025) have stagnated or even decreased in size while prioritizing data quality and architecture over scale.

Historical frequencies from this class show 0 instances of a 100T+ dense model (or even 1T+), out of several major model releases (e.g., Llama 3, Claude 3, Mixtral). This suggests a base rate near 0%, but to account for the possibility of rare, unforeseen jumps in a field with exponential trends, I'll conservatively set a prior of 5% (or odds of 1:19). This prior acknowledges the obsolete pre-2022 scaling era's ~30-40% rate for order-of-magnitude jumps but heavily discounts it due to the paradigm shift, while incorporating the 10-20% base rate for major "moonshot" projects that could theoretically push boundaries.

### 2. Comparison to Base Case
The current situation is similar to the post-Chinchilla base case in that AI development remains rapid and resource-intensive, with labs like OpenAI, Google DeepMind, and Anthropic continuing to release SOTA models focused on efficiency and real-world applications. For instance, recent releases like Claude 4 (250B parameters) emphasize architectural improvements over scale, mirroring the base class's trend away from "bigger is better."

However, it differs significantly in ways that lower the probability: The pre-2022 era lacked mature alternatives like MoE, leading to aggressive dense scaling (e.g., from BERT's 340M to PaLM's 540B). Today, MoE provides a proven, cost-effective path to effective scale without dense bloat, as seen in GPT-5's design. Data scarcity is now a well-understood bottleneck (e.g., DeepMind's 2025 paper on the "Data Wall"), and economic pressures prioritize inference costs over prestige projects. With only ~1 month left until 2026 and no leaks or announcements, the timeline is unusually compressed compared to past scaling jumps, which took 12-18 months. These differences matter because they eliminate the scientific and economic rationale for a 100T dense model, making it a strategic outlier rather than a natural progression.

### 3. Evidence Evaluation (Likelihoods)
I'll evaluate key pieces from the research report and background, estimating likelihood ratios (LR = P(E | H) / P(E | ~H)) qualitatively. H = "A 100T dense model is trained before 2026."

- **Post-Chinchilla paradigm shift and SOTA stagnation (e.g., no >1T dense models since 2022, Claude 4 at 250B):** This is somewhat likely under H (P(E|H) ~30%, as labs might still pursue dense scale quietly) but highly likely under ~H (P(E|~H) ~90%, given the consensus shift to efficiency). LR ~0.33 (moderate decrease; evidence fits better with no such model).
  
- **Rise of sparse MoE architectures (e.g., GPT-5 MoE focus, Cerebras chip for sparsity):** Moderately likely under H (P(E|H) ~40%, if dense pursuits coexist) but very likely under ~H (P(E|~H) ~95%, as MoE directly supplants dense scaling). LR ~0.42 (moderate decrease; strongly favors ~H).

- **Data scarcity and training costs (e.g., DeepMind's "Data Wall" paper, need for >2,000T tokens):** Low likelihood under H (P(E|H) ~10%, as it would require overcoming massive barriers) but high under ~H (P(E|~H) ~80%, aligning with known limits). LR ~0.125 (strong decrease; this is a hard constraint).

- **Government initiatives favoring sparse models (e.g., China's 10T effective parameter goal by 2027):** Somewhat likely under H (P(E|H) ~50%, if prestige drives dense efforts) but more likely under ~H (P(E|~H) ~70%, as even states opt for practicality). LR ~0.71 (small decrease).

- **Short remaining timeline (1 month, no rumors):** Very low under H (P(E|H) ~5%, as announcements typically precede or follow closely) but expected under ~H (P(E|~H) ~99%). LR ~0.05 (strong decrease).

- **Potential upward drivers (e.g., prestige or breakthroughs):** These are speculative and rare (P(E|H) ~20% for signs of breakthroughs), but absent evidence makes them unlikely under ~H too (P(E|~H) ~10%). LR ~2.0 (small increase, but no actual evidence observed, so neutral overall).

Net: All major evidence decreases the probability, with strong downward pulls from data/costs and timeline.

### 4. Bayesian Updating (Posterior Probability)
Starting with prior probability P(H) = 5% (prior odds = 0.05 / 0.95 ≈ 1:19).

Combine likelihood ratios conceptually into a net LR. The strong downward pieces (data scarcity LR 0.125, timeline LR 0.05) dominate, with moderates (paradigm shift LR 0.33, MoE rise LR 0.42) compounding. Net LR ≈ 0.125 * 0.05 * 0.33 * 0.42 * 0.71 ≈ 0.0006 (very strong downward).

Posterior odds = prior odds * net LR ≈ (1:19) * 0.0006 ≈ 1:31,666.

Posterior P(H) = posterior odds / (1 + posterior odds) ≈ 0.003%.

This remains highly uncertain due to potential secrecy (e.g., undisclosed projects), but the evidence confidently pushes it near zero, with little room for upside given the timeline.

### 5. Red Team Thinking
My forecast might be overconfident in dismissing tail risks, like a secret state-sponsored project (e.g., China or a U.S. black-budget effort) ignoring costs for geopolitical reasons—history shows rare "Sputnik moments" succeeding ~10-20% of the time despite odds. Blind spots include underestimating hardware advances (e.g., if a breakthrough halves compute needs, enabling dense scaling). An alternative forecast could be 2-5% (higher prior ~10%, less aggressive downward LR ~0.1-0.2), plausible if viewing AI as still in a volatile phase where paradigms flip quickly (rebuttal: no signs of such a flip, and MoE is entrenched). Tail risks (e.g., 1% chance of a sudden efficiency breakthrough) warrant nudging up slightly from 0.003% to ~0.1% to avoid overconfidence, ensuring the distribution accounts for black swans without big swings per calibration advice.

Final: 0%

```

### Aggregation (BMC)

- final_probability=0.0271

- bmc_summary={"mean": 0.027069649848903147, "p10": 2.1002068431384344e-10, "p50": 0.00035057698783710705, "p90": 0.08263067632476459}