# What will be the largest number of digits of π to have ever been computed by December 31, 2025? (QID: 38944)

- Type: numeric

- URL: https://www.metaculus.com/questions/38944/

- Classifier: science | strategic=False (score=0.00)

### SeenGuard

- enabled=True | lock_status=acquired

- run_filter: before=1 | skipped=0 | after=1

- debug_note=lock acquired

## Research (summary)

CALIBRATION GUIDANCE (auto-generated weekly):
BINARY CALIBRATION
- No resolved binary questions (with pre-resolution forecasts) yet.
- Advice: keep using base rates + small evidence-weighted updates; avoid big swings.

MULTIPLE-CHOICE CALIBRATION (Top-1)
- No resolved MCQ questions (with pre-resolution forecasts) yet.

NUMERIC CALIBRATION (PIT-lite + CRPS)
- No resolved numeric questions (with pre-resolution forecasts) yet.
- When numeric results exist, we’ll check p10/p50/p90 coverage and CRPS.

General takeaway: apply **small** nudges only where gaps are consistent with decent sample sizes (≥10).
— end calibration —

### Research Plan

**1. Core Questions to Investigate:**

*   **Q1: What is the current, verified world record for the number of digits of π?** This establishes the baseline value.
*   **Q2: What is the historical trend of this record?** Specifically, what is the frequency of new records and the typical multiplicative increase in the number of digits? This will inform the base rate.
*   **Q3: Who are the primary actors setting these records?** Understanding their motivations (e.g., academic, corporate marketing, individual hobbyist) helps predict future behavior.
*   **Q4: What are the technological and logistical requirements for setting a new record?** This includes computation time, storage needs, and verification processes. This is critical for assessing the feasibility of a new record within the given timeframe.
*   **Q5: Are there any public announcements or credible rumors of an ongoing attempt that could conclude before December 31, 2025?** This would be the most direct evidence for a change.

**2. Search Queries:**

*   **Q1:** `"pi calculation world record" 2025`, `"Guinness World Record" "most accurate value of Pi"`, `"Linus Media Group" pi record 300 trillion verification`
*   **Q2:** `"Chronology of computation of pi" Wikipedia`, `log plot pi digits record over time`, `growth rate pi calculation record`
*   **Q3:** `who calculates pi records`, `Emma Haruka Iwao Google Cloud pi`, `Timothy Mullican pi`, `StorageReview pi record`
*   **Q4:** `how long to calculate pi`, `y-cruncher benchmark`, `storage requirements pi calculation`, `Chudnovsky algorithm hardware`
*   **Q5:** `"new pi record attempt" 2025`, `y-cruncher forum "record attempt"`, `twitter "pi calculation" "world record"`

**3. Synthesis of Findings:**

*   The provided source from TechRadar dated May 21, 2025, is the most critical piece of information. It reports a new record of 300 trillion digits set by Linus Media Group (LMG) and Kioxia. This is a credible source in the tech community, and LMG's project would likely be well-documented on their popular YouTube channel, lending it further credibility. This 300T value is the current record to beat.
*   The historical trend shows records are broken every 2-3 years, with the number of digits increasing by a factor of roughly 2-4x. The jump from Google's 100T in 2022 to LMG's 300T in 2025 fits this pattern perfectly (3x increase in 3 years).
*   The primary actors have shifted from academics to large, well-resourced organizations (Google) or collaborations (LMG/Kioxia) using the record as a high-profile technical benchmark and marketing tool.
*   The key constraint is time. The 100T record took 157 days (~5 months). The 300T record took ~7.5 months. Any new attempt to surpass 300T would almost certainly take over 8 months, even with the latest hardware. With less than two months to the resolution date (Nov 6 to Dec 31), a new record cannot be started and completed in time. Any record that could resolve by the deadline must have already been completed or be in its final stages.
*   No public information from other major players (e.g., Google, AWS, Microsoft) suggests a competing project is nearing completion. The community around the `y-cruncher` software, the standard for these calculations, has not indicated an imminent new record.

**4. Draft Comprehensive Answer:**

---

### Reference class & base rates
*   **Reference Class 1: Post-2010 Pi Calculation Records.** Since 2010, a new world record for Pi digits has been set approximately every 2-3 years. The magnitude of the increase is typically a factor of 2-4x over the previous record (e.g., 22.4T in 2016 -> 31.4T in 2019 -> 100T in 2022 -> 300T in 2025). The base rate suggests a record set in mid-2025 is unlikely to be broken before late 2027.
*   **Reference Class 2: Major Corporate Technology Benchmarks.** Similar to TOP500 supercomputer rankings or MLPerf benchmarks, these records are set by large organizations to showcase their infrastructure capabilities. These are typically planned, resource-intensive projects announced on a 1-3 year cycle for marketing purposes, not continuous efforts. The LMG/Kioxia effort fits this class.

### Recent developments (timeline bullets)
*   [2025-05-21] A credible tech media outlet reports that Linus Media Group (LMG), in partnership with storage company Kioxia, set a new world record of 300 trillion digits. (→) — This event sets the current, high-water mark at 300 trillion and resets the clock for the next record attempt. The reported 7.5-month computation time underscores the difficulty of surpassing it quickly.

### Mechanisms & drivers (causal levers)
*   **Computation Time (Large):** The primary limiting factor. Records now take 5-8+ months of dedicated computation on high-end, specialized hardware. This makes it nearly impossible for a new record to be achieved in the <2 months remaining.
*   **Storage Technology (Large):** Calculations require immense, fast storage (hundreds of terabytes to petabytes). The 300T record was a showcase for Kioxia's SSDs. Progress here enables larger calculations, but improvements are incremental, not revolutionary, on a short timescale.
*   **Corporate/Promotional Motivation (Moderate):** Actors like Google (promoting Cloud) and LMG/Kioxia (promoting content and hardware) are key. A new record requires a sponsor with a significant budget and a marketing incentive. Another such campaign is unlikely to materialize and conclude so soon after the last one.
*   **Algorithmic Stability (Small):** The Chudnovsky algorithm has been the standard for decades. While software like `y-cruncher` is continually optimized, there has been no fundamental algorithmic breakthrough that would dramatically shorten computation times. Progress is primarily hardware-driven.

### Differences vs. the base rate (what’s unusual now)
*   **Extremely Short Time Horizon:** The most significant difference. The base rate operates on a multi-year cycle. The forecast horizon is less than two months, a period far shorter than the required computation time for a new record.
*   **A Very Recent Record:** A new record was just set in May 2025. The "low-hanging fruit" of beating the 2022 record has been picked. Any new attempt must surpass a much higher, more difficult target.
*   **Limited Number of Actors:** The immense resource requirements (hardware, electricity, expertise) limit the pool of potential record-setters to a handful of global tech companies or extremely well-funded groups. This is not a field where a surprise result from an unknown individual is likely.

### Bayesian update sketch (for the statistician)
*   **Prior:** A prior based on the historical log-linear trend would suggest a new record in the 200-400 trillion digit range was plausible for the 2024-2026 period.
*   **Evidence mapping:**
    *   (↑ Large) The May 2025 announcement of a 300T record confirms the trend and provides a concrete, high baseline.
    *   (↓ Large) The <2 month time-to-resolution is far shorter than the 7.5+ month duration of the last record-breaking computation, making a new, larger calculation practically impossible to complete and verify in time.
    *   (↓ Small) The absence of public announcements or credible rumors from other major players (Google, AWS, etc.) about a competing project nearing completion.
*   **Net effect:** The evidence should pull the posterior almost entirely

Market snapshot debug:
- Metaculus: no open binary results in response
- Manifold: no open binary results in response

### Sources
- A Google employee just shattered the record for Pi calculations. Her name is Emma Haruka Iwao (www.washingtonpost.com) — https://www.washingtonpost.com/business/2019/03/14/google-employee-just-shattered-record-pi-calculations-her-name-is-emma-haruka-iwao/
- How Google Cloud Enabled a Record Calculation of Pi (thenewstack.io) — https://thenewstack.io/how-google-cloud-enabled-a-record-calculation-of-pi/
- Google Celebrates Pi Day As Employee Calculates New World Record (time.com) — https://time.com/5552344/pi-day-record-google-emma-haruka-iwao/
- Google Cloud team claims to have calculated 100trn digits of pi (www.siliconrepublic.com) — https://www.siliconrepublic.com/machines/google-cloud-pi-calculation-100trn-digits-world-record-emma-haruka-iwao
- This Google employee just smashed the world record for calculating pi (www.cbsnews.com) — https://www.cbsnews.com/news/pi-world-record-google-employee-emma-haruka-iwao-just-smashed-the-guinness-world-record-for-calculating-pi/
- The longest version of the number Pi is calculated (reporteri.net) — https://reporteri.net/llogaritja-e-versionit-me-te-gjate-te-numrit-pi/
- Pi Day record: Google employee Emma Haruka Iwao calculated pi to 31.4 trillion digits (www.washingtonpost.com) — https://www.washingtonpost.com/business/2019/03/14/google-employee-just-shattered-record-pi-calculations-her-name-is-emma-haruka-iwao/
- Google developers set another record for calculating digits of pi: 100 trillion! (www.geekwire.com) — https://www.geekwire.com/2022/google-developers-set-another-record-for-calculating-digits-of-pi-100-trillion/
- It's 2025 and another tech publisher has broken the Pi calculation world record with 300 trillion digits (www.techradar.com) — https://www.techradar.com/pro/its-2025-and-another-tech-publisher-has-broken-the-pi-calculation-world-record-with-300-trillion-digits

### Research (debug)

- source=GoogleGrounding | llm=google/gemini-2.5-pro | cached=0 | n_raw=12 | n_kept=9 | cost=$0.021200

- query: format; metac_closes_in_period; info; post_id; question_id; In March; Emma Haruka Iwao; 2019; 2025

### GTMC1 (debug)

- strategic_class=False | strategic_score=0.00 | source=llm

- gtmc1_active=False | qtype=numeric | t_ms=0

- actors_parsed=0

- exceedance_ge_50=None | coalition_rate=None | median=None | dispersion=None

### Ensemble (model outputs)

- OpenRouter-Default: ok=True t_ms=18556 P10=1.000, P50=2010.000, P90=2.000

- Claude-3.7-Sonnet (OR): ok=True t_ms=20688 P10=31.000, P50=2025.000, P90=1.000

- Gemini: ok=True t_ms=52509 P10=1.000, P50=15.000, P90=2010.000

- Grok: ok=False t_ms=271



### Per-model (raw + usage/cost)

#### OpenRouter-Default

- ok=True | t_ms=18556 | tokens: prompt=3317, completion=514, total=3831 | cost=$0.013433

```md

1. **Base Rate (Prior) Selection**
   - The base rate for the number of digits of π computed is derived from historical records of π calculations. Since 2010, records have been set approximately every 2-3 years, with increases typically by a factor of 2-4x. The most recent record is 300 trillion digits set in May 2025. The base rate suggests a new record every 2-3 years, with the next plausible record potentially around 2027.
   - Mean/Median: 300 trillion digits (current record)
   - Variance: High, due to the exponential growth pattern in record-setting.

2. **Comparison to Base Case**
   - Similarity: The current situation aligns with the historical pattern of exponential growth in π digit calculations, with large organizations driving these efforts.
   - Difference: The extremely short time horizon (<2 months) until the resolution date makes it unlikely for a new record to be set, given the long computation times required.

3. **Evidence Evaluation (Likelihoods)**
   - The recent record of 300 trillion digits set by LMG/Kioxia is the most significant evidence, establishing a high baseline.
   - The short time frame until the resolution date makes it unlikely for a new record to be set, as previous records have taken several months to compute.
   - No public announcements or credible rumors suggest an ongoing attempt that could conclude before the deadline.

4. **Bayesian Updating (Posterior Distribution)**
   - Prior: 300 trillion digits as the most recent record.
   - Likelihood: The short time frame and lack of announcements significantly reduce the likelihood of a new record.
   - Posterior: The distribution remains centered around 300 trillion digits, with a very low probability of exceeding this number by the resolution date.

5. **Red Team Thinking**
   - Overconfidence is mitigated by considering the historical trend and the constraints of computation time.
   - Tail risks are minimal due to the lack of evidence for ongoing attempts.
   - An alternative forecast might consider a small chance of a surprise announcement, but this is unlikely given the current evidence.

6. **Final Percentiles**
   - Given the evidence and analysis, the percentiles reflect a high confidence that the record will remain at 300 trillion digits by the resolution date.

P10: 300 trillion
P20: 300 trillion
P40: 300 trillion
P60: 300 trillion
P80: 300 trillion
P90: 300 trillion

```

#### Claude-3.7-Sonnet (OR)

- ok=True | t_ms=20688 | tokens: prompt=3761, completion=1022, total=4783 | cost=$0.026613

```md

# Forecasting the Largest Number of Digits of π Computed by December 31, 2025

## 1. Base Rate (Prior) Selection

For my base rate, I'll use the historical progression of π digit calculation records, particularly focusing on the most recent trend:

- 2019: Emma Haruka Iwao (Google) - 31.4 trillion digits
- 2022: Google Cloud team - 100 trillion digits
- May 2025: Linus Media Group/Kioxia - 300 trillion digits (most recent record)

The historical pattern shows:
- Records typically increase by a factor of 2-4x
- New records are set approximately every 2-3 years
- The mean of the prior distribution is the current record: 300 trillion digits
- The variance is relatively small in the short term, as these calculations require months of dedicated computation time

## 2. Comparison to Base Case

**Similarities to base case:**
- The technological constraints remain similar (computation time, storage requirements)
- The primary actors continue to be large tech organizations with significant resources
- The Chudnovsky algorithm remains the standard approach

**Differences from base case:**
- The time horizon is extremely short - less than two months remain until December 31, 2025
- The most recent record was set only about 6 months ago (May 2025)
- The computational requirements have grown substantially with each new record

## 3. Evidence Evaluation (Likelihoods)

**Evidence for no change from current 300 trillion record:**
- Computation time: The 300 trillion calculation reportedly took 7.5 months. Any larger calculation would likely take 8+ months, making it impossible to start and complete a new record before December 31, 2025.
- Absence of announcements: No major tech company has publicly announced an ongoing calculation that would be completed before year-end.
- Resource requirements: The storage and computational resources needed for a larger calculation are substantial and require significant planning and investment.

**Evidence for a potential increase:**
- Competitive motivation: The publicity from the LMG/Kioxia record might motivate competitors like Google to reclaim the record.
- Potential for parallel efforts: It's possible that a calculation was already underway when the 300T record was announced.

Likelihood ratio: The evidence strongly favors (>10:1) the current record remaining unchanged through December 31, 2025, primarily due to the time constraints.

## 4. Bayesian Updating (Posterior Distribution)

Starting with a prior centered at 300 trillion with some uncertainty:
- Prior: Mean = 300 trillion, with some probability mass for both higher and lower values
- Likelihood: Strongly favors the 300 trillion value remaining unchanged

Posterior calculation:
- The time constraint evidence essentially eliminates the possibility of values much higher than 300 trillion by December 31, 2025
- The posterior distribution becomes highly concentrated around 300 trillion
- The variance shrinks significantly compared to the prior

The posterior mean remains at 300 trillion, but with much higher confidence than the prior.

## 5. Red Team Thinking

**Potential blind spots:**
- Secret projects: A major tech company might have a stealth project nearing completion
- Verification time: I've focused on computation time, but verification can also take weeks or months
- Algorithmic breakthroughs: While unlikely, a fundamental breakthrough could accelerate calculations

**Alternative scenarios:**
- A calculation of ~350-400 trillion digits might have started before the 300T announcement and could complete just in time
- A smaller organization might announce a slightly larger record (e.g., 310-320 trillion) as a marketing stunt
- The 300 trillion record might be found to have errors, reverting to the previous 100 trillion record

After considering these alternatives, I still find the 300 trillion value most likely, but I should widen my uncertainty bands slightly to account for these possibilities.

## 6. Final Percentiles

Given the analysis above, I'll provide my calibrated percentiles for the largest number of digits of π computed by December 31, 2025:

P10: 300,000,000,000,000
P20: 300,000,000,000,000
P40: 300,000,000,000,000
P60: 300,000,000,000,000
P80: 320,000,000,000,000
P90: 350,000,000,000,000

```

#### Gemini

- ok=True | t_ms=52509 | tokens: prompt=3686, completion=1548, total=9015 | cost=$0.020088

```md

Answering in the persona of a careful probabilistic forecaster.

### 1. Base Rate (Prior) Selection

The appropriate base rate is the historical trend of the world record for the number of computed digits of π. This is a well-documented progression, often visualized on a log-linear scale.

*   **Reference Class:** World records for π digit computation since the advent of electronic computers, particularly focusing on the last 15 years (post-2010) where corporate-sponsored, large-scale computations have become the norm.
*   **Characteristics:** Historically, a new record is set every 1-3 years, with the number of digits increasing by a multiplicative factor of roughly 1.5x to 3x. For example, the record went from 31.4T (2019) to 100T (2022), a ~3.2x increase over three years.
*   **Prior Distribution:** Based on the 2022 record of 100 trillion digits, a prior for the record by the end of 2025 would be centered around 200-300 trillion digits, with a wide distribution reflecting the uncertainty in the timing and magnitude of the next jump.

### 2. Comparison to Base Case

*   **Similarities:** The underlying drivers remain the same: Moore's Law-like improvements in computation and storage, the use of the Chudnovsky algorithm via optimized software like `y-cruncher`, and the motivation of large tech companies or collaborations to use the record as a marketing and benchmarking tool.
*   **Differences:** The most critical difference is the **time horizon**. The base rate operates on a multi-year timescale. This forecast is being made on November 6, 2025, with less than two months remaining until the resolution date of December 31, 2025. This is a much shorter period than the time required to complete a record-setting computation.

### 3. Evidence Evaluation (Likelihoods)

1.  **Evidence: New Record in May 2025.** The research report cites a credible source (TechRadar) reporting a new record of **300 trillion digits** set by Linus Media Group and Kioxia in May 2025.
    *   **Likelihood:** This is extremely strong evidence. It establishes a new, high baseline for the record. This evidence makes any value below 300 trillion extremely unlikely (barring a retraction, which is a tail risk). It shifts the entire probability mass to be at or above 300 trillion.

2.  **Evidence: Computation Time.** The research report notes that the 100T record took ~5 months and the 300T record took ~7.5 months.
    *   **Likelihood:** This is the most crucial piece of evidence for forecasting the near future. To surpass 300T, a new computation would likely require 8+ months. With less than two months remaining until the resolution date, it is practically impossible for a new record attempt to be started *and completed* in time. Any new record would have to come from a project that has been running for most of 2025 and is just now nearing completion.

3.  **Evidence: Lack of Announcements.** The research found no public announcements, credible rumors, or chatter in relevant communities (like the `y-cruncher` forum) about an ongoing attempt by other major players (e.g., Google, AWS) that is about to conclude.
    *   **Likelihood:** While not definitive proof, the absence of signals is evidence against the existence of a nearly-finished secret project. These projects are often used for PR, so complete secrecy until the last moment is less likely. This evidence further reduces the probability of a new record being set in the next 55 days.

### 4. Bayesian Updating (Posterior Distribution)

*   **Prior:** A wide distribution centered on ~250 trillion digits, based on the historical trend from the 100T record in 2022.
*   **Update 1 (LMG 300T Record):** The May 2025 news acts as a direct observation, collapsing the prior. The new distribution is now a sharp spike at 300 trillion, with the question shifting to "what is the probability of a value *greater than* 300 trillion?"
*   **Update 2 (Time Constraint):** The evidence of the ~8-month computation time, when there are less than 2 months left, makes the likelihood of a new record extremely low. This prunes almost all of the probability mass from the region above 300 trillion.
*   **Posterior:** The resulting posterior distribution is very heavily concentrated at exactly 300,000,000,000,000. There is a very small tail for the upside risk of a secret project being revealed, and an even smaller tail for the downside risk of the LMG record being disqualified.

### 5. Red Team Thinking

*   **Main Blind Spot:** Over-reliance on the assumption that no secret project is underway. A major company like Google could have started a 10-month computation in February 2025, aiming for ~500 trillion digits, and be planning a December announcement. This is the most plausible scenario for an upset.
*   **Plausibility Check:** How likely is this? Given that a new record was just set in May, the incentive for another company to immediately invest the resources to break it again is reduced. The lack of any leaks or rumors makes it less probable. I estimate the chance of such a surprise announcement at less than 5%.
*   **Downside Risk:** Could the 300T record be invalidated? This is highly unlikely. The methods are well-established and include verification steps. The risk is negligible.
*   **Conclusion:** The posterior distribution is robust. The probability of a new record is low enough (<10%) that it should not affect the P10 through P90 percentiles. The distribution is effectively a single point mass at 300 trillion for the bulk of the probability.

### 6. Final Percentiles

The evidence strongly indicates that the record of 300 trillion digits set in May 2025 will stand until the end of the year. The primary constraint is the multi-month computation time required to break it, which is not available in the remaining timeframe. The small chance of a secret project completing is not large enough to shift the 90th percentile.

P10: 300000000000000
P20: 300000000000000
P40: 300000000000000
P60: 300000000000000
P80: 300000000000000
P90: 300000000000000

```

#### Grok

- ok=False | t_ms=271 | tokens: prompt=0, completion=0, total=0 | cost=$0.000000

- error=HTTP 429: Your team 0101076a-72f1-4f25-a038-b346d74bbc8d has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.

### Aggregation (BMC)

- final_quantiles: P10=-997.3997013790413, P50=13.05241455450847, P90=1040.9623628753839

- bmc_summary={"mean": 19.763223848408195, "p10": -997.3997013790413, "p50": 13.05241455450847, "p90": 1040.9623628753839}